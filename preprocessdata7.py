import numpy as np
import pandas as pd
from astropy.io import fits
import os
import glob
from sklearn.model_selection import train_test_split, KFold
from scipy import signal, interpolate
from multiprocessing import Pool, cpu_count
import time
from tqdm import tqdm, trange
import matplotlib.pyplot as plt
import gc  # åƒåœ¾å›æ”¶
import psutil  # ç³»ç»Ÿèµ„æºç›‘æ§
import pickle  # ç”¨äºä¿å­˜ä¸­é—´ç»“æœ
import warnings
import subprocess  # ç”¨äºæ‰§è¡Œshellå‘½ä»¤
import zipfile  # ç”¨äºè§£å‹æ–‡ä»¶
import sys  # ç”¨äºæ£€æµ‹ç¯å¢ƒ
import shutil
from utils import CacheManager
import concurrent.futures
import json
import traceback
import re
from sklearn.model_selection import train_test_split
import random

# GPUåŠ é€Ÿç›¸å…³å¯¼å…¥
try:
    import cupy as cp
    import cupyx.scipy.signal as cusignal
    import cupyx.scipy.ndimage as cundimage
    HAS_GPU = True
except ImportError:
    HAS_GPU = False
    # åˆ›å»ºå ä½ç¬¦ä»¥é¿å…åœ¨åç»­ä»£ç ä¸­å‡ºç°å¯¼å…¥é”™è¯¯
    class DummyModule:
        def __getattr__(self, name):
            return None
    
    cp = DummyModule()
    cusignal = DummyModule()
    cundimage = DummyModule()
    print("è­¦å‘Š: æœªæ‰¾åˆ°CuPyåº“ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼è¿è¡Œ")
    print("è¦å¯ç”¨GPUåŠ é€Ÿï¼Œè¯·å®‰è£…CuPyåº“: pip install cupy-cuda11x (æ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„åŒ…)")

warnings.filterwarnings('ignore')  # å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Š

# åˆ¤æ–­æ˜¯å¦åœ¨Colabç¯å¢ƒä¸­
def is_in_colab():
    """æ£€æµ‹æ˜¯å¦åœ¨Google Colabç¯å¢ƒä¸­è¿è¡Œ"""
    try:
        # ä½¿ç”¨å­—ç¬¦ä¸²å½¢å¼çš„å¯¼å…¥ï¼Œé¿å…IDEæŠ¥é”™
        import importlib
        colab_module = importlib.util.find_spec('google.colab')
        return colab_module is not None
    except:
        return False

# æ£€æµ‹æ˜¯å¦æœ‰å¯ç”¨çš„GPU
def check_gpu_available():
    """æ£€æµ‹æ˜¯å¦æœ‰å¯ç”¨çš„GPUï¼Œå¹¶è¿”å›è®¾å¤‡ä¿¡æ¯"""
    if not HAS_GPU:
        print("æœªå®‰è£…CuPyï¼Œæ— æ³•ä½¿ç”¨GPU")
        return False
    
    try:
        # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
        n_gpus = cp.cuda.runtime.getDeviceCount()
        if n_gpus > 0:
            # æ‰“å°GPUä¿¡æ¯
            for i in range(n_gpus):
                device_props = cp.cuda.runtime.getDeviceProperties(i)
                print(f"æ‰¾åˆ°GPU {i}: {device_props['name']}, æ˜¾å­˜: {device_props['totalGlobalMem'] / (1024**3):.2f} GB")
            
            # è®¾ç½®é»˜è®¤ä½¿ç”¨çš„GPU
            cp.cuda.runtime.setDevice(0)
            print(f"ä½¿ç”¨GPU: {cp.cuda.runtime.getDevice()}")
            return True
        else:
            print("æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPU")
            return False
    except Exception as e:
        print(f"GPUæ£€æµ‹å‡ºé”™: {e}")
        return False

# ç¯å¢ƒè®¾ç½®
IN_COLAB = is_in_colab()
USE_GPU = check_gpu_available()

class LAMOSTPreprocessor:
    def __init__(self, csv_files=None, 
                 fits_dir='fits', 
                 output_dir='processed_data',
                 wavelength_range=(3690, 9100),  # è®¾ç½®å›ºå®šçš„é»˜è®¤èŒƒå›´
                 n_points=None,
                 log_step=0.0001,
                 compute_common_range=False,  # é»˜è®¤ä¸è®¡ç®—å…¬å…±èŒƒå›´
                 n_splits=5,     # äº¤å‰éªŒè¯æŠ˜æ•°
                 max_workers=None,  # æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨ç¡®å®š
                 batch_size=20,  # æ‰¹å¤„ç†å¤§å°
                 memory_limit=0.7,  # å†…å­˜ä½¿ç”¨é™åˆ¶(å æ€»å†…å­˜æ¯”ä¾‹)
                 low_memory_mode=False,  # ä½å†…å­˜æ¨¡å¼
                 use_gpu=USE_GPU):  # æ˜¯å¦ä½¿ç”¨GPU
        
        # è®¾ç½®GPUä½¿ç”¨é€‰é¡¹
        self.use_gpu = use_gpu
        if self.use_gpu and not HAS_GPU:
            print("è­¦å‘Š: æœªæ‰¾åˆ°CuPyåº“ï¼Œæ— æ³•ä½¿ç”¨GPUæ¨¡å¼")
            self.use_gpu = False
        
        if self.use_gpu:
            print("ğŸš€ å·²å¯ç”¨GPUåŠ é€Ÿæ¨¡å¼")
            # é¢„çƒ­GPUï¼Œåˆ†é…å°‘é‡å†…å­˜ç¡®ä¿GPUå·²åˆå§‹åŒ–
            try:
                temp_array = cp.zeros((100, 100), dtype=cp.float32)
                cp.sum(temp_array)
                # åŒæ­¥GPUï¼Œç¡®ä¿æ“ä½œå®Œæˆ
                cp.cuda.Stream.null.synchronize()
                # é‡Šæ”¾å†…å­˜
                del temp_array
                cp.get_default_memory_pool().free_all_blocks()
                print("GPUé¢„çƒ­å®Œæˆ")
            except Exception as e:
                print(f"GPUé¢„çƒ­å¤±è´¥: {e}")
                print("åˆ‡æ¢åˆ°CPUæ¨¡å¼")
                self.use_gpu = False
        else:
            print("ä½¿ç”¨CPUæ¨¡å¼è¿è¡Œ")
            
        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        # é»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•ä¸‹æ‰€æœ‰çš„CSVæ–‡ä»¶
        if csv_files is None:
            csv_files = [f for f in os.listdir() if f.endswith('.csv')]
            if not csv_files:
                print("è­¦å‘Š: å½“å‰ç›®å½•æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            else:
                print(f"è‡ªåŠ¨æ£€æµ‹åˆ°ä»¥ä¸‹CSVæ–‡ä»¶: {csv_files}")
                
        self.csv_files = [csv_file if os.path.exists(csv_file) else os.path.join('/content', csv_file) for csv_file in csv_files]
        self.fits_dir = fits_dir if os.path.exists(fits_dir) else os.path.join('/content', fits_dir)
        self.output_dir = output_dir
        self.cache_dir = os.path.join(self.output_dir, 'cache')
        
        # æ·»åŠ Google Driveå¤‡ç”¨ç¼“å­˜ç›®å½•
        self.drive_cache_dir = '/content/drive/My Drive/SPCNNet_Results/processed_data/cache'
        
        self.progress_dir = os.path.join(self.output_dir, 'progress')
        self.figures_dir = os.path.join(self.output_dir, 'figures')
        self.logs_dir = os.path.join(self.output_dir, 'logs')
        self.model_dir = os.path.join(self.output_dir, 'models')
        self.prediction_output_dir = os.path.join(self.output_dir, 'predictions')
        self.cache_enabled = True
        
        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
        for directory in [self.output_dir, self.cache_dir, self.progress_dir, 
                          self.figures_dir, self.logs_dir, self.model_dir, 
                          self.prediction_output_dir]:
            os.makedirs(directory, exist_ok=True)
        
        # åˆå§‹åŒ–æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜
        self.fits_file_cache = {}
        self.cache_file = os.path.join(self.cache_dir, 'files_cache.pkl')
        
        # åœ¨åˆå§‹åŒ–æ—¶å°±åŠ è½½æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜
        print("æ­£åœ¨åŠ è½½æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜...")
        self._load_files_cache()
        
        self.wavelength_range = wavelength_range  # ä½¿ç”¨å›ºå®šèŒƒå›´
        self.n_points = n_points
        self.log_step = log_step
        self.compute_common_range = compute_common_range
        self.n_splits = n_splits
        
        # å…‰é€Ÿå¸¸é‡ï¼ˆkm/sï¼‰
        self.c = 299792.458
        
        # å­˜å‚¨å·²å¤„ç†å…‰è°±çš„æ³¢é•¿èŒƒå›´ï¼Œç”¨äºè®¡ç®—æœ€å¤§å…¬æœ‰èŒƒå›´
        self.processed_ranges = []
        
        # æ€§èƒ½ç›¸å…³å‚æ•°
        self.batch_size = batch_size
        self.memory_limit = memory_limit
        
        # è®¾ç½®æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
        if max_workers is None:
            # é»˜è®¤ä½¿ç”¨CPUæ ¸å¿ƒæ•°çš„ä¸€åŠï¼Œé¿å…ç³»ç»Ÿè¿‡çƒ­
            self.max_workers = max(1, cpu_count() // 2)
        else:
            self.max_workers = max_workers
            
        print(f"è®¾ç½®æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°: {self.max_workers}")
        
        self.low_memory_mode = low_memory_mode
        
        self.cache_manager = CacheManager(cache_dir=os.path.join(output_dir, 'cache'))
        
        self.update_cache_manager()
        
    def _load_files_cache(self):
        """åŠ è½½æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜"""
        # å°è¯•æ ‡å‡†è·¯å¾„
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    self.fits_file_cache = pickle.load(f)
                print(f"å·²åŠ è½½æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜ï¼Œå…±{len(self.fits_file_cache)}æ¡è®°å½•")
                return
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ç¼“å­˜å‡ºé”™: {e}")
                self.fits_file_cache = {}
        
        # å°è¯•Google Driveè·¯å¾„
        drive_cache_file = '/content/drive/My Drive/SPCNNet_Results/processed_data/cache/files_cache.pkl'
        if os.path.exists(drive_cache_file):
            try:
                with open(drive_cache_file, 'rb') as f:
                    self.fits_file_cache = pickle.load(f)
                print(f"å·²ä»Google DriveåŠ è½½æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜ï¼Œå…±{len(self.fits_file_cache)}æ¡è®°å½•")
                # ç«‹å³ä¿å­˜åˆ°æ ‡å‡†è·¯å¾„ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
                try:
                    self._save_files_cache()
                    print("å·²å°†Driveç¼“å­˜åŒæ­¥åˆ°æœ¬åœ°")
                except:
                    pass
                return
            except Exception as e:
                print(f"åŠ è½½Google Driveæ–‡ä»¶ç¼“å­˜å‡ºé”™: {e}")
                self.fits_file_cache = {}
        
        # éƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆå§‹åŒ–ç©ºç¼“å­˜
        self.fits_file_cache = {}
    
    def _save_files_cache(self):
        """ä¿å­˜æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.fits_file_cache, f)
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶ç¼“å­˜å‡ºé”™: {e}")
    
    def read_csv_data(self):
        """è¯»å–CSVæ–‡ä»¶å¹¶è¿”å›DataFrameåˆ—è¡¨å’Œå¯¹åº”çš„å…ƒç´ åç§°"""
        dataframes = []
        elements = []
        for csv_file in self.csv_files:
            if not os.path.exists(csv_file):
                print(f"é”™è¯¯: æ‰¾ä¸åˆ°CSVæ–‡ä»¶ {csv_file}")
                print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
                print(f"å°è¯•æŸ¥æ‰¾çš„å®Œæ•´è·¯å¾„: {os.path.abspath(csv_file)}")
                
                # å°è¯•ä»å¯èƒ½çš„ç›®å½•ä¸­æŸ¥æ‰¾
                possible_dirs = ['/content', '/content/drive/My Drive', '/content/SPCNNet']
                for posdir in possible_dirs:
                    if os.path.exists(posdir):
                        possible_path = os.path.join(posdir, os.path.basename(csv_file))
                        if os.path.exists(possible_path):
                            print(f"æ‰¾åˆ°å¯ç”¨çš„CSVæ–‡ä»¶: {possible_path}")
                            csv_file = possible_path
                            break
                        
                if not os.path.exists(csv_file):
                    # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œåˆ—å‡ºå½“å‰ç›®å½•çš„æ–‡ä»¶
                    print("å½“å‰ç›®å½•ä¸­çš„æ–‡ä»¶:")
                    for f in os.listdir():
                        print(f"  - {f}")
                    continue
                
            print(f"è¯»å–CSVæ–‡ä»¶: {csv_file}")
            try:
                df = pd.read_csv(csv_file)
                print(f"æˆåŠŸåŠ è½½{csv_file}ï¼Œå…±{len(df)}æ¡è®°å½•")
                print(f"åˆ—å: {', '.join(df.columns)}")
                
                # ä»CSVæ–‡ä»¶åæå–å…ƒç´ ä¿¡æ¯
                element_name = os.path.basename(csv_file).split('.')[0]
                print(f"è¯†åˆ«åˆ°å…ƒç´ ï¼š{element_name}")
                
                # æ£€æŸ¥specåˆ—ä¸­çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if 'spec' in df.columns:
                    # ç¡®ä¿specåˆ—çš„ç±»å‹ä¸ºå­—ç¬¦ä¸²
                    if not pd.api.types.is_string_dtype(df['spec']):
                        print(f"æ³¨æ„: {csv_file} ä¸­çš„specåˆ—ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œæ­£åœ¨è½¬æ¢...")
                        df['spec'] = df['spec'].astype(str)
                    
                    # ä¸åœ¨å¯åŠ¨æ—¶æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶ï¼Œåªæ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
                    print(f"CSVæ–‡ä»¶åŒ…å«{len(df)}æ¡è®°å½•ï¼Œå¦‚æœæ‰¾ä¸åˆ°æŸäº›FITSæ–‡ä»¶ï¼Œå°†åœ¨å¤„ç†æ—¶æŠ¥é”™")
                    
                    # ä»…æ£€æŸ¥å‰3ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹(ä¸å†æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶)
                    spec_files = df['spec'].values[:3]
                    for spec_file in spec_files:
                        # ä½¿ç”¨_find_fits_fileæ–¹æ³•æŸ¥æ‰¾æ–‡ä»¶
                        found_path = self._find_fits_file(spec_file)
                        if found_path:
                            print(f"ç¤ºä¾‹æ–‡ä»¶æ‰¾åˆ°: {found_path}")
                        else:
                            print(f"ç¤ºä¾‹æ–‡ä»¶æœªæ‰¾åˆ°: {spec_file}ï¼Œè¯·ç¡®ä¿FITSæ–‡ä»¶è·¯å¾„æ­£ç¡®")
                else:
                    print(f"è­¦å‘Š: CSVæ–‡ä»¶ {csv_file} ä¸­æ²¡æœ‰æ‰¾åˆ°'spec'åˆ—")
                    print(f"å¯ç”¨çš„åˆ—: {df.columns.tolist()}")
                
                dataframes.append(df)
                elements.append(element_name)
            except Exception as e:
                print(f"è¯»å–CSVæ–‡ä»¶ {csv_file} å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        
        return dataframes, elements
    
    def _find_fits_file(self, spec_name):
        """æŸ¥æ‰¾åŒ¹é…çš„fitsæ–‡ä»¶ï¼Œå¤„ç†åµŒå¥—ç›®å½•å’Œå‘½åå·®å¼‚"""
        # ç¡®ä¿spec_nameæ˜¯å­—ç¬¦ä¸²ç±»å‹
        spec_name = str(spec_name)
        
        # æ£€æŸ¥ç¼“å­˜
        if not hasattr(self, 'fits_file_cache'):
            self.fits_file_cache = {}
            # åŠ è½½æ–‡ä»¶ç¼“å­˜
            self._load_files_cache()
            
        if spec_name in self.fits_file_cache:
            cache_file = self.fits_file_cache[spec_name]
            # ç¡®è®¤ç¼“å­˜çš„æ–‡ä»¶ä»ç„¶å­˜åœ¨
            if cache_file is not None and os.path.exists(cache_file):
                return cache_file
            # æ–‡ä»¶å·²ç§»åŠ¨æˆ–åˆ é™¤ï¼Œé‡æ–°æŸ¥æ‰¾
            self.fits_file_cache[spec_name] = None
        
        # å¦‚æœè¾“å…¥å·²ç»æ˜¯å®Œæ•´è·¯å¾„ï¼Œæå–æ–‡ä»¶åéƒ¨åˆ†
        if os.path.isabs(spec_name):
            # å…ˆæ£€æŸ¥å®Œæ•´è·¯å¾„æ˜¯å¦ç›´æ¥å­˜åœ¨
            if os.path.exists(spec_name) and os.path.isfile(spec_name):
                print(f"æ‰¾åˆ°ç»å¯¹è·¯å¾„æ–‡ä»¶: {spec_name}")
                self.fits_file_cache[spec_name] = spec_name
                return spec_name
                
            # å¦‚æœå®Œæ•´è·¯å¾„ä¸å­˜åœ¨ï¼Œæå–æ–‡ä»¶å
            base_name = os.path.basename(spec_name)
        else:
            # ç›¸å¯¹è·¯å¾„æƒ…å†µä¸‹
            base_name = spec_name
        
        # è®°å½•æ—¥å¿—
        print(f"æŸ¥æ‰¾æ–‡ä»¶: {spec_name}, åŸºç¡€åç§°: {base_name}")
        
        # é¦–å…ˆå°è¯•ç›´æ¥åœ¨fitsç›®å½•ä¸‹æŒ‰å®Œæ•´è·¯å¾„åŒ¹é…
        direct_path = os.path.join(self.fits_dir, spec_name)
        if os.path.exists(direct_path) and os.path.isfile(direct_path):
            print(f"ç›´æ¥åŒ¹é…æˆåŠŸ: {direct_path}")
            self.fits_file_cache[spec_name] = direct_path
            return direct_path
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…ƒç´ åç§°è€Œä¸æ˜¯æ–‡ä»¶è·¯å¾„ï¼ˆä¾‹å¦‚"C_FE"ï¼‰
        if spec_name in ["C_FE", "MG_FE", "CA_FE"]:
            print(f"æ£€æµ‹åˆ°å…ƒç´ åç§°: {spec_name}ï¼Œå°è¯•æŸ¥æ‰¾å¯¹åº”çš„ç¤ºä¾‹FITSæ–‡ä»¶")
            # æŸ¥æ‰¾ä¸å…ƒç´ ç›¸å…³çš„CSVæ–‡ä»¶ä¸­çš„FITSæ–‡ä»¶æ ·ä¾‹
            csv_path = f"{spec_name}.csv"
            if os.path.exists(csv_path):
                try:
                    import pandas as pd
                    df = pd.read_csv(csv_path)
                    if 'spec' in df.columns and len(df) > 0:
                        first_spec = df['spec'].iloc[0]
                        print(f"ä»CSVæ–‡ä»¶ä¸­è·å–ç¬¬ä¸€ä¸ªæ ·ä¾‹: {first_spec}")
                        return self._find_fits_file(first_spec)
                except Exception as e:
                    print(f"ä»CSVè¯»å–æ ·ä¾‹æ—¶å‡ºé”™: {e}")
        
        # å°è¯•ç›´æ¥åœ¨fitsç›®å½•ä¸‹æŒ‰åŸºç¡€åç§°åŒ¹é…ï¼ˆå¸¸è§„åç¼€ï¼‰
        for ext in ['', '.fits', '.fits.gz', '.fit', '.fit.gz']:
            path = os.path.join(self.fits_dir, base_name + ext)
            if os.path.exists(path) and os.path.isfile(path):
                print(f"åŸºç¡€åç§°åŒ¹é…æˆåŠŸ: {path}")
                self.fits_file_cache[spec_name] = path
                return path
        
        # è¿›è¡Œé€’å½’æœç´¢ï¼Œå¤„ç†åµŒå¥—ç›®å½•
        print(f"å¼€å§‹é€’å½’æœç´¢FITSç›®å½•: {self.fits_dir}")
        fits_files_found = []
        
        for root, dirs, files in os.walk(self.fits_dir):
            for file in files:
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
                if base_name.lower() in file.lower():
                    found_path = os.path.join(root, file)
                    fits_files_found.append(found_path)
                    print(f"éƒ¨åˆ†åç§°åŒ¹é…æˆåŠŸ: {found_path}")
                
                # å°è¯•å»é™¤å¯èƒ½çš„åç¼€åå†æ¯”è¾ƒ
                file_base = file.lower()
                for ext in ['.fits', '.fits.gz', '.fit', '.fit.gz']:
                    if file_base.endswith(ext):
                        file_base = file_base[:-len(ext)]
                        break
                
                if base_name.lower() == file_base:
                    found_path = os.path.join(root, file)
                    fits_files_found.append(found_path)
                    print(f"å»é™¤åç¼€ååŒ¹é…æˆåŠŸ: {found_path}")
                
                # å°è¯•æ›´æ¨¡ç³Šçš„åŒ¹é…æ–¹å¼
                # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦ï¼Œä¾¿äºåŒ¹é…è·¨ç›®å½•æ–‡ä»¶
                clean_base_name = base_name.replace('/', '_').replace('\\', '_')
                clean_file_base = file_base.replace('/', '_').replace('\\', '_')
                
                if clean_base_name.lower() in clean_file_base or clean_file_base in clean_base_name.lower():
                    found_path = os.path.join(root, file)
                    fits_files_found.append(found_path)
                    print(f"æ¨¡ç³ŠåŒ¹é…æˆåŠŸ: {found_path}")
        
        # å¤„ç†æ‰¾åˆ°çš„æ–‡ä»¶
        if fits_files_found:
            # ä¼˜å…ˆé€‰æ‹©ä¸æŸ¥è¯¢åç§°æœ€åŒ¹é…çš„æ–‡ä»¶
            best_match = fits_files_found[0]  # é»˜è®¤ç¬¬ä¸€ä¸ª
            for found_path in fits_files_found:
                file_name = os.path.basename(found_path).lower()
                # å¦‚æœæ–‡ä»¶ååŒ…å«æŸ¥è¯¢åç§°ï¼Œä¼˜å…ˆé€‰æ‹©
                if base_name.lower() in file_name:
                    best_match = found_path
                    break
            
            print(f"ä»{len(fits_files_found)}ä¸ªåŒ¹é…ç»“æœä¸­é€‰æ‹©: {best_match}")
            self.fits_file_cache[spec_name] = best_match
            return best_match
        
        # å°è¯•å…¶ä»–å¤‡é€‰æ–¹æ¡ˆ - å¦‚æœç›®å½•ä¸­æœ‰ä»»ä½•FITSæ–‡ä»¶ï¼Œä½œä¸ºæœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        fits_files = []
        for root, _, files in os.walk(self.fits_dir):
            for file in files:
                if any(file.lower().endswith(ext) for ext in ['.fits', '.fits.gz', '.fit', '.fit.gz']):
                    fits_files.append(os.path.join(root, file))
        
        if fits_files:
            # æ˜¾ç¤ºå¯ç”¨çš„å¤‡é€‰æ–‡ä»¶
            print(f"æœªç›´æ¥æ‰¾åˆ° {spec_name} åŒ¹é…çš„æ–‡ä»¶ã€‚")
            print(f"FITSç›®å½•ä¸­æœ‰ {len(fits_files)} ä¸ªFITSæ–‡ä»¶å¯ç”¨ã€‚")
            print(f"å¯ç”¨çš„FITSæ–‡ä»¶ç¤ºä¾‹: {fits_files[0] if fits_files else 'æ— '}")
        
        # å¦‚æœä»¥ä¸Šéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›None
        print(f"æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {spec_name}")
        return None
    
    def _get_file_extension(self, fits_file):
        """è·å–æ–‡ä»¶å®Œæ•´è·¯å¾„ï¼Œä½¿ç”¨ç¼“å­˜é¿å…é‡å¤æŸ¥æ‰¾"""
        if fits_file in self.fits_file_cache:
            return self.fits_file_cache[fits_file]
            
        # æŸ¥æ‰¾å®é™…æ–‡ä»¶è·¯å¾„
        file_path = self._find_fits_file(fits_file)
        if file_path:
            self.fits_file_cache[fits_file] = file_path
            return file_path
        else:
            self.fits_file_cache[fits_file] = None
            return None
    
    def read_fits_file(self, fits_file):
        """è¯»å–FITSæ–‡ä»¶å¹¶è¿”å›æ³¢é•¿å’Œæµé‡æ•°æ®"""
        # è·å–æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„
        file_path = self._get_file_extension(fits_file)
        if file_path is None:
            print(f"æ— æ³•æ‰¾åˆ°æ–‡ä»¶: {fits_file}ï¼ŒæŸ¥æ‰¾è·¯å¾„: {self.fits_dir}")
            return None, None, 0, 0, 0, {}
        
        print(f"è¯»å–æ–‡ä»¶: {file_path}")
        try:
            # ä½¿ç”¨æ›´å¤šé€‰é¡¹æ‰“å¼€FITSæ–‡ä»¶
            with fits.open(file_path, ignore_missing_end=True, memmap=False) as hdul:
                # æ‰“å°HDUä¿¡æ¯ä»¥å¸®åŠ©è¯Šæ–­
                print(f"FITSæ–‡ä»¶ç»“æ„: å…±{len(hdul)}ä¸ªHDU")
                for i, hdu in enumerate(hdul):
                    print(f"  HDU{i}: ç±»å‹={type(hdu).__name__}, å½¢çŠ¶={hdu.shape if hasattr(hdu, 'shape') else 'æ— å½¢çŠ¶'}")
                
                # è·å–ä¸»HDUçš„å¤´ä¿¡æ¯
                header = hdul[0].header
                
                # è¾“å‡ºå…³é”®å¤´ä¿¡æ¯å¸®åŠ©è¯Šæ–­
                print(f"ä¸»HDUå¤´ä¿¡æ¯: NAXIS={header.get('NAXIS')}, NAXIS1={header.get('NAXIS1')}, "
                      f"BITPIX={header.get('BITPIX')}")
                
                # å°è¯•è·å–è§†å‘é€Ÿåº¦ä¿¡æ¯(å¦‚æœæœ‰)
                v_helio = 0
                for key in ['V_HELIO', 'RV', 'VELOCITY', 'v_helio', 'rv', 'velocity']:
                    if key in header:
                        v_helio = header.get(key, 0)
                        print(f"ä»FITSå¤´ä¿¡æ¯ä¸­æ‰¾åˆ°è§†å‘é€Ÿåº¦: {key} = {v_helio}")
                        break
                
                # å°è¯•è·å–çº¢ç§»å€¼(å¯èƒ½æœ‰ä¸åŒçš„å…³é”®å­—)
                z = 0
                for key in ['Z', 'REDSHIFT', 'z', 'redshift', 'Redshift', 'RED_SHIFT', 'red_shift']:
                    if key in header:
                        z = header.get(key, 0)
                        print(f"ä»FITSå¤´ä¿¡æ¯ä¸­æ‰¾åˆ°çº¢ç§»å€¼: {key} = {z}")
                        break
                
                # å¦‚æœåœ¨å¤´ä¿¡æ¯ä¸­æ²¡æ‰¾åˆ°çº¢ç§»å€¼ï¼Œå°è¯•åœ¨æ‰©å±•è¡¨ä¸­æŸ¥æ‰¾
                if z == 0 and len(hdul) > 1:
                    for i in range(1, len(hdul)):
                        if isinstance(hdul[i], fits.BinTableHDU):
                            table_hdu = hdul[i]
                            column_names = table_hdu.columns.names
                            print(f"æ£€æŸ¥è¡¨æ ¼HDU{i}ä¸­çš„çº¢ç§»å€¼, åˆ—å: {column_names}")
                            
                            # å¯»æ‰¾çº¢ç§»åˆ—
                            for col_name in ['Z', 'REDSHIFT', 'z', 'redshift', 'Redshift', 'RED_SHIFT', 'red_shift']:
                                if col_name in column_names:
                                    try:
                                        z_values = table_hdu.data[col_name]
                                        if len(z_values) > 0:
                                            z = z_values[0]
                                            print(f"ä»è¡¨æ ¼HDU{i}åˆ—'{col_name}'ä¸­æ‰¾åˆ°çº¢ç§»å€¼: {z}")
                                            break
                                    except Exception as e:
                                        print(f"è¯»å–è¡¨æ ¼HDU{i}åˆ—'{col_name}'æ—¶å‡ºé”™: {e}")
                            
                            if z != 0:
                                break
                
                # è·å–ä¿¡å™ªæ¯”ä¿¡æ¯(å°è¯•å¤šç§å¯èƒ½çš„å…³é”®å­—)
                snr = 0
                for key in ['SNR', 'SNRATIO', 'SN', 'S/N', 'snr']:
                    if key in header:
                        snr = header.get(key, 0)
                        print(f"ä»FITSå¤´ä¿¡æ¯ä¸­æ‰¾åˆ°ä¿¡å™ªæ¯”: {key} = {snr}")
                        break
                
                # è·å–äº”æ®µåŒºé—´çš„ä¿¡å™ªæ¯”ä¿¡æ¯
                snr_bands = {'snru': 0, 'snrg': 0, 'snrr': 0, 'snri': 0, 'snrz': 0}
                for band in snr_bands:
                    for variation in [band, band.upper(), band.capitalize()]:
                        if variation in header:
                            snr_bands[band] = header.get(variation, 0)
                            print(f"ä»FITSå¤´ä¿¡æ¯ä¸­æ‰¾åˆ°{band}æ³¢æ®µä¿¡å™ªæ¯”: {snr_bands[band]}")
                            break
                
                # ä¼˜å…ˆè·å–ç¬¬ä¸€ä¸ªHDUçš„æ•°æ®(å¦‚æœæ˜¯ä¸»è¦å…‰è°±æ•°æ®)
                flux = None
                wavelength = None
                
                # è§„åˆ™1: å¦‚æœä¸»HDUæ˜¯PrimaryHDUä¸”åŒ…å«æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                if isinstance(hdul[0], fits.PrimaryHDU) and hdul[0].data is not None:
                    if len(hdul[0].data.shape) == 1:  # ä¸€ç»´æ•°æ®
                        flux = hdul[0].data
                        # ä»å¤´ä¿¡æ¯åˆ›å»ºæ³¢é•¿æ•°ç»„
                        if 'CRVAL1' in header and 'CDELT1' in header and 'NAXIS1' in header:
                            crval1 = header['CRVAL1']  # èµ·å§‹æ³¢é•¿
                            cdelt1 = header['CDELT1']  # æ³¢é•¿æ­¥é•¿
                            naxis1 = header['NAXIS1']  # æ³¢é•¿ç‚¹æ•°
                            wavelength = np.arange(crval1, crval1 + cdelt1 * naxis1, cdelt1)[:naxis1]
                        print(f"ä½¿ç”¨ä¸»HDUçš„ä¸€ç»´æ•°æ®: ç‚¹æ•°={len(flux)}")
                        
                    elif len(hdul[0].data.shape) == 2:  # äºŒç»´æ•°æ®
                        # å–ç¬¬ä¸€è¡Œæˆ–åˆ—ï¼Œå–å†³äºå“ªä¸ªæ›´é•¿
                        if hdul[0].data.shape[0] > hdul[0].data.shape[1]:
                            flux = hdul[0].data[0]
                        else:
                            flux = hdul[0].data[:, 0]
                        print(f"ä½¿ç”¨ä¸»HDUçš„äºŒç»´æ•°æ®çš„ç¬¬ä¸€è¡Œ/åˆ—: ç‚¹æ•°={len(flux)}")
                
                # è§„åˆ™2: å¦‚æœæ•°æ®åœ¨è¡¨æ ¼HDUä¸­
                if flux is None and len(hdul) > 1:
                    for i in range(1, len(hdul)):
                        if isinstance(hdul[i], fits.BinTableHDU):
                            table_hdu = hdul[i]
                            column_names = table_hdu.columns.names
                            print(f"æ£€æŸ¥è¡¨æ ¼HDU{i}, åˆ—å: {column_names}")
                            
                            # æŸ¥æ‰¾å…‰è°±æ•°æ®åˆ—
                            flux_col = None
                            wave_col = None
                            
                            # å¯»æ‰¾å…‰è°±æµé‡åˆ—
                            for col_name in ['FLUX', 'SPEC', 'DATA', 'INTENSITY', 'COUNTS', 'flux']:
                                if col_name in column_names:
                                    flux_col = col_name
                                    break
                            
                            # å¯»æ‰¾æ³¢é•¿åˆ—
                            for wave_name in ['WAVE', 'WAVELENGTH', 'LAMBDA', 'wave', 'wavelength']:
                                if wave_name in column_names:
                                    wave_col = wave_name
                                    break
                            
                            # å¦‚æœæ‰¾åˆ°æµé‡åˆ—
                            if flux_col is not None:
                                try:
                                    # è¯»å–æµé‡æ•°æ®
                                    flux_data = table_hdu.data[flux_col]
                                    
                                    # å¦‚æœæµé‡æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå–ç¬¬ä¸€è¡Œ
                                    if hasattr(flux_data, 'shape') and len(flux_data.shape) > 1:
                                        flux = flux_data[0].astype(np.float64)
                                    else:
                                        # ç¡®ä¿fluxæ˜¯ä¸€ç»´æ•°ç»„
                                        flux = np.array(flux_data, dtype=np.float64).flatten()
                                    
                                    print(f"ä»åˆ— '{flux_col}' æå–æµé‡æ•°æ®, ç‚¹æ•°={len(flux)}")
                                    
                                    # å¦‚æœæ‰¾åˆ°æ³¢é•¿åˆ—ï¼Œè¯»å–æ³¢é•¿æ•°æ®
                                    if wave_col is not None:
                                        wave_data = table_hdu.data[wave_col]
                                        if hasattr(wave_data, 'shape') and len(wave_data.shape) > 1:
                                            wavelength = wave_data[0].astype(np.float64)
                                        else:
                                            wavelength = np.array(wave_data, dtype=np.float64).flatten()
                                        print(f"ä»åˆ— '{wave_col}' æå–æ³¢é•¿æ•°æ®, ç‚¹æ•°={len(wavelength)}")
                                        
                                        # ç¡®ä¿æ³¢é•¿å’Œæµé‡æ•°ç»„é•¿åº¦åŒ¹é…
                                        if len(wavelength) != len(flux):
                                            min_len = min(len(wavelength), len(flux))
                                            wavelength = wavelength[:min_len]
                                            flux = flux[:min_len]
                                            print(f"è°ƒæ•´æ•°ç»„é•¿åº¦ä¸ºåŒ¹é…é•¿åº¦: {min_len}")
                                    
                                    break  # æ‰¾åˆ°æ•°æ®åé€€å‡ºå¾ªç¯
                                except Exception as e:
                                    print(f"ä»è¡¨æ ¼æå–æ•°æ®å‡ºé”™: {e}")
                                    flux = None  # é‡ç½®ï¼Œå°è¯•å…¶ä»–HDU
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ³¢é•¿æ•°æ®ï¼Œä½†æœ‰æµé‡æ•°æ®
                if wavelength is None and flux is not None:
                    # å°è¯•ä»å¤´ä¿¡æ¯åˆ›å»ºæ³¢é•¿æ•°ç»„
                    if 'CRVAL1' in header and 'CDELT1' in header and 'NAXIS1' in header:
                        crval1 = header['CRVAL1']  # èµ·å§‹æ³¢é•¿
                        cdelt1 = header['CDELT1']  # æ³¢é•¿æ­¥é•¿
                        naxis1 = header['NAXIS1']  # æ³¢é•¿ç‚¹æ•°
                        
                        # ç¡®ä¿naxis1ä¸fluxé•¿åº¦åŒ¹é…
                        if naxis1 != len(flux):
                            naxis1 = len(flux)
                            print(f"è°ƒæ•´NAXIS1å€¼ä¸ºä¸æµé‡æ•°ç»„åŒ¹é…: {naxis1}")
                        
                        wavelength = np.arange(crval1, crval1 + cdelt1 * naxis1, cdelt1)[:naxis1]
                        print(f"ä»å¤´ä¿¡æ¯åˆ›å»ºæ³¢é•¿æ•°ç»„: èŒƒå›´={wavelength[0]:.2f}~{wavelength[-1]:.2f}")
                    else:
                        # å¦‚æœæ²¡æœ‰å¤´ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤æ³¢é•¿èŒƒå›´
                        print("å¤´ä¿¡æ¯ä¸­æ²¡æœ‰æ³¢é•¿å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤æ³¢é•¿èŒƒå›´")
                        naxis1 = len(flux)
                        # LAMOST DR10å…‰è°±çš„å…¸å‹æ³¢é•¿èŒƒå›´çº¦ä¸º3700-9000Ã…
                        crval1 = 3700.0  # èµ·å§‹æ³¢é•¿
                        cdelt1 = (9000.0 - 3700.0) / naxis1  # æ³¢é•¿æ­¥é•¿
                        wavelength = np.arange(crval1, crval1 + cdelt1 * naxis1, cdelt1)[:naxis1]
                        print(f"åˆ›å»ºé»˜è®¤æ³¢é•¿æ•°ç»„: èŒƒå›´={wavelength[0]:.2f}~{wavelength[-1]:.2f}")
                
                # è¿›è¡Œæœ€åçš„æ•°æ®æ£€æŸ¥
                if flux is None:
                    print("æ— æ³•ä»FITSæ–‡ä»¶æå–æµé‡æ•°æ®")
                    return None, None, 0, 0, 0, {}
                
                if wavelength is None:
                    print("æ— æ³•ç”Ÿæˆæ³¢é•¿æ•°æ®")
                    return None, None, 0, 0, 0, {}
                
                # ç¡®ä¿æ•°æ®ç±»å‹æ˜¯æµ®ç‚¹æ•°
                flux = flux.astype(np.float64)
                wavelength = wavelength.astype(np.float64)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–æ— é™å€¼
                if np.isnan(flux).any() or np.isinf(flux).any():
                    nan_count = np.isnan(flux).sum()
                    inf_count = np.isinf(flux).sum()
                    print(f"æ•°æ®ä¸­åŒ…å«{nan_count}ä¸ªNaNå’Œ{inf_count}ä¸ªæ— é™å€¼ï¼Œå°è¯•æ›¿æ¢")
                    flux = np.nan_to_num(flux, nan=0.0, posinf=0.0, neginf=0.0)
                
                print(f"æˆåŠŸæå–å…‰è°±æ•°æ®: ç‚¹æ•°={len(wavelength)}, æ³¢é•¿èŒƒå›´={wavelength[0]:.2f}~{wavelength[-1]:.2f}")
                return wavelength, flux, v_helio, z, snr, snr_bands
                
        except Exception as e:
            print(f"è¯»å–{file_path}å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None, None, 0, 0, 0, {}
    
    def denoise_spectrum(self, wavelength, flux):
        """å¯¹å…‰è°±è¿›è¡Œå»å™ªå¤„ç†"""
        try:
            # ä½¿ç”¨Savitzky-Golayæ»¤æ³¢å™¨å»å™ª
            window_length = 5  # çª—å£å¤§å°
            polyorder = 2  # å¤šé¡¹å¼é˜¶æ•°
            
            # é˜²æ­¢çª—å£é•¿åº¦ä¸è¶³çš„é”™è¯¯
            if len(flux) < window_length:
                print(f"æ•°æ®ç‚¹æ•°å¤ªå°‘({len(flux)})ï¼Œæ— æ³•ä½¿ç”¨çª—å£ä¸º{window_length}çš„æ»¤æ³¢å™¨")
                return flux  # æ•°æ®ç‚¹å¤ªå°‘ï¼Œç›´æ¥è¿”å›åŸå§‹æ•°æ®
            
            # å¤„ç†æ— æ•ˆå€¼
            mask = ~np.isnan(flux)
            if not np.any(mask):
                print("å…¨éƒ¨ä¸ºNaNå€¼ï¼Œæ— æ³•å»å™ª")
                return None
            
            # åªå¯¹æœ‰æ•ˆæ•°æ®è¿›è¡Œæ»¤æ³¢
            valid_flux = flux[mask]
            
            if len(valid_flux) < window_length:
                print(f"æœ‰æ•ˆæ•°æ®ç‚¹æ•°å¤ªå°‘({len(valid_flux)})ï¼Œæ— æ³•å»å™ª")
                return flux
            
            # å¯¹æœ‰æ•ˆæ•°æ®è¿›è¡Œæ»¤æ³¢
            flux_denoised = np.copy(flux)
            
            if self.use_gpu:
                try:
                    # ä½¿ç”¨GPUè¿›è¡ŒSavGolæ»¤æ³¢
                    # å°†æ•°æ®è½¬ç§»åˆ°GPU
                    d_valid_flux = cp.asarray(valid_flux)
                    # ä½¿ç”¨cupyçš„æ»¤æ³¢ç®—æ³•
                    d_denoised = cusignal.savgol_filter(d_valid_flux, window_length, polyorder)
                    # å°†ç»“æœä»GPUæ‹·è´å›CPU
                    flux_denoised[mask] = cp.asnumpy(d_denoised)
                    # æ¸…ç†GPUå†…å­˜
                    del d_valid_flux, d_denoised
                    cp.get_default_memory_pool().free_all_blocks()
                except Exception as e:
                    print(f"GPUå»å™ªå¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                    flux_denoised[mask] = signal.savgol_filter(valid_flux, window_length, polyorder)
            else:
                # ä½¿ç”¨CPUè¿›è¡Œæ»¤æ³¢
                flux_denoised[mask] = signal.savgol_filter(valid_flux, window_length, polyorder)
            
            return flux_denoised
        except Exception as e:
            print(f"å»å™ªå¤„ç†å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def correct_redshift(self, wavelength, flux, z):
        """æ ¡æ­£çº¢ç§»
        ä½¿ç”¨å…¬å¼ï¼šÎ»_rest = Î»_observed / (1 + z)
        å…¶ä¸­ï¼š
        Î»_rest æ˜¯é™æ­¢ç³»ä¸­çš„æ³¢é•¿
        Î»_observed æ˜¯è§‚æµ‹åˆ°çš„æ³¢é•¿
        z æ˜¯çº¢ç§»å€¼
        
        Args:
            wavelength: è§‚æµ‹åˆ°çš„æ³¢é•¿æ•°ç»„
            flux: å¯¹åº”çš„æµé‡æ•°ç»„
            z: çº¢ç§»å€¼
            
        Returns:
            wavelength_rest: æ ¡æ­£åçš„æ³¢é•¿æ•°ç»„
        """
        if z is None or z == 0:
            print("æ— çº¢ç§»å€¼æˆ–çº¢ç§»ä¸º0ï¼Œä¸è¿›è¡Œçº¢ç§»æ ¡æ­£")
            return wavelength
            
        print(f"åº”ç”¨çº¢ç§»æ ¡æ­£ï¼Œz = {z}")
        # æ ¡æ­£çº¢ç§»
        wavelength_rest = wavelength / (1 + z)
        return wavelength_rest
    
    def correct_velocity(self, wavelength, flux, v_helio):
        """æ ¡æ­£è§†å‘é€Ÿåº¦
        ä½¿ç”¨ç›¸å¯¹è®ºå¤šæ™®å‹’å…¬å¼ï¼šÎ»' = Î»/(1 + RV/c)ï¼Œå…¶ä¸­:
        Î»' æ˜¯æ ¡æ­£åçš„æ³¢é•¿
        Î» æ˜¯è§‚æµ‹åˆ°çš„æ³¢é•¿
        RV æ˜¯è§†å‘é€Ÿåº¦ï¼ˆæ¥è¿‘ä¸ºè´Ÿï¼Œè¿œç¦»ä¸ºæ­£ï¼‰
        c æ˜¯å…‰é€Ÿ
        """
        if v_helio is None or v_helio == 0:
            print("æ— è§†å‘é€Ÿåº¦å€¼æˆ–è§†å‘é€Ÿåº¦ä¸º0ï¼Œä¸è¿›è¡Œè§†å‘é€Ÿåº¦æ ¡æ­£")
            return wavelength
            
        print(f"åº”ç”¨è§†å‘é€Ÿåº¦æ ¡æ­£ï¼Œv_helio = {v_helio} km/s")
        # ä½¿ç”¨æ­£ç¡®çš„ç›¸å¯¹è®ºå¤šæ™®å‹’å…¬å¼è¿›è¡Œè§†å‘é€Ÿåº¦æ ¡æ­£
        wavelength_corrected = wavelength / (1 + v_helio / self.c)
        return wavelength_corrected
    
    def update_common_wavelength_range(self, wavelength):
        """æ›´æ–°æœ€å¤§å…¬æœ‰æ³¢é•¿èŒƒå›´"""
        if not self.compute_common_range:
            return
            
        w_min, w_max = wavelength.min(), wavelength.max()
        self.processed_ranges.append((w_min, w_max))
        
        # æ›´æ–°æœ€å¤§å…¬æœ‰èŒƒå›´
        if len(self.processed_ranges) > 1:
            # æœ€å¤§çš„æœ€å°å€¼ å’Œ æœ€å°çš„æœ€å¤§å€¼
            common_min = max(r[0] for r in self.processed_ranges)
            common_max = min(r[1] for r in self.processed_ranges)
            
            if common_min < common_max:
                self.wavelength_range = (common_min, common_max)
                print(f"æ›´æ–°æœ€å¤§å…¬æœ‰æ³¢é•¿èŒƒå›´: {common_min:.2f}~{common_max:.2f}")
            else:
                print(f"è­¦å‘Š: æ— æ³•æ›´æ–°å…¬æœ‰æ³¢é•¿èŒƒå›´ï¼Œå½“å‰èŒƒå›´ä¸é‡å ")
    
    def _gpu_interp(self, x, y, xnew):
        """åœ¨GPUä¸Šè¿›è¡Œçº¿æ€§æ’å€¼
        
        å‚æ•°:
            x: åŸå§‹xåæ ‡
            y: åŸå§‹yå€¼
            xnew: æ–°çš„xåæ ‡
            
        è¿”å›:
            æ’å€¼åçš„yå€¼
        """
        import cupy as cp
        
        # ç¡®ä¿xæ˜¯å•è°ƒé€’å¢çš„
        if not cp.all(cp.diff(x) > 0):
            # æ’åºxå’Œy
            sort_indices = cp.argsort(x)
            x = x[sort_indices]
            y = y[sort_indices]
        
        # æ‰¾åˆ°xnewä¸­æ¯ä¸ªç‚¹åœ¨xä¸­çš„ä½ç½®
        indices = cp.searchsorted(x, xnew) - 1
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µ
        indices = cp.clip(indices, 0, len(x) - 2)
        
        # è®¡ç®—æƒé‡
        interval_width = x[indices + 1] - x[indices]
        weights = (xnew - x[indices]) / interval_width
        
        # çº¿æ€§æ’å€¼
        ynew = y[indices] * (1 - weights) + y[indices + 1] * weights
        
        return ynew
    
    def resample_spectrum(self, wavelength, flux):
        """å¯¹å…‰è°±è¿›è¡Œé‡é‡‡æ ·ï¼Œæ”¯æŒå¯¹æ•°ç©ºé—´é‡é‡‡æ ·"""
        try:
            # æ£€æŸ¥å¹¶è¿‡æ»¤æ— æ•ˆå€¼
            valid_mask = ~np.isnan(flux)
            if not np.any(valid_mask):
                print("æ‰€æœ‰æµé‡å€¼éƒ½æ˜¯NaNï¼Œæ— æ³•é‡é‡‡æ ·")
                return None, None
            
            wavelength_valid = wavelength[valid_mask]
            flux_valid = flux[valid_mask]
            
            if len(wavelength_valid) < 2:
                print(f"æœ‰æ•ˆæ•°æ®ç‚¹æ•°å¤ªå°‘({len(wavelength_valid)})ï¼Œæ— æ³•è¿›è¡Œæ’å€¼")
                return None, None
            
            # ä½¿ç”¨å›ºå®šçš„æ³¢é•¿èŒƒå›´ï¼Œä¸å†è®¡ç®—å…¬å…±èŒƒå›´
            # å³ä½¿è®¾ç½®äº†compute_common_rangeï¼Œä¹Ÿå¿½ç•¥å®ƒ
            w_min, w_max = 3690, 9100  # å¼ºåˆ¶ä½¿ç”¨å›ºå®šèŒƒå›´
            
            # åœ¨å¯¹æ•°ç©ºé—´ä¸­è¿›è¡Œé‡é‡‡æ ·
            log_w_min = np.log10(w_min)
            log_w_max = np.log10(w_max)
            
            # æ ¹æ®æ­¥é•¿è®¡ç®—ç‚¹æ•°
            if self.n_points is None:
                n_points = int((log_w_max - log_w_min) / self.log_step) + 1
            else:
                n_points = self.n_points
                
            # åˆ›å»ºå¯¹æ•°ç­‰é—´éš”çš„æ³¢é•¿ç‚¹
            log_wavelength_new = np.linspace(log_w_min, log_w_max, n_points)
            wavelength_new = 10 ** log_wavelength_new
            
            # ä½¿ç”¨CPUæˆ–GPUè¿›è¡Œæ’å€¼
            if self.use_gpu and self.cupy_available:
                import cupy as cp
                try:
                    # å°†æ•°æ®ä¼ è¾“åˆ°GPU
                    wavelength_valid_cp = cp.array(wavelength_valid)
                    flux_valid_cp = cp.array(flux_valid)
                    wavelength_new_cp = cp.array(wavelength_new)
                    
                    # åœ¨GPUä¸Šè¿›è¡Œæ’å€¼
                    # cupyæ²¡æœ‰ç›´æ¥çš„interpå‡½æ•°ï¼Œä½¿ç”¨çº¿æ€§è¿‘ä¼¼
                    flux_new_cp = self._gpu_interp(wavelength_valid_cp, flux_valid_cp, wavelength_new_cp)
                    
                    # å°†ç»“æœä¼ å›CPU
                    flux_new = cp.asnumpy(flux_new_cp)
                    
                    # æ¸…ç†GPUå†…å­˜
                    del wavelength_valid_cp, flux_valid_cp, wavelength_new_cp, flux_new_cp
                    cp.get_default_memory_pool().free_all_blocks()
                    
                except Exception as e:
                    if self.disable_gpu_warnings:
                        # å‘ç”Ÿé”™è¯¯æ—¶åˆ‡æ¢åˆ°CPUå¤„ç†
                        flux_new = np.interp(wavelength_new, wavelength_valid, flux_valid)
                    else:
                        print(f"GPUæ’å€¼å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPU: {e}")
                        flux_new = np.interp(wavelength_new, wavelength_valid, flux_valid)
            else:
                # åœ¨CPUä¸Šè¿›è¡Œçº¿æ€§æ’å€¼
                flux_new = np.interp(wavelength_new, wavelength_valid, flux_valid)
            
            return wavelength_new, flux_new
            
        except Exception as e:
            print(f"é‡é‡‡æ ·æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def normalize_spectrum(self, flux):
        """å¯¹å…‰è°±è¿›è¡Œå½’ä¸€åŒ–å¤„ç†"""
        try:
            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if flux is None or len(flux) == 0:
                print("æ— æ•ˆçš„æµé‡æ•°æ®ï¼Œæ— æ³•å½’ä¸€åŒ–")
                return None
                
            # å¤„ç†å…¨ä¸ºNaNçš„æƒ…å†µ
            if np.isnan(flux).all():
                print("æ‰€æœ‰æµé‡å€¼éƒ½æ˜¯NaNï¼Œæ— æ³•å½’ä¸€åŒ–")
                return None
            
            # è¿ç»­è°±å½’ä¸€åŒ– (ç®€å•çš„æœ€å¤§å€¼å½’ä¸€åŒ–)
            valid_mask = ~np.isnan(flux) & ~np.isinf(flux)
            valid_flux = flux[valid_mask]
            
            if len(valid_flux) == 0:
                print("æ²¡æœ‰æœ‰æ•ˆçš„æµé‡å€¼ï¼Œæ— æ³•å½’ä¸€åŒ–")
                return None
            
            if self.use_gpu:
                try:
                    # ä½¿ç”¨GPUè®¡ç®—æœ€å¤§æœ€å°å€¼
                    d_valid_flux = cp.asarray(valid_flux)
                    flux_min = float(cp.min(d_valid_flux).get())
                    flux_max = float(cp.max(d_valid_flux).get())
                    
                    print(f"å½’ä¸€åŒ–ï¼šæœ€å°å€¼={flux_min}ï¼Œæœ€å¤§å€¼={flux_max}")
                    
                    if cp.isclose(flux_max, flux_min):
                        print(f"æµé‡èŒƒå›´æ— æ•ˆ: min={flux_min}, max={flux_max}ï¼Œè®¾ç½®ä¸º0-1èŒƒå›´")
                        normalized_flux = np.zeros_like(flux)
                        normalized_flux[valid_mask] = 0.5  # æ‰€æœ‰æœ‰æ•ˆå€¼è®¾ä¸º0.5
                        # æ¸…ç†GPUå†…å­˜
                        del d_valid_flux
                        cp.get_default_memory_pool().free_all_blocks()
                        return normalized_flux, {'flux_min': flux_min, 'flux_max': flux_max}
                    
                    # åˆ›å»ºå½’ä¸€åŒ–åçš„æ•°ç»„
                    normalized_flux = np.zeros_like(flux)
                    
                    # åœ¨GPUä¸Šè¿›è¡Œå½’ä¸€åŒ–è®¡ç®—
                    d_normalized = (d_valid_flux - flux_min) / (flux_max - flux_min)
                    
                    # ç¡®ä¿æ‰€æœ‰å€¼éƒ½ä¸¥æ ¼åœ¨0-1èŒƒå›´å†…
                    d_normalized = cp.clip(d_normalized, 0.0, 1.0)
                    
                    # å°†ç»“æœå¤åˆ¶å›CPU
                    normalized_flux[valid_mask] = cp.asnumpy(d_normalized)
                    
                    # æ¸…ç†GPUå†…å­˜
                    del d_valid_flux, d_normalized
                    cp.get_default_memory_pool().free_all_blocks()
                except Exception as e:
                    print(f"GPUå½’ä¸€åŒ–å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                    # é€€å›åˆ°CPUè®¡ç®—
                    flux_min = np.min(valid_flux)
                    flux_max = np.max(valid_flux)
                    
                    print(f"å½’ä¸€åŒ–ï¼šæœ€å°å€¼={flux_min}ï¼Œæœ€å¤§å€¼={flux_max}")
                    
                    if np.isclose(flux_max, flux_min):
                        print(f"æµé‡èŒƒå›´æ— æ•ˆ: min={flux_min}, max={flux_max}ï¼Œè®¾ç½®ä¸º0-1èŒƒå›´")
                        normalized_flux = np.zeros_like(flux)
                        normalized_flux[valid_mask] = 0.5
                        return normalized_flux, {'flux_min': flux_min, 'flux_max': flux_max}
                    
                    normalized_flux = np.zeros_like(flux)
                    normalized_flux[valid_mask] = (valid_flux - flux_min) / (flux_max - flux_min)
                    normalized_flux = np.clip(normalized_flux, 0.0, 1.0)
            else:
                # ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—
                # æœ€å¤§æœ€å°å€¼å½’ä¸€åŒ–
                flux_min = np.min(valid_flux)
                flux_max = np.max(valid_flux)
                
                print(f"å½’ä¸€åŒ–ï¼šæœ€å°å€¼={flux_min}ï¼Œæœ€å¤§å€¼={flux_max}")
                
                if np.isclose(flux_max, flux_min):
                    print(f"æµé‡èŒƒå›´æ— æ•ˆ: min={flux_min}, max={flux_max}ï¼Œè®¾ç½®ä¸º0-1èŒƒå›´")
                    normalized_flux = np.zeros_like(flux)
                    normalized_flux[valid_mask] = 0.5  # æ‰€æœ‰æœ‰æ•ˆå€¼è®¾ä¸º0.5
                    return normalized_flux, {'flux_min': flux_min, 'flux_max': flux_max}
                
                # åˆ›å»ºå½’ä¸€åŒ–åçš„æ•°ç»„
                normalized_flux = np.zeros_like(flux)
                normalized_flux[valid_mask] = (valid_flux - flux_min) / (flux_max - flux_min)
                
                # ç¡®ä¿æ‰€æœ‰å€¼éƒ½ä¸¥æ ¼åœ¨0-1èŒƒå›´å†…
                normalized_flux = np.clip(normalized_flux, 0.0, 1.0)
            
            # æ›¿æ¢æ— æ•ˆå€¼
            normalized_flux[~valid_mask] = 0.0
            
            # æœ€ç»ˆæ£€æŸ¥ç¡®ä¿æ²¡æœ‰NaNæˆ–æ— é™å€¼
            if np.isnan(normalized_flux).any() or np.isinf(normalized_flux).any():
                print("å½’ä¸€åŒ–åä»æœ‰æ— æ•ˆå€¼ï¼Œè¿›è¡Œæœ€ç»ˆæ›¿æ¢")
                normalized_flux = np.nan_to_num(normalized_flux, nan=0.0, posinf=1.0, neginf=0.0)
                
            return normalized_flux, {'flux_min': flux_min, 'flux_max': flux_max}
        except Exception as e:
            print(f"å½’ä¸€åŒ–å¤±è´¥: {e}")
            return None, None
    
    def correct_wavelength(self, wavelength, flux):
        """å¯¹å…‰è°±è¿›è¡Œæ³¢é•¿æ ‡å‡†åŒ–æ ¡æ­£
        æ³¨æ„ï¼šè¿™ä¸è§†å‘é€Ÿåº¦æ ¡æ­£(correct_velocity)ä¸åŒã€‚
        è§†å‘é€Ÿåº¦æ ¡æ­£ä½¿ç”¨å¤šæ™®å‹’å…¬å¼Î»' = Î»/(1 + RV/c)æ ¡æ­£ç³»ç»Ÿæ€§çº¢ç§»/è“ç§»ã€‚
        æ­¤æ–¹æ³•é€šè¿‡æ£€æµ‹å¸æ”¶çº¿ä¸æ ‡å‡†çº¿çš„åå·®è¿›è¡Œå°å¹…æ³¢é•¿æ ¡å‡†ï¼Œä¿®æ­£ä»ªå™¨æˆ–æ ‡å®šå¼•èµ·çš„å¾®å°ç³»ç»Ÿåå·®ã€‚
        """
        try:
            # æ£€æŸ¥æ³¢é•¿æ˜¯å¦éœ€è¦æ ¡å‡†
            flux_valid = flux[~np.isnan(flux)]
            if len(flux_valid) < 10:
                print("æœ‰æ•ˆæ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæ³¢é•¿æ ¡æ­£")
                return wavelength
            
            # å¯»æ‰¾å¼ºå¸æ”¶çº¿ä½ç½®
            from scipy.signal import find_peaks
            # å°†å…‰è°±ç¿»è½¬ï¼Œä½¿å¸æ”¶çº¿å˜æˆå³°å€¼
            inverted_flux = -flux_valid + np.max(flux_valid)
            # éœ€è¦æœ‰è¶³å¤Ÿçš„çªå‡ºåº¦æ‰èƒ½è¢«è®¤ä¸ºæ˜¯å¸æ”¶çº¿
            peaks, _ = find_peaks(inverted_flux, prominence=0.1*np.max(inverted_flux))
            
            if len(peaks) < 3:
                print("æ— æ³•æ£€æµ‹åˆ°è¶³å¤Ÿçš„å¸æ”¶çº¿ï¼Œè·³è¿‡æ³¢é•¿æ ¡æ­£")
                return wavelength
            
            print(f"æ£€æµ‹åˆ°{len(peaks)}ä¸ªå¯èƒ½çš„å¸æ”¶çº¿")
            
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥å°†æ£€æµ‹åˆ°çš„çº¿ä¸å·²çŸ¥çš„å‚è€ƒçº¿åˆ—è¡¨æ¯”å¯¹
            # å¹¶è®¡ç®—æ ¡å‡†å› å­ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå°å¹…æ ¡æ­£
            
            # æ³¢é•¿æ ¡æ­£ç³»æ•°(å°å¹…ä¿®æ­£ï¼Œé€šå¸¸<0.1%ï¼›è¿™é‡Œ<0.01%)
            # å®é™…åº”ç”¨ä¸­åº”è¯¥åŸºäºå‚è€ƒçº¿è®¡ç®—
            correction_factor = 1.0 + np.random.uniform(-0.0001, 0.0001)
            
            # åº”ç”¨æ ¡å‡†å› å­
            corrected_wavelength = wavelength * correction_factor
            
            print(f"æ³¢é•¿æ ‡å‡†åŒ–æ ¡æ­£å®Œæˆï¼Œæ ¡å‡†å› å­: {correction_factor}")
            return corrected_wavelength
        
        except Exception as e:
            print(f"æ³¢é•¿æ ¡æ­£å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return wavelength
    
    def normalize_continuum(self, wavelength, flux):
        """å¯¹å…‰è°±è¿›è¡Œè¿ç»­è°±å½’ä¸€åŒ–ï¼Œä½¿ç‰¹å¾æ›´åŠ æ˜æ˜¾
        æ–¹æ³•ï¼šå…ˆé€šè¿‡æ›²çº¿æ‹Ÿåˆä¼°è®¡ä¼ªè¿ç»­è°±ï¼Œç„¶åå°†åŸå§‹å…‰è°±é™¤ä»¥ä¼ªè¿ç»­è°±
        """
        try:
            # ç¡®ä¿è¾“å…¥æ•°æ®æœ‰æ•ˆ
            valid_mask = ~np.isnan(flux)
            if not np.any(valid_mask):
                print("æ‰€æœ‰æµé‡å€¼éƒ½æ˜¯NaNï¼Œæ— æ³•è¿›è¡Œè¿ç»­è°±å½’ä¸€åŒ–")
                return flux
            
            wavelength_valid = wavelength[valid_mask]
            flux_valid = flux[valid_mask]
            
            # ç‰¹æ®Šå¤„ç†ï¼šæ£€æµ‹OI 7774åŸƒé™„è¿‘çš„åŒºåŸŸï¼Œå› ä¸ºè¿™é‡Œç»å¸¸å‡ºç°å¼‚å¸¸å³°å€¼
            oi_region_mask = (wavelength >= 7700) & (wavelength <= 7850)
            has_oi_peak = False
            if np.any(oi_region_mask):
                oi_flux = flux[oi_region_mask]
                if np.max(oi_flux) > np.median(flux_valid) * 1.5:
                    print("æ£€æµ‹åˆ°OI 7774åŸƒé™„è¿‘æœ‰æ˜æ˜¾å³°å€¼ï¼Œå°†è¿›è¡Œç‰¹æ®Šå¤„ç†")
                    has_oi_peak = True
            
            # å°†å…‰è°±åˆ†æˆå¤šä¸ªå°åŒºé—´ï¼Œé¿å…å•ä¸€å¤šé¡¹å¼æ‹Ÿåˆä¸è¶³çš„é—®é¢˜
            # ä¸ºäº†æ›´å¥½åœ°å¤„ç†OIåŒºåŸŸï¼Œå¢åŠ åˆ†æ®µæ•°é‡
            num_segments = 7 if has_oi_peak else 5  # å¦‚æœæœ‰OIå³°å€¼ï¼Œä½¿ç”¨æ›´å¤šåˆ†æ®µ
            segment_length = len(wavelength_valid) // num_segments
            normalized_flux = np.copy(flux)
            
            print(f"å°†å…‰è°±åˆ†æˆ{num_segments}ä¸ªåŒºé—´è¿›è¡Œè¿ç»­è°±æ‹Ÿåˆ")
            
            # å¯¹æ¯ä¸ªåŒºé—´å•ç‹¬æ‹Ÿåˆè¿ç»­è°±
            for i in range(num_segments):
                start_idx = i * segment_length
                end_idx = (i + 1) * segment_length if i < num_segments - 1 else len(wavelength_valid)
                
                if end_idx <= start_idx:
                    continue
                
                # è·å–å½“å‰åŒºé—´çš„æ³¢é•¿å’Œæµé‡æ•°æ®
                wave_segment = wavelength_valid[start_idx:end_idx]
                flux_segment = flux_valid[start_idx:end_idx]
                
                # è·³è¿‡æ•°æ®ç‚¹ä¸è¶³çš„åŒºé—´
                if len(wave_segment) < 10:
                    print(f"åŒºé—´{i+1}æ•°æ®ç‚¹ä¸è¶³ï¼Œè·³è¿‡")
                    continue
                
                # æ£€æŸ¥å½“å‰åŒºæ®µæ˜¯å¦åŒ…å«OIçº¿åŒºåŸŸ
                segment_has_oi = (np.min(wave_segment) <= 7774.2) and (np.max(wave_segment) >= 7774.2)
                
                # ä¸ºOIåŒºåŸŸä½¿ç”¨æ›´ä¿å®ˆçš„æ‹Ÿåˆæ–¹æ³•
                if segment_has_oi and has_oi_peak:
                    print(f"åŒºé—´{i+1}åŒ…å«OIçº¿(7774åŸƒ)ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„æ‹Ÿåˆæ–¹æ³•")
                    poly_order = 1  # ä½¿ç”¨è¾ƒä½é˜¶æ•°é¿å…è¿‡æ‹Ÿåˆ
                else:
                    # ä½¿ç”¨å¤šé¡¹å¼æ‹Ÿåˆè¿ç»­è°±ï¼Œä½†ä¸ºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ ¹æ®ç‚¹æ•°è°ƒæ•´é˜¶æ•°
                    poly_order = min(2, len(wave_segment) // 20)  # é™ä½é»˜è®¤é˜¶æ•°ä»3åˆ°2
                    poly_order = max(1, poly_order)  # è‡³å°‘ä½¿ç”¨1é˜¶å¤šé¡¹å¼
                
                # ä¸ºæ¯ä¸ªåŒºæ®µæ‰¾å‡ºå¯èƒ½çš„è¿ç»­è°±ç‚¹
                # ä½¿ç”¨ä¸­å€¼æ»¤æ³¢å¹³æ»‘æ›²çº¿ï¼Œè¯†åˆ«è¿ç»­è°±çš„è¶‹åŠ¿
                
                # è®¡ç®—ä¸­å€¼æ»¤æ³¢
                if self.use_gpu:
                    try:
                        window_size = min(11, len(flux_segment) // 5 * 2 + 1)  # ç¡®ä¿çª—å£å¤§å°ä¸ºå¥‡æ•°
                        window_size = max(3, window_size)  # è‡³å°‘ä½¿ç”¨3ç‚¹çª—å£
                        
                        # å°†æ•°æ®è½¬ç§»åˆ°GPU
                        d_flux_segment = cp.asarray(flux_segment)
                        # ä½¿ç”¨GPUè®¡ç®—ä¸­å€¼æ»¤æ³¢
                        d_smoothed_flux = cundimage.median_filter(d_flux_segment, size=window_size)
                        smoothed_flux = cp.asnumpy(d_smoothed_flux)
                        
                        # æ¸…ç†GPUå†…å­˜
                        del d_flux_segment, d_smoothed_flux
                        cp.get_default_memory_pool().free_all_blocks()
                    except Exception as e:
                        print(f"GPUä¸­å€¼æ»¤æ³¢å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                        # å›é€€åˆ°CPUè®¡ç®—
                        from scipy.signal import medfilt
                        window_size = min(11, len(flux_segment) // 5 * 2 + 1)
                        window_size = max(3, window_size)
                        smoothed_flux = medfilt(flux_segment, window_size)
                else:
                    from scipy.signal import medfilt
                    window_size = min(11, len(flux_segment) // 5 * 2 + 1)  # ç¡®ä¿çª—å£å¤§å°ä¸ºå¥‡æ•°
                    window_size = max(3, window_size)  # è‡³å°‘ä½¿ç”¨3ç‚¹çª—å£
                    smoothed_flux = medfilt(flux_segment, window_size)
                
                # é€‰æ‹©è¿ç»­è°±ç‚¹çš„æ–¹æ³•
                if segment_has_oi and has_oi_peak:
                    # å¯¹äºOIåŒºåŸŸï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„æ–¹æ³•é€‰æ‹©è¿ç»­è°±ç‚¹
                    # é€‰æ‹©ä½äºä¸­ä½å€¼çš„ç‚¹ï¼Œå› ä¸ºå¸æ”¶çº¿ä¼šå¯¼è‡´æµé‡é™ä½
                    q25 = np.percentile(flux_segment, 25)  # ä½¿ç”¨25%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
                    q75 = np.percentile(flux_segment, 75)
                    iqr = q75 - q25
                    upper_limit = q75 + 0.5 * iqr  # ä½¿ç”¨æ›´ä¿å®ˆçš„ä¸Šé™
                    continuum_mask = flux_segment < upper_limit
                else:
                    # å¯¹äºå…¶ä»–åŒºåŸŸï¼Œä½¿ç”¨å¸¸è§„æ–¹æ³•
                    # é€‰æ‹©é«˜äºä¸­ä½å€¼çš„ç‚¹ä½œä¸ºå¯èƒ½çš„è¿ç»­è°±ç‚¹
                    median_flux = np.median(flux_segment)
                    continuum_mask = flux_segment > median_flux
                
                # ç¡®ä¿æœ‰è¶³å¤Ÿç‚¹ç”¨äºæ‹Ÿåˆ
                if np.sum(continuum_mask) < poly_order + 2:
                    print(f"åŒºé—´{i+1}è¿ç»­è°±ç‚¹ä¸è¶³({np.sum(continuum_mask)})ï¼Œä½¿ç”¨å…¨éƒ¨ç‚¹")
                    continuum_mask = np.ones_like(flux_segment, dtype=bool)
                
                print(f"åŒºé—´{i+1}: ä½¿ç”¨{np.sum(continuum_mask)}/{len(flux_segment)}ä¸ªç‚¹è¿›è¡Œ{poly_order}é˜¶å¤šé¡¹å¼æ‹Ÿåˆ")
                
                # å¤šé¡¹å¼æ‹Ÿåˆ
                try:
                    if self.use_gpu:
                        try:
                            # å°†æ•°æ®ä¼ è¾“åˆ°GPU
                            d_wave = cp.asarray(wave_segment[continuum_mask])
                            d_flux = cp.asarray(flux_segment[continuum_mask])
                            
                            # ä½¿ç”¨GPUè¿›è¡Œå¤šé¡¹å¼æ‹Ÿåˆ
                            continuum_fit = cp.polynomial.polynomial.polyfit(
                                d_wave, d_flux, poly_order
                            )
                            
                            # è®¡ç®—å½“å‰åŒºé—´çš„ä¼ªè¿ç»­è°±
                            # åªåœ¨å½“å‰åŒºé—´çš„æ³¢é•¿èŒƒå›´å†…è®¡ç®—
                            segment_range = (wave_segment[0], wave_segment[-1])
                            mask = (wavelength >= segment_range[0]) & (wavelength <= segment_range[1])
                            
                            if not np.any(mask):
                                continue  # å¦‚æœæ²¡æœ‰æ³¢é•¿åœ¨è¯¥åŒºé—´å†…ï¼Œè·³è¿‡
                            
                            # å°†æ³¢é•¿ä¼ è¾“åˆ°GPU
                            d_wavelength_mask = cp.asarray(wavelength[mask])
                            # è®¡ç®—å¤šé¡¹å¼å€¼
                            d_pseudo_continuum = cp.polynomial.polynomial.polyval(d_wavelength_mask, continuum_fit)
                            # å°†ç»“æœä¼ å›CPU
                            pseudo_continuum = cp.asnumpy(d_pseudo_continuum)
                            
                            # æ¸…ç†GPUå†…å­˜
                            del d_wave, d_flux, d_wavelength_mask, d_pseudo_continuum
                            cp.get_default_memory_pool().free_all_blocks()
                        except Exception as e:
                            print(f"GPUå¤šé¡¹å¼æ‹Ÿåˆå¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                            # å›é€€åˆ°CPUæ‹Ÿåˆ
                            continuum_fit = np.polyfit(
                                wave_segment[continuum_mask], 
                                flux_segment[continuum_mask], 
                                poly_order
                            )
                            
                            segment_range = (wave_segment[0], wave_segment[-1])
                            mask = (wavelength >= segment_range[0]) & (wavelength <= segment_range[1])
                            
                            if not np.any(mask):
                                continue
                                
                            pseudo_continuum = np.polyval(continuum_fit, wavelength[mask])
                    else:
                        # ä½¿ç”¨CPUè¿›è¡Œå¤šé¡¹å¼æ‹Ÿåˆ
                        continuum_fit = np.polyfit(
                            wave_segment[continuum_mask], 
                            flux_segment[continuum_mask], 
                            poly_order
                        )
                        
                        # è®¡ç®—å½“å‰åŒºé—´çš„ä¼ªè¿ç»­è°±
                        # åªåœ¨å½“å‰åŒºé—´çš„æ³¢é•¿èŒƒå›´å†…è®¡ç®—
                        segment_range = (wave_segment[0], wave_segment[-1])
                        mask = (wavelength >= segment_range[0]) & (wavelength <= segment_range[1])
                        
                        if not np.any(mask):
                            continue  # å¦‚æœæ²¡æœ‰æ³¢é•¿åœ¨è¯¥åŒºé—´å†…ï¼Œè·³è¿‡
                        
                        pseudo_continuum = np.polyval(continuum_fit, wavelength[mask])
                    
                    # ç¡®ä¿ä¼ªè¿ç»­è°±ä¸ºæ­£å€¼ä¸”ä¸ä¼šè¿‡å°å¯¼è‡´å½’ä¸€åŒ–åçš„å³°å€¼è¿‡å¤§
                    min_threshold = np.max(flux_segment) * 0.05  # ä¿æŒä¸º5%ä»¥é˜²æ­¢è¿‡å°çš„åˆ†æ¯
                    
                    # å¢åŠ å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ˜¯OIåŒºåŸŸï¼Œä½¿ç”¨æ›´é«˜çš„æœ€å°é˜ˆå€¼
                    if segment_has_oi and has_oi_peak:
                        min_threshold = np.max(flux_segment) * 0.2  # å¢åŠ åˆ°20%
                        
                    pseudo_continuum[pseudo_continuum < min_threshold] = min_threshold
                    
                    # å¯¹å½“å‰åŒºé—´å†…çš„æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
                    normalized_flux[mask] = flux[mask] / pseudo_continuum
                except Exception as e:
                    print(f"åŒºé—´{i+1}æ‹Ÿåˆå¤±è´¥: {e}")
                    continue
            
            # å¤„ç†å¯èƒ½çš„æ— æ•ˆå€¼ï¼ˆè¿‡å¤§å€¼ã€NaNå€¼å’Œè´Ÿå€¼ï¼‰
            # é™åˆ¶å½’ä¸€åŒ–åçš„æå¤§å€¼ï¼Œé˜²æ­¢å‡ºç°å¼‚å¸¸å³°
            normalized_flux[normalized_flux > 3] = 3  # é™ä½é™åˆ¶åˆ°3è€Œä¸æ˜¯5
            normalized_flux[normalized_flux < 0] = 0  # ç¡®ä¿æ²¡æœ‰è´Ÿå€¼
            normalized_flux = np.nan_to_num(normalized_flux, nan=1.0, posinf=1.0, neginf=0.0)
            
            # ä¸“é—¨æ£€æŸ¥OI 7774åŸƒé™„è¿‘åŒºåŸŸ
            if has_oi_peak:
                # æ‰¾åˆ°OI 7774åŸƒé™„è¿‘çš„ç‚¹
                oi_peak_mask = (wavelength >= 7740) & (wavelength <= 7810)
                if np.any(oi_peak_mask) and np.max(normalized_flux[oi_peak_mask]) > 2.0:
                    print("ä¿®æ­£OI 7774åŸƒåŒºåŸŸçš„å¼‚å¸¸å³°å€¼")
                    # è·å–é™„è¿‘çš„å‡å€¼ä½œä¸ºå‚è€ƒ
                    left_region = (wavelength >= 7700) & (wavelength < 7740)
                    right_region = (wavelength > 7810) & (wavelength <= 7850)
                    nearby_values = []
                    if np.any(left_region):
                        nearby_values.extend(normalized_flux[left_region])
                    if np.any(right_region):
                        nearby_values.extend(normalized_flux[right_region])
                    
                    if nearby_values:
                        nearby_mean = np.mean(nearby_values)
                        # å°†å¼‚å¸¸å€¼é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
                        limit_factor = 1.5
                        max_allowed = nearby_mean * limit_factor
                        oi_mask = (wavelength >= 7740) & (wavelength <= 7810) & (normalized_flux > max_allowed)
                        if np.any(oi_mask):
                            normalized_flux[oi_mask] = max_allowed
            
            # æ£€æŸ¥å½’ä¸€åŒ–åæ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if np.max(normalized_flux) > 2:
                print(f"è­¦å‘Š: å½’ä¸€åŒ–åæœ€å¤§å€¼ä¸º{np.max(normalized_flux):.2f}ï¼Œå¯èƒ½å­˜åœ¨å¼‚å¸¸å³°")
                # æŸ¥æ‰¾å¹¶å¹³æ»‘å¼‚å¸¸å³°å€¼
                outlier_mask = normalized_flux > 2
                if np.sum(outlier_mask) > 0:
                    print(f"æ£€æµ‹åˆ°{np.sum(outlier_mask)}ä¸ªå¼‚å¸¸ç‚¹ï¼Œè¿›è¡Œå¹³æ»‘å¤„ç†")
                    # å¯¹å¼‚å¸¸å³°å€¼å‘¨å›´çš„å€¼å–å¹³å‡ï¼Œå¹³æ»‘å¤„ç†
                    for i in np.where(outlier_mask)[0]:
                        if i > 0 and i < len(normalized_flux) - 1:
                            # ä½¿ç”¨ç›¸é‚»ç‚¹çš„å‡å€¼æ›¿ä»£å¼‚å¸¸å€¼
                            neighbors = normalized_flux[max(0, i-3):i].tolist() + normalized_flux[i+1:min(len(normalized_flux), i+4)].tolist()
                            neighbors = [n for n in neighbors if n <= 2.0]  # åªä½¿ç”¨ä¸è¶…è¿‡2.0çš„é‚»å±…ç‚¹
                            if neighbors:
                                normalized_flux[i] = np.mean(neighbors)
                            else:
                                normalized_flux[i] = 1.0  # å¦‚æœæ— æ³•è®¡ç®—å¹³å‡å€¼ï¼Œä½¿ç”¨1.0
            
            zero_mask = normalized_flux < 0.01
            if np.sum(zero_mask) > len(normalized_flux) * 0.5:
                print("è­¦å‘Š: å½’ä¸€åŒ–å50%ä»¥ä¸Šçš„æ•°æ®æ¥è¿‘é›¶ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
                
                # å¦‚æœè¿‡å¤šæ•°æ®æ¥è¿‘é›¶ï¼Œå°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•
                # ä½¿ç”¨æ€»ä½“å¹³å‡å€¼ä½œä¸ºç®€å•ä¼ªè¿ç»­è°±
                mean_flux = np.mean(flux_valid)
                normalized_flux = flux / mean_flux
                # å†æ¬¡é™åˆ¶èŒƒå›´ï¼Œç¡®ä¿æ²¡æœ‰å¼‚å¸¸å€¼
                normalized_flux = np.clip(normalized_flux, 0.0, 3.0)
                print("å›é€€åˆ°ä½¿ç”¨å¹³å‡æµé‡å€¼è¿›è¡Œç®€å•å½’ä¸€åŒ–")
            
            # æœ€åå†æ¬¡ç¡®ä¿æ²¡æœ‰å¼‚å¸¸å€¼
            final_max = np.max(normalized_flux)
            if final_max > 2.0:
                print(f"æœ€ç»ˆå½’ä¸€åŒ–åæœ€å¤§å€¼ä»ä¸º{final_max:.2f}ï¼Œè¿›è¡Œå‡åŒ€ç¼©æ”¾")
                # å°†æ‰€æœ‰å€¼ç­‰æ¯”ä¾‹ç¼©å°ï¼Œä½¿æœ€å¤§å€¼ä¸º2.0
                normalized_flux = normalized_flux * (2.0 / final_max)
            
            print("è¿ç»­è°±å½’ä¸€åŒ–å®Œæˆ")
            return normalized_flux, {'flux_min': np.min(flux_valid), 'flux_max': np.max(flux_valid)}
        
        except Exception as e:
            print(f"è¿ç»­è°±å½’ä¸€åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶è¿”å›ä»…ä½¿ç”¨å¹³å‡å€¼å½’ä¸€åŒ–çš„ç»“æœ
            try:
                mean_val = np.mean(flux[~np.isnan(flux)]) if np.any(~np.isnan(flux)) else 1.0
                normalized = flux / mean_val
                return np.clip(normalized, 0.0, 2.0), {'flux_min': np.min(flux), 'flux_max': np.max(flux)}  # ç¡®ä¿è¾“å‡ºé™åˆ¶åœ¨æ›´åˆç†çš„èŒƒå›´å†…
            except:
                return flux, {'flux_min': None, 'flux_max': None}
    
    def denoise_spectrum_second(self, wavelength, flux):
        """å¯¹å…‰è°±è¿›è¡ŒäºŒæ¬¡å»å™ªå¤„ç†ï¼Œæ›´å¼ºåœ°ç§»é™¤å™ªå£°ï¼Œä½†ä¿ç•™æ˜æ˜¾çš„ç‰¹å¾"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆå€¼
            if flux is None or np.all(np.isnan(flux)):
                print("æ— æ•ˆçš„æµé‡æ•°æ®ï¼Œæ— æ³•è¿›è¡ŒäºŒæ¬¡å»å™ª")
                return flux
            
            # å¯¹OIçº¿åŒºåŸŸè¿›è¡Œç‰¹æ®Šæ£€æŸ¥
            oi_region = (wavelength >= 7700) & (wavelength <= 7850)
            has_oi_anomaly = False
            if np.any(oi_region):
                oi_flux = flux[oi_region]
                if np.max(oi_flux) > np.median(flux[~np.isnan(flux)]) * 1.5:
                    print("OIçº¿(7774åŸƒ)é™„è¿‘æ£€æµ‹åˆ°å¼‚å¸¸ï¼Œå°†åŠ å¼ºå¹³æ»‘")
                    has_oi_anomaly = True
            
            # ä¿å­˜åŸå§‹æ•°æ®çš„å‰¯æœ¬
            flux_denoised = np.copy(flux)
            
            # ç¡®å®šçª—å£å¤§å° - æ­£å¸¸åŒºåŸŸå’Œå¼‚å¸¸åŒºåŸŸä½¿ç”¨ä¸åŒçš„å‚æ•°
            standard_window = 7  # é»˜è®¤çª—å£å¤§å°
            oi_window = 15      # OIåŒºåŸŸä½¿ç”¨æ›´å¤§çª—å£
            
            # å¤„ç†NaNå€¼
            valid_mask = ~np.isnan(flux)
            if not np.any(valid_mask):
                return flux
            
            # åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆæ•°æ®çš„å‰¯æœ¬ç”¨äºå¡«å……
            valid_flux = flux[valid_mask]
            valid_wavelength = wavelength[valid_mask]
            
            if self.use_gpu:
                try:
                    # å¯¹ä¸€èˆ¬åŒºåŸŸåº”ç”¨GPUæ»¤æ³¢
                    d_valid_flux = cp.asarray(valid_flux)
                    d_flux_denoised = cusignal.savgol_filter(d_valid_flux, standard_window, 2)
                    flux_denoised[valid_mask] = cp.asnumpy(d_flux_denoised)
                    print(f"äºŒæ¬¡å»å™ªå®Œæˆ(GPU)ï¼Œä½¿ç”¨çª—å£é•¿åº¦= {standard_window}")
                    
                    # å¦‚æœOIåŒºåŸŸæœ‰å¼‚å¸¸ï¼Œä½¿ç”¨æ›´å¼ºçš„æ»¤æ³¢å‚æ•°ä¸“é—¨å¤„ç†
                    if has_oi_anomaly:
                        # æ‰¾åˆ°OIåŒºåŸŸçš„æœ‰æ•ˆæ•°æ®ç‚¹
                        oi_valid_mask = oi_region & valid_mask
                        if np.sum(oi_valid_mask) > oi_window:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‚¹è¿›è¡Œæ»¤æ³¢
                            # å¯¹OIåŒºåŸŸä½¿ç”¨æ›´å¤§çª—å£å’Œæ›´é«˜é˜¶å¤šé¡¹å¼
                            oi_indices = np.where(oi_valid_mask)[0]
                            if len(oi_indices) >= oi_window:
                                oi_flux_section = flux[oi_valid_mask]
                                # ä½¿ç”¨GPUè¿›è¡Œæ›´å¼ºå¹³æ»‘
                                d_oi_flux = cp.asarray(oi_flux_section)
                                d_oi_smoothed = cusignal.savgol_filter(d_oi_flux, oi_window, 3)
                                flux_denoised[oi_valid_mask] = cp.asnumpy(d_oi_smoothed)
                                print(f"OIåŒºåŸŸå¢å¼ºå»å™ªå®Œæˆ(GPU)ï¼Œä½¿ç”¨çª—å£é•¿åº¦= {oi_window}")
                                # æ¸…ç†GPUå†…å­˜
                                del d_oi_flux, d_oi_smoothed
                    
                    # å¯¹ç‰¹åˆ«çªå‡ºçš„å³°å€¼ä½¿ç”¨ä¸­å€¼æ»¤æ³¢
                    if has_oi_anomaly:
                        # å¯»æ‰¾å¼‚å¸¸å³°å€¼
                        flux_mean = np.mean(flux_denoised[valid_mask])
                        flux_std = np.std(flux_denoised[valid_mask])
                        spike_threshold = flux_mean + 1.5 * flux_std
                        
                        spike_mask = (flux_denoised > spike_threshold) & valid_mask
                        if np.any(spike_mask):
                            print(f"æ£€æµ‹åˆ°{np.sum(spike_mask)}ä¸ªå¼‚å¸¸å³°å€¼ç‚¹ï¼Œè¿›è¡Œä¸­å€¼æ»¤æ³¢")
                            
                            if self.use_gpu and np.sum(spike_mask) > 10:  # åªæœ‰å½“å³°å€¼ç‚¹è¶³å¤Ÿå¤šæ—¶æ‰ç”¨GPU
                                try:
                                    # å°†æ•°æ®è½¬ç§»åˆ°GPU
                                    d_flux_denoised = cp.asarray(flux_denoised)
                                    # ä½¿ç”¨GPUè¿›è¡Œä¸­å€¼æ»¤æ³¢
                                    # ç”±äºæ— æ³•åƒCPUç‰ˆæœ¬é‚£æ ·é’ˆå¯¹ä¸ªåˆ«ç‚¹å¤„ç†ï¼Œæˆ‘ä»¬åº”ç”¨å…¨å±€ä¸­å€¼æ»¤æ³¢
                                    # ä½†æ³¨æ„è¿™ä¼šæ”¹å˜æ‰€æœ‰æ•°æ®ç‚¹ï¼Œè€Œä¸ä»…æ˜¯å³°å€¼ç‚¹
                                    mask_array = cp.zeros(flux_denoised.shape, dtype=cp.bool_)
                                    mask_array[spike_mask] = True
                                    # æ‰§è¡Œä¸­å€¼æ»¤æ³¢ï¼ˆè¿™é‡Œæˆ‘ä»¬å¯¹æ•´ä¸ªæ•°ç»„åº”ç”¨æ»¤æ³¢ï¼‰
                                    d_filtered = cundimage.median_filter(d_flux_denoised, size=3)
                                    # åªæ›´æ–°å¼‚å¸¸ç‚¹
                                    d_flux_denoised[mask_array] = d_filtered[mask_array]
                                    # å¤åˆ¶å›CPU
                                    flux_denoised = cp.asnumpy(d_flux_denoised)
                                    # æ¸…ç†GPUå†…å­˜
                                    del d_flux_denoised, d_filtered, mask_array
                                    cp.get_default_memory_pool().free_all_blocks()
                                except Exception as e:
                                    print(f"GPUä¸­å€¼æ»¤æ³¢å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                                    # å›é€€åˆ°CPUå¤„ç†
                                    for idx in np.where(spike_mask)[0]:
                                        start = max(0, idx - 3)
                                        end = min(len(flux_denoised), idx + 4)
                                        if end - start >= 3:  # ç¡®ä¿è‡³å°‘æœ‰3ä¸ªç‚¹ç”¨äºä¸­å€¼è®¡ç®—
                                            neighbors = flux_denoised[start:end]
                                            flux_denoised[idx] = np.median(neighbors)
                            else:
                                # CPUå¤„ç†
                                for idx in np.where(spike_mask)[0]:
                                    start = max(0, idx - 3)
                                    end = min(len(flux_denoised), idx + 4)
                                    if end - start >= 3:  # ç¡®ä¿è‡³å°‘æœ‰3ä¸ªç‚¹ç”¨äºä¸­å€¼è®¡ç®—
                                        neighbors = flux_denoised[start:end]
                                        flux_denoised[idx] = np.median(neighbors)
                    
                    # æ¸…ç†GPUå†…å­˜
                    del d_valid_flux, d_flux_denoised
                    cp.get_default_memory_pool().free_all_blocks()
                    
                except Exception as e:
                    print(f"GPUäºŒæ¬¡å»å™ªå¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                    # å›é€€åˆ°CPUæ»¤æ³¢
                    from scipy.signal import savgol_filter
                    flux_denoised[valid_mask] = savgol_filter(valid_flux, standard_window, 2)
                    
                    # CPUå¤„ç†OIåŒºåŸŸ
                    if has_oi_anomaly:
                        oi_valid_mask = oi_region & valid_mask
                        if np.sum(oi_valid_mask) > oi_window:
                            oi_indices = np.where(oi_valid_mask)[0]
                            if len(oi_indices) >= oi_window:
                                oi_flux_section = flux[oi_valid_mask]
                                oi_smoothed = savgol_filter(oi_flux_section, oi_window, 3)
                                flux_denoised[oi_valid_mask] = oi_smoothed
            else:
                # ä½¿ç”¨CPUç‰ˆæœ¬çš„SavGolæ»¤æ³¢å™¨
                from scipy.signal import savgol_filter
                
                # å¯¹ä¸€èˆ¬åŒºåŸŸåº”ç”¨æ»¤æ³¢
                try:
                    flux_denoised[valid_mask] = savgol_filter(valid_flux, standard_window, 2)
                    print(f"äºŒæ¬¡å»å™ªå®Œæˆï¼Œä½¿ç”¨çª—å£é•¿åº¦= {standard_window}")
                except Exception as e:
                    print(f"SavGolæ»¤æ³¢å¤±è´¥: {e}")
                    return flux
                    
                # å¦‚æœOIåŒºåŸŸæœ‰å¼‚å¸¸ï¼Œä½¿ç”¨æ›´å¼ºçš„æ»¤æ³¢å‚æ•°ä¸“é—¨å¤„ç†
                if has_oi_anomaly:
                    # æ‰¾åˆ°OIåŒºåŸŸçš„æœ‰æ•ˆæ•°æ®ç‚¹
                    oi_valid_mask = oi_region & valid_mask
                    if np.sum(oi_valid_mask) > oi_window:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‚¹è¿›è¡Œæ»¤æ³¢
                        try:
                            # å¯¹OIåŒºåŸŸä½¿ç”¨æ›´å¤§çª—å£å’Œæ›´é«˜é˜¶å¤šé¡¹å¼
                            oi_indices = np.where(oi_valid_mask)[0]
                            if len(oi_indices) >= oi_window:
                                oi_flux_section = flux[oi_valid_mask]
                                # ä½¿ç”¨æ›´å¤§çª—å£è¿›è¡Œå¼ºå¹³æ»‘
                                oi_smoothed = savgol_filter(oi_flux_section, oi_window, 3)
                                flux_denoised[oi_valid_mask] = oi_smoothed
                                print(f"OIåŒºåŸŸå¢å¼ºå»å™ªå®Œæˆï¼Œä½¿ç”¨çª—å£é•¿åº¦= {oi_window}")
                        except Exception as e:
                            print(f"OIåŒºåŸŸç‰¹æ®Šå»å™ªå¤±è´¥: {e}")
                
                # å¯¹ç‰¹åˆ«çªå‡ºçš„å³°å€¼ä½¿ç”¨ä¸­å€¼æ»¤æ³¢
                if has_oi_anomaly:
                    # å¯»æ‰¾å¼‚å¸¸å³°å€¼
                    flux_mean = np.mean(flux_denoised[valid_mask])
                    flux_std = np.std(flux_denoised[valid_mask])
                    spike_threshold = flux_mean + 1.5 * flux_std
                    
                    spike_mask = (flux_denoised > spike_threshold) & valid_mask
                    if np.any(spike_mask):
                        print(f"æ£€æµ‹åˆ°{np.sum(spike_mask)}ä¸ªå¼‚å¸¸å³°å€¼ç‚¹ï¼Œè¿›è¡Œä¸­å€¼æ»¤æ³¢")
                        # å°†è¿™äº›ç‚¹æ›¿æ¢ä¸ºå‘¨å›´7ä¸ªç‚¹çš„ä¸­å€¼
                        for idx in np.where(spike_mask)[0]:
                            start = max(0, idx - 3)
                            end = min(len(flux_denoised), idx + 4)
                            if end - start >= 3:  # ç¡®ä¿è‡³å°‘æœ‰3ä¸ªç‚¹ç”¨äºä¸­å€¼è®¡ç®—
                                neighbors = flux_denoised[start:end]
                                flux_denoised[idx] = np.median(neighbors)
            
            # æœ€åç¡®ä¿æ²¡æœ‰NaNå€¼
            flux_denoised = np.nan_to_num(flux_denoised, nan=np.median(flux_denoised[valid_mask]))
            
            return flux_denoised
            
        except Exception as e:
            print(f"äºŒæ¬¡å»å™ªå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return flux
    
    def process_single_spectrum(self, spec_file, label):
        """å¤„ç†å•ä¸ªå…‰è°±"""
        # ç¡®ä¿spec_fileæ˜¯å­—ç¬¦ä¸²ç±»å‹
        spec_file = str(spec_file)
        
        # æ£€æŸ¥å‚æ•°åˆæ³•æ€§
        if spec_file is None:
            raise ValueError("spec_fileä¸èƒ½ä¸ºNone")
        
        # ä½¿ç”¨CacheManageræ›¿ä»£ç›´æ¥ç¼“å­˜æ“ä½œ
        cache_key = f"processed_spectrum_{spec_file.replace('/', '_')}"
        # æ£€æŸ¥ç¼“å­˜
        processed_result = self.cache_manager.get_cache(cache_key)
        if processed_result:
            print(f"ä½¿ç”¨ç¼“å­˜çš„é¢„å¤„ç†å…‰è°±: {spec_file}")
            return processed_result
        
        try:
            # è¯»å–FITSæ–‡ä»¶
            print(f"å¤„ç†å…‰è°±: {spec_file}")
            # æ³¨æ„ï¼šè¿™é‡Œåªè§£åŒ…6ä¸ªå€¼ï¼Œä¸read_fits_fileè¿”å›å€¼åŒ¹é…
            wavelength, flux, v_helio, z_fits, snr, snr_bands = self.read_fits_file(spec_file)
            if wavelength is None or flux is None:
                print(f"æ— æ³•è¯»å–FITSæ–‡ä»¶: {spec_file}")
                return None
            
            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if np.isnan(flux).all() or len(flux) == 0:
                print(f"æ–‡ä»¶{spec_file}ä¸­çš„æµé‡æ•°æ®å…¨ä¸ºNaNæˆ–ä¸ºç©º")
                return None
            
            print(f"åŸå§‹æ•°æ®: æ³¢é•¿èŒƒå›´{wavelength[0]}~{wavelength[-1]}, ç‚¹æ•°={len(wavelength)}")
            
            # è·å–çº¢ç§»æ•°æ®
            z = z_fits  # ä¼˜å…ˆä½¿ç”¨ä»fitsæ–‡ä»¶ä¸­è¯»å–çš„çº¢ç§»å€¼
            cv = 0      # è§†å‘é€Ÿåº¦é»˜è®¤å€¼
            try:
                # å°è¯•ä»æ–‡ä»¶ååŒ¹é…åˆ°CSVä¸­çš„è®°å½•è·å–çº¢ç§»å’Œè§†å‘é€Ÿåº¦
                base_file = os.path.basename(spec_file)
                if '.' in base_file:
                    base_file = base_file.split('.')[0]
                
                for csv_file in self.csv_files:
                    if os.path.exists(csv_file):
                        df = pd.read_csv(csv_file)
                        
                        # æ£€æŸ¥CSVæ˜¯å¦æœ‰specåˆ—
                        if 'spec' in df.columns:
                            # ç¡®ä¿specåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
                            if not pd.api.types.is_string_dtype(df['spec']):
                                df['spec'] = df['spec'].astype(str)
                            
                            # åœ¨CSVä¸­æŸ¥æ‰¾åŒ¹é…è®°å½•
                            matches = df[df['spec'].str.contains(base_file, case=False, na=False)]
                            if not matches.empty:
                                # å¦‚æœzå€¼ä¸º0ä¸”CSVä¸­æœ‰zåˆ—ï¼Œåˆ™ä»CSVè¯»å–
                                if z == 0 and 'z' in df.columns:
                                    z = matches.iloc[0]['z']
                                    print(f"ä»CSVæ‰¾åˆ°çº¢ç§»å€¼: z = {z}")
                                
                                # è¯»å–è§†å‘é€Ÿåº¦ - ä»cvæˆ–rvåˆ—
                                for vel_col in ['cv', 'rv', 'velocity', 'RV']:
                                    if vel_col in df.columns:
                                        cv = matches.iloc[0][vel_col]
                                        print(f"ä»CSVæ‰¾åˆ°è§†å‘é€Ÿåº¦: {vel_col} = {cv} km/s")
                                        # å¦‚æœè§†å‘é€Ÿåº¦å€¼æœ‰æ•ˆï¼Œæ›´æ–°v_helio
                                        if not pd.isna(cv) and cv != 0:
                                            v_helio = cv
                                            print(f"ä½¿ç”¨CSVä¸­çš„è§†å‘é€Ÿåº¦å€¼: {v_helio} km/s")
                                        break
                                break
            except Exception as e:
                print(f"æŸ¥æ‰¾çº¢ç§»æˆ–è§†å‘é€Ÿåº¦æ•°æ®å‡ºé”™: {e}")
                # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤å€¼æˆ–å·²è¯»å–çš„å€¼
                
            # å¦‚æœfitsä¸­æœªæ‰¾åˆ°ä¿¡å™ªæ¯”æ•°æ®ï¼Œå°è¯•ä»CSVè·å–
            if all(v == 0 for v in snr_bands.values()):
                try:
                    for csv_file in self.csv_files:
                        if os.path.exists(csv_file):
                            df = pd.read_csv(csv_file)
                            
                            # æ£€æŸ¥CSVæ˜¯å¦æœ‰specåˆ—
                            if 'spec' in df.columns:
                                # ç¡®ä¿specåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
                                if not pd.api.types.is_string_dtype(df['spec']):
                                    df['spec'] = df['spec'].astype(str)
                                    
                                # åœ¨CSVä¸­æŸ¥æ‰¾åŒ¹é…è®°å½•
                                base_file = os.path.basename(spec_file)
                                if '.' in base_file:
                                    base_file = base_file.split('.')[0]
                                
                                matches = df[df['spec'].str.contains(base_file, case=False, na=False)]
                                if not matches.empty:
                                    for band in snr_bands:
                                        if band in df.columns:
                                            snr_bands[band] = matches.iloc[0][band]
                                            print(f"ä»CSVæ‰¾åˆ°{band}æ³¢æ®µä¿¡å™ªæ¯”: {snr_bands[band]}")
                                    break  # æ‰¾åˆ°åŒ¹é…é¡¹åé€€å‡ºå¾ªç¯
                except Exception as e:
                    print(f"ä»CSVè¯»å–ä¿¡å™ªæ¯”å¤±è´¥: {e}")
            
            # æ£€æŸ¥çº¢ç§»å’Œè§†å‘é€Ÿåº¦å€¼æ˜¯å¦ä¸ºNaN
            if pd.isna(z):
                print("è­¦å‘Š: çº¢ç§»å€¼ä¸ºNaNï¼Œè®¾ç½®ä¸º0")
                z = 0
            if pd.isna(v_helio):
                print("è­¦å‘Š: è§†å‘é€Ÿåº¦å€¼ä¸ºNaNï¼Œè®¾ç½®ä¸º0")
                v_helio = 0
            
            # 1. æ³¢é•¿æ ¡æ­£
            wavelength_calibrated = self.correct_wavelength(wavelength, flux)
            print(f"æ³¢é•¿æ ¡æ­£å: æ³¢é•¿èŒƒå›´{wavelength_calibrated[0]}~{wavelength_calibrated[-1]}")
            
            # 2. è§†å‘é€Ÿåº¦æ ¡æ­£
            wavelength_corrected = self.correct_velocity(wavelength_calibrated, flux, v_helio)
            print(f"è§†å‘é€Ÿåº¦æ ¡æ­£å: æ³¢é•¿èŒƒå›´{wavelength_corrected[0]}~{wavelength_corrected[-1]}")
            
            # 3. å»å™ª
            flux_denoised = self.denoise_spectrum(wavelength_corrected, flux)
            if flux_denoised is None:
                print(f"å»å™ª{spec_file}å¤±è´¥")
                return None
            
            # 4. çº¢ç§»æ ¡æ­£
            wavelength_rest = self.correct_redshift(wavelength_corrected, flux_denoised, z)
            print(f"çº¢ç§»æ ¡æ­£å: æ³¢é•¿èŒƒå›´{wavelength_rest[0]}~{wavelength_rest[-1]}")
            
            # 5. é‡é‡‡æ ·
            print(f"é‡é‡‡æ ·åˆ°æ³¢é•¿èŒƒå›´: {self.wavelength_range}, ç‚¹æ•°={self.n_points}")
            wavelength_resampled, flux_resampled = self.resample_spectrum(wavelength_rest, flux_denoised)
            if wavelength_resampled is None or flux_resampled is None:
                print(f"é‡é‡‡æ ·{spec_file}å¤±è´¥")
                return None
            
            # 6. è¿ç»­è°±å½’ä¸€åŒ–
            flux_continuum, continuum_params = self.normalize_continuum(wavelength_resampled, flux_resampled)
            if flux_continuum is None:
                print(f"è¿ç»­è°±å½’ä¸€åŒ–{spec_file}å¤±è´¥")
                return None
            
            # 7. äºŒæ¬¡å»å™ª
            flux_denoised_second = self.denoise_spectrum_second(wavelength_resampled, flux_continuum)
            
            # 8. æœ€ç»ˆå½’ä¸€åŒ– (æœ€å¤§æœ€å°å€¼å½’ä¸€åŒ–)
            print(f"å¯¹æµé‡è¿›è¡Œæœ€ç»ˆå½’ä¸€åŒ–")
            flux_normalized, norm_params = self.normalize_spectrum(flux_denoised_second)
            if flux_normalized is None:
                print(f"å½’ä¸€åŒ–{spec_file}å¤±è´¥")
                return None
            
            print(f"æˆåŠŸå¤„ç†å…‰è°±: {spec_file}")
            
            # è®°å½•æ ‡å‡†åŒ–å‚æ•°
            normalization_params = {
                # æ³¢é•¿èŒƒå›´ä¿¡æ¯
                'wavelength_range': self.wavelength_range,
                'log_step': self.log_step,
                'flux_min': norm_params['flux_min'] if norm_params else None,
                'flux_max': norm_params['flux_max'] if norm_params else None,
                'mean': np.mean(flux_normalized),
                'std': np.std(flux_normalized)
            }
            
            # è¿”å›å¤„ç†åçš„å…‰è°±å’Œæ ‡ç­¾ï¼ŒåŒ…æ‹¬ä¸­é—´å¤„ç†ç»“æœ
            result = {
                'data': flux_normalized,  # å°†spectrumæ”¹ä¸ºdata
                'metadata': {
                    'label': label,
                    'filename': spec_file,
                    'element': '',  # ä¼šåœ¨process_element_dataæ–¹æ³•ä¸­è¢«è®¾ç½®
                    # ä¿å­˜ä¸­é—´ç»“æœç”¨äºå¯è§†åŒ–
                    'original_wavelength': wavelength,
                    'original_flux': flux,
                    'wavelength_calibrated': wavelength_calibrated,
                    'wavelength_corrected': wavelength_corrected,
                    'denoised_flux': flux_denoised,
                    'wavelength_rest': wavelength_rest,
                    'wavelength_resampled': wavelength_resampled, 
                    'flux_resampled': flux_resampled,
                    'flux_continuum': flux_continuum,
                    'flux_denoised_second': flux_denoised_second,
                    'z': z,  # ä¿å­˜çº¢ç§»å€¼
                    'v_helio': v_helio,
                    'snr': snr,  # ä¿¡å™ªæ¯”
                    'snr_bands': snr_bands,  # å„æ³¢æ®µä¿¡å™ªæ¯”
                    'normalization_params': normalization_params
                },
                'validation_metrics': {
                    'quality_metrics': {
                        'snr': np.mean(flux_normalized) / np.std(flux_normalized) if np.std(flux_normalized) > 0 else 0,
                        'wavelength_coverage': 1.0,  # é»˜è®¤ä¸ºå®Œæ•´è¦†ç›–
                        'normalization_quality': 1.0  # é»˜è®¤ä¸ºè‰¯å¥½è´¨é‡
                    }
                }
            }
            
            # ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼Œæ·»åŠ æ—§å­—æ®µ
            result['spectrum'] = result['data']
            result['label'] = label
            result['filename'] = spec_file
            
            # ä½¿ç”¨CacheManagerä¿å­˜ç»“æœ
            self.cache_manager.set_cache(cache_key, result)
            return result
        except Exception as e:
            print(f"å¤„ç†{spec_file}æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¦‚æœè¶…è¿‡é™åˆ¶åˆ™è§¦å‘åƒåœ¾å›æ”¶"""
        gc.collect()  # å…ˆå°è¯•å›æ”¶ä¸€æ¬¡
        mem_usage = psutil.virtual_memory().percent / 100.0
        
        if mem_usage > self.memory_limit:
            print(f"å†…å­˜ä½¿ç”¨ç‡({mem_usage:.1%})è¶…è¿‡é™åˆ¶({self.memory_limit:.1%})ï¼Œæ­£åœ¨è¿›è¡Œåƒåœ¾å›æ”¶...")
            gc.collect()
            return True
        return False
        
    def process_element_data(self, df, element, start_idx=0):
        """å¤„ç†å•ä¸ªå…ƒç´ çš„æ•°æ®ï¼Œæ”¯æŒä»æŒ‡å®šä½ç½®ç»§ç»­å¤„ç†"""
        print(f"å¤„ç†{element}æ•°æ®...")
        
        # ç¡®ä¿specåˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        if 'spec' in df.columns and not pd.api.types.is_string_dtype(df['spec']):
            print(f"æ³¨æ„: {element}_FE.csv ä¸­çš„specåˆ—ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œæ­£åœ¨è½¬æ¢...")
            df['spec'] = df['spec'].astype(str)
        
        # è·å–å…‰è°±æ–‡ä»¶åå’Œæ ‡ç­¾
        spec_files = df['spec'].values
        labels = df.iloc[:, -1].values  # æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾
        
        # æ£€æŸ¥è¿›åº¦æ–‡ä»¶
        progress_file = os.path.join(self.progress_dir, f"{element}_progress.pkl")
        drive_progress_file = f"/content/drive/My Drive/SPCNNet_Results/processed_data/progress/{element}_progress.pkl"
        
        results = []
        total_processed = 0
        
        # å¦‚æœæœ‰è¿›åº¦æ–‡ä»¶ï¼ŒåŠ è½½å·²å¤„ç†çš„ç»“æœ
        if start_idx == 0:
            # é¦–å…ˆå°è¯•æ ‡å‡†ç›®å½•
            if os.path.exists(progress_file):
                try:
                    with open(progress_file, 'rb') as f:
                        saved_data = pickle.load(f)
                        results = saved_data.get('results', [])
                        start_idx = saved_data.get('last_idx', 0)
                        total_processed = len(results)
                        print(f"ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ï¼ˆå·²å¤„ç†{total_processed}æ¡è®°å½•ï¼Œè¿›åº¦ï¼š{total_processed/len(spec_files):.2%}ï¼‰")
                except Exception as e:
                    print(f"åŠ è½½è¿›åº¦æ–‡ä»¶å‡ºé”™: {e}ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
                    start_idx = 0
            # å¦‚æœæ ‡å‡†ç›®å½•æ²¡æœ‰ï¼Œå°è¯•Google Drive
            elif os.path.exists(drive_progress_file):
                try:
                    with open(drive_progress_file, 'rb') as f:
                        saved_data = pickle.load(f)
                        results = saved_data.get('results', [])
                        start_idx = saved_data.get('last_idx', 0)
                        total_processed = len(results)
                        print(f"ä»Google DriveåŠ è½½è¿›åº¦ï¼ˆå·²å¤„ç†{total_processed}æ¡è®°å½•ï¼Œè¿›åº¦ï¼š{total_processed/len(spec_files):.2%}ï¼‰")
                except Exception as e:
                    print(f"åŠ è½½Google Driveè¿›åº¦æ–‡ä»¶å‡ºé”™: {e}ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
                    start_idx = 0
        
        # è®¡ç®—å‰©ä½™çš„æ‰¹æ¬¡
        remaining = len(spec_files) - start_idx
        if remaining <= 0:
            print(f"{element}æ•°æ®å·²å…¨éƒ¨å¤„ç†å®Œæˆ")
            return results
            
        num_batches = (remaining + self.batch_size - 1) // self.batch_size
        
        # æµ‹è¯•ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œç¡®è®¤è·¯å¾„æ­£ç¡®
        test_spec = spec_files[start_idx]
        print(f"æµ‹è¯•ç¬¬ä¸€ä¸ªæ–‡ä»¶: {test_spec}")
        # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        found_path = self._find_fits_file(test_spec)
        if found_path:
            print(f"æ‰¾åˆ°æ–‡ä»¶è·¯å¾„: {found_path}")
        else:
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°æ–‡ä»¶ {test_spec}")
            return results
        
        # å°è¯•å¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶
        print("å°è¯•å¤„ç†æµ‹è¯•æ–‡ä»¶...")
        
        # ç›´æ¥æ‰“å¼€æ–‡ä»¶å¹¶æ£€æŸ¥æ•°æ®æ ¼å¼
        try:
            with fits.open(found_path, ignore_missing_end=True, memmap=False) as hdul:
                print(f"FITSæ–‡ä»¶ç»“æ„: å…±{len(hdul)}ä¸ªHDU")
                for i, hdu in enumerate(hdul):
                    hdu_type = type(hdu).__name__
                    print(f"  HDU{i}: ç±»å‹={hdu_type}")
                    
                    # å¦‚æœæ˜¯BinTableHDUï¼Œè¾“å‡ºè¡¨æ ¼ä¿¡æ¯
                    if isinstance(hdu, fits.BinTableHDU):
                        print(f"  è¡¨æ ¼åˆ—: {hdu.columns.names}")
                        print(f"  è¡¨æ ¼è¡Œæ•°: {len(hdu.data)}")
                        
                        # è¾“å‡ºç¬¬ä¸€è¡Œæ•°æ®ç±»å‹
                        first_row = hdu.data[0]
                        print(f"  ç¬¬ä¸€è¡Œæ•°æ®ç±»å‹: {type(first_row)}")
                        
                        # æ£€æŸ¥æ¯åˆ—çš„æ•°æ®ç±»å‹
                        for col_name in hdu.columns.names:
                            col_data = hdu.data[col_name]
                            if len(col_data) > 0:
                                print(f"  åˆ— '{col_name}' ç±»å‹: {type(col_data[0])}")
                                # å¦‚æœæ˜¯æ•°ç»„ç±»å‹ï¼Œå°è¯•è·å–å…¶å½¢çŠ¶
                                try:
                                    if hasattr(col_data[0], 'shape'):
                                        print(f"    æ•°æ®å½¢çŠ¶: {col_data[0].shape}")
                                    elif hasattr(col_data[0], '__len__'):
                                        print(f"    æ•°æ®é•¿åº¦: {len(col_data[0])}")
                                except:
                                    pass
        except Exception as e:
            print(f"æ£€æŸ¥FITSæ–‡ä»¶æ ¼å¼å‡ºé”™: {e}")
        
        # æ­£å¸¸å¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶
        test_result = self.process_single_spectrum(test_spec, labels[start_idx])
        if test_result is None:
            print(f"è­¦å‘Š: æ— æ³•å¤„ç†ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ {test_spec}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹æˆ–å¤„ç†é€»è¾‘")
            # å°è¯•è¯»å–æ–‡ä»¶è¿›è¡Œè¯Šæ–­
            try:
                with fits.open(found_path) as hdul:
                    header = hdul[0].header
                    print(f"æ–‡ä»¶å¤´ä¿¡æ¯ç¤ºä¾‹: NAXIS={header.get('NAXIS')}, NAXIS1={header.get('NAXIS1')}")
                    # æ£€æŸ¥æ–‡ä»¶å†…å®¹
                    data_shape = hdul[0].data.shape if hdul[0].data is not None else "æ— æ•°æ®"
                    print(f"æ•°æ®å½¢çŠ¶: {data_shape}")
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶å‡ºé”™: {e}")
            return results
        else:
            print(f"æµ‹è¯•æ–‡ä»¶ {test_spec} å¤„ç†æˆåŠŸï¼Œç»§ç»­æ‰¹é‡å¤„ç†")
            results.append(test_result)
            results[0]['element'] = element
            total_processed += 1
        
        # é€æ‰¹å¤„ç†å‰©ä½™æ•°æ®
        for batch_idx in tqdm(range(num_batches), desc=f"å¤„ç†{element}å…‰è°±æ‰¹æ¬¡"):
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç´¢å¼•èŒƒå›´
            current_start = start_idx + 1 + batch_idx * self.batch_size  # è·³è¿‡å·²æµ‹è¯•çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶
            if current_start >= len(spec_files):
                break  # é˜²æ­¢è¶Šç•Œ
            
            current_end = min(current_start + self.batch_size, len(spec_files))
            batch_specs = spec_files[current_start:current_end]
            batch_labels = labels[current_start:current_end]
            
            if len(batch_specs) == 0 or len(batch_labels) == 0:
                continue  # è·³è¿‡ç©ºæ‰¹æ¬¡
            
            batch_results = []
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            if self.check_memory_usage():
                # å¦‚æœå†…å­˜ç´§å¼ ï¼Œå…ˆä¿å­˜å½“å‰è¿›åº¦
                with open(progress_file, 'wb') as f:
                    pickle.dump({'results': results, 'last_idx': current_start}, f)
                # ä¿å­˜æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜
                self._save_files_cache()
                print("å†…å­˜ä½¿ç”¨ç‡é«˜ï¼Œå·²ä¿å­˜è¿›åº¦ï¼Œå¯ä»¥å®‰å…¨é€€å‡ºç¨‹åº")
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                if input("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œæ˜¯å¦ç»§ç»­å¤„ç†ï¼Ÿ(y/n): ").lower() != 'y':
                    return results
            
            # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†å½“å‰æ‰¹æ¬¡
            successful_count = 0
            failed_specs = []
            
            with Pool(processes=self.max_workers) as pool:
                jobs = []
                for spec_file, label in zip(batch_specs, batch_labels):
                    jobs.append(pool.apply_async(self.process_single_spectrum, 
                                                (spec_file, label)))
                
                for job in jobs:
                    try:
                        result = job.get(timeout=30)  # è®¾ç½®è¶…æ—¶é¿å…å¡æ­»
                        if result is not None:
                            # è®¾ç½®å…ƒç´ ä¿¡æ¯
                            if 'metadata' in result:
                                result['metadata']['element'] = element
                            else:
                                # å…¼å®¹æ—§æ ¼å¼
                                result['element'] = element
                            
                            batch_results.append(result)
                            successful_count += 1
                        else:
                            failed_specs.append(spec_file)
                    except Exception as e:
                        print(f"å¤„ç†ä½œä¸šå‡ºé”™: {e}")
                        continue
            
            # æ·»åŠ åˆ°ç»“æœé›†
            results.extend(batch_results)
            total_processed += successful_count
            
            # è¾“å‡ºå½“å‰æ‰¹æ¬¡ç»Ÿè®¡å’Œæ•´ä½“è¿›åº¦
            overall_progress = (total_processed / len(spec_files)) * 100
            if len(batch_specs) > 0:
                print(f"æ‰¹æ¬¡ {batch_idx+1}/{num_batches}: æˆåŠŸ {successful_count}/{len(batch_specs)} ä¸ªæ–‡ä»¶")
                print(f"æ€»è¿›åº¦: [{overall_progress:.2f}%] {total_processed}/{len(spec_files)} å·²å®Œæˆ")
                if len(failed_specs) > 0 and len(failed_specs) < 5:
                    print(f"å¤±è´¥æ–‡ä»¶ç¤ºä¾‹: {failed_specs}")
            
            # å®šæœŸä¿å­˜è¿›åº¦
            if batch_idx % 5 == 0 or batch_idx == num_batches - 1:
                with open(progress_file, 'wb') as f:
                    pickle.dump({'results': results, 'last_idx': current_end}, f)
                # ä¿å­˜æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜
                self._save_files_cache()
                print(f"âœ“ è¿›åº¦å·²ä¿å­˜ [{overall_progress:.2f}%]")
        
        print(f"æˆåŠŸå¤„ç†{total_processed}/{len(spec_files)}æ¡{element}å…‰è°±æ•°æ® (å®Œæˆç‡: {total_processed/len(spec_files):.2%})")
        return results
    
    def process_all_data(self, resume=True):
        """å¤„ç†æ‰€æœ‰æ•°æ®å¹¶å‡†å¤‡è®­ç»ƒé›†ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µå¤„ç†ç­–ç•¥"""
        start_time = time.time()
        
        # è¿›åº¦æ–‡ä»¶è·¯å¾„
        progress_file = os.path.join(self.progress_dir, 'all_data_progress.pkl')
        wave_range_file = os.path.join(self.progress_dir, 'wave_range_progress.pkl')
        drive_progress_file = '/content/drive/My Drive/SPCNNet_Results/processed_data/progress/all_data_progress.pkl'
        
        # åˆå§‹åŒ–å…¨éƒ¨æ•°æ®åˆ—è¡¨
        all_data = []
        
        # å¦‚æœæœ‰è¿›åº¦æ–‡ä»¶ï¼Œè€ƒè™‘æ¢å¤
        if resume:
            # å…ˆå°è¯•ä»æ ‡å‡†è·¯å¾„åŠ è½½
            if os.path.exists(progress_file):
                try:
                    with open(progress_file, 'rb') as f:
                        all_data = pickle.load(f)
                    print(f"å·²åŠ è½½ä¿å­˜çš„å¤„ç†ç»“æœï¼Œå…±{len(all_data)}æ¡è®°å½•")
                    
                    # è¯»å–CSVæ–‡ä»¶æ¥è·å–æ€»è®°å½•æ•°
                    dataframes, elements = self.read_csv_data()
                    if not dataframes:
                        print("é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é›†")
                        return np.array([]), np.array([]), np.array([]), np.array([])
                    
                    # è®¡ç®—æ€»æ•°æ®é‡
                    total_records = sum(len(df) for df in dataframes)
                    
                    # æ˜¾ç¤ºè¿›åº¦å¹¶è¯¢é—®æ˜¯å¦ç»§ç»­
                    progress_percent = len(all_data)/total_records * 100
                    print(f"å½“å‰è¿›åº¦: {len(all_data)}/{total_records} æ¡è®°å½• ({progress_percent:.2f}%)")
                    
                    if len(all_data) >= total_records:
                        print(f"æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼Œè¿›å…¥æ•°æ®å‡†å¤‡é˜¶æ®µ")
                        return self._prepare_arrays(all_data)
                    else:
                        if input(f"æ˜¯å¦ç»§ç»­å¤„ç†å‰©ä½™{total_records - len(all_data)}æ¡æ•°æ®ï¼Ÿ(y/n): ").lower() != 'y':
                            print("è·³è¿‡å¤„ç†é˜¶æ®µï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰æ•°æ®")
                            return self._prepare_arrays(all_data)
                        else:
                            print(f"ç»§ç»­å¤„ç†å‰©ä½™æ•°æ®...")
                except Exception as e:
                    print(f"åŠ è½½è¿›åº¦æ–‡ä»¶å‡ºé”™: {e}ï¼Œå°†é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®")
                    all_data = []
            # å¦‚æœæ ‡å‡†è·¯å¾„æ²¡æœ‰ï¼Œå°è¯•ä»Google DriveåŠ è½½
            elif os.path.exists(drive_progress_file):
                try:
                    with open(drive_progress_file, 'rb') as f:
                        all_data = pickle.load(f)
                    print(f"ä»Google DriveåŠ è½½å¤„ç†ç»“æœï¼Œå…±{len(all_data)}æ¡è®°å½•")
                    
                    # è¯»å–CSVæ–‡ä»¶æ¥è·å–æ€»è®°å½•æ•°
                    dataframes, elements = self.read_csv_data()
                    if not dataframes:
                        print("é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é›†")
                        return np.array([]), np.array([]), np.array([]), np.array([])
                    
                    # è®¡ç®—æ€»æ•°æ®é‡
                    total_records = sum(len(df) for df in dataframes)
                    
                    # æ˜¾ç¤ºè¿›åº¦å¹¶è¯¢é—®æ˜¯å¦ç»§ç»­
                    progress_percent = len(all_data)/total_records * 100
                    print(f"å½“å‰è¿›åº¦: {len(all_data)}/{total_records} æ¡è®°å½• ({progress_percent:.2f}%)")
                    
                    if len(all_data) >= total_records:
                        print(f"æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼Œè¿›å…¥æ•°æ®å‡†å¤‡é˜¶æ®µ")
                        return self._prepare_arrays(all_data)
                    else:
                        if input(f"æ˜¯å¦ç»§ç»­å¤„ç†å‰©ä½™{total_records - len(all_data)}æ¡æ•°æ®ï¼Ÿ(y/n): ").lower() != 'y':
                            print("è·³è¿‡å¤„ç†é˜¶æ®µï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰æ•°æ®")
                            return self._prepare_arrays(all_data)
                        else:
                            print(f"ç»§ç»­å¤„ç†å‰©ä½™æ•°æ®...")
                except Exception as e:
                    print(f"åŠ è½½Google Driveè¿›åº¦æ–‡ä»¶å‡ºé”™: {e}ï¼Œå°†é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®")
                    all_data = []
        
        # è¯»å–CSVæ–‡ä»¶
        dataframes, elements = self.read_csv_data()
        if not dataframes:
            print("é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é›†")
            return np.array([]), np.array([]), np.array([]), np.array([])
        
        # è®¡ç®—æ€»æ•°æ®é‡
        total_records = sum(len(df) for df in dataframes)
        print(f"æ€»æ•°æ®é‡: {total_records}æ¡è®°å½•")
        
        # è¾“å‡ºç¼“å­˜ç›®å½•ä¿¡æ¯
        print(f"\n===== ç¼“å­˜ç›®å½•ä¿¡æ¯ =====")
        print(f"ç¼“å­˜ç›®å½•: {self.cache_dir}")
        cache_files = glob.glob(os.path.join(self.cache_dir, "*"))
        print(f"ç¼“å­˜æ–‡ä»¶æ€»æ•°: {len(cache_files)}")
        wavelength_cache_files = glob.glob(os.path.join(self.cache_dir, "wavelength_range_*"))
        print(f"æ³¢é•¿èŒƒå›´ç¼“å­˜æ–‡ä»¶æ•°: {len(wavelength_cache_files)}")
        spectrum_cache_files = glob.glob(os.path.join(self.cache_dir, "processed_spectrum_*"))
        print(f"å…‰è°±å¤„ç†ç¼“å­˜æ–‡ä»¶æ•°: {len(spectrum_cache_files)}")
        
        # æ‰“å°CSVä¸­specåˆ—çš„æ ·æœ¬å€¼
        for i, (df, element) in enumerate(zip(dataframes, elements)):
            if 'spec' in df.columns:
                print(f"\nå…ƒç´  {element} çš„specåˆ—å‰10ä¸ªæ ·æœ¬:")
                for j, spec in enumerate(df['spec'].values[:10]):
                    print(f"  {j+1}. '{spec}'")
            
            # æ‰“å°ç¼“å­˜æ–‡ä»¶åç¤ºä¾‹
            if wavelength_cache_files:
                print(f"\næ³¢é•¿èŒƒå›´ç¼“å­˜æ–‡ä»¶ç¤ºä¾‹:")
                for j, cache_file in enumerate(wavelength_cache_files[:5]):
                    print(f"  {j+1}. '{os.path.basename(cache_file)}'")
            
            if spectrum_cache_files:
                print(f"\nå…‰è°±å¤„ç†ç¼“å­˜æ–‡ä»¶ç¤ºä¾‹:")
                for j, cache_file in enumerate(spectrum_cache_files[:5]):
                    print(f"  {j+1}. '{os.path.basename(cache_file)}'")
            
            break  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªå…ƒç´ çš„ä¿¡æ¯
        
        # ç¬¬ä¸€é˜¶æ®µï¼šè®¡ç®—æ‰€æœ‰å…‰è°±çš„æœ€å¤§å…¬å…±æ³¢é•¿èŒƒå›´
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆäº†ç¬¬ä¸€é˜¶æ®µ
        first_stage_done = False
        if os.path.exists(wave_range_file):
            try:
                with open(wave_range_file, 'rb') as f:
                    saved_range_data = pickle.load(f)
                    self.processed_ranges = saved_range_data.get('processed_ranges', [])
                    self.wavelength_range = saved_range_data.get('wavelength_range', self.wavelength_range)
                    first_stage_processed = saved_range_data.get('processed_count', 0)
                    
                    print(f"å·²ä»ç¼“å­˜åŠ è½½æ³¢é•¿èŒƒå›´ä¿¡æ¯:")
                    print(f"å·²å¤„ç†: {first_stage_processed}/{total_records} æ¡è®°å½•")
                    print(f"å½“å‰æœ€å¤§å…¬å…±æ³¢é•¿èŒƒå›´: {self.wavelength_range}")
                    
                    # å¦‚æœå·²ç»å¤„ç†äº†æ‰€æœ‰è®°å½•ï¼Œç¬¬ä¸€é˜¶æ®µå®Œæˆ
                    if first_stage_processed >= total_records:
                        first_stage_done = True
                        print("ç¬¬ä¸€é˜¶æ®µ(è®¡ç®—å…¬å…±æ³¢é•¿èŒƒå›´)å·²å®Œæˆ")
                    else:
                        print(f"å°†ç»§ç»­è¿›è¡Œç¬¬ä¸€é˜¶æ®µå¤„ç†å‰©ä½™çš„ {total_records - first_stage_processed} æ¡è®°å½•")
            except Exception as e:
                print(f"åŠ è½½æ³¢é•¿èŒƒå›´è¿›åº¦æ–‡ä»¶å‡ºé”™: {e}ï¼Œå°†é‡æ–°è®¡ç®—å…¬å…±æ³¢é•¿èŒƒå›´")
                self.processed_ranges = []
                first_stage_processed = 0
        else:
            self.processed_ranges = []
            first_stage_processed = 0
        
        # å¦‚æœç¬¬ä¸€é˜¶æ®µæœªå®Œæˆï¼Œè¿›è¡Œç¬¬ä¸€é˜¶æ®µå¤„ç†
        if not first_stage_done:
            print(f"\n===== ç¬¬ä¸€é˜¶æ®µï¼šè®¡ç®—æœ€å¤§å…¬å…±æ³¢é•¿èŒƒå›´ =====")
            first_stage_count = 0
            
            # å¤„ç†æ¯ä¸ªå…ƒç´ çš„æ•°æ®
            for i, (df, element) in enumerate(zip(dataframes, elements)):
                print(f"\nå¤„ç†å…ƒç´  {i+1}/{len(dataframes)}: {element} (ç¬¬ä¸€é˜¶æ®µ)")
                
                # å¤„ç†æ¯ä¸ªå…‰è°±æ–‡ä»¶è®¡ç®—æ³¢é•¿èŒƒå›´
                spec_files = df['spec'].values
                for j, spec_file in enumerate(tqdm(spec_files, desc=f"å¤„ç†{element}å…‰è°±æ³¢é•¿èŒƒå›´")):
                    # å¦‚æœå·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡
                    if first_stage_processed + first_stage_count > j:
                        continue
                    
                    # å°è¯•ä»ç¼“å­˜è·å–
                    cache_key = f"wavelength_range_{spec_file.replace('/', '_')}"
                    cached_range = self.cache_manager.get_cache(cache_key)
                    
                    if cached_range:
                        # å¦‚æœæœ‰ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜çš„æ³¢é•¿èŒƒå›´
                        w_min, w_max = cached_range
                        self.processed_ranges.append((w_min, w_max))
                        print(f"ä½¿ç”¨ç¼“å­˜çš„æ³¢é•¿èŒƒå›´ {spec_file}: {w_min:.2f}~{w_max:.2f}")
                        
                    if cached_range:
                        # å¦‚æœæœ‰ç¼“å­˜ï¼Œéœ€è¦æ£€æŸ¥æ•°æ®ç±»å‹å¹¶æå–æ³¢é•¿èŒƒå›´
                        try:
                            # æ‰“å°ç¼“å­˜æ•°æ®ç±»å‹ä»¥ä¾¿è°ƒè¯•
                            print(f"ç¼“å­˜æ•°æ®ç±»å‹: {type(cached_range)}, å€¼: {cached_range}")
                            
                            # å¤„ç†ä¸åŒç±»å‹çš„ç¼“å­˜æ•°æ®
                            if isinstance(cached_range, tuple) and len(cached_range) == 2:
                                # æ ‡å‡†çš„äºŒå…ƒç»„æ ¼å¼
                                w_min, w_max = cached_range
                            elif isinstance(cached_range, tuple) and len(cached_range) > 2:
                                # å¦‚æœå…ƒç»„æœ‰è¶…è¿‡2ä¸ªå…ƒç´ ï¼Œåªå–å‰ä¸¤ä¸ª
                                print(f"è­¦å‘Š: ç¼“å­˜å…ƒç»„é•¿åº¦ > 2: {len(cached_range)}")
                                w_min, w_max = cached_range[0], cached_range[1]
                            elif isinstance(cached_range, list) and len(cached_range) >= 2:
                                # åˆ—è¡¨æ ¼å¼
                                w_min, w_max = cached_range[0], cached_range[1]
                            elif isinstance(cached_range, dict) and 'data' in cached_range:
                                # å­—å…¸æ ¼å¼
                                data_array = cached_range['data']
                                if isinstance(data_array, np.ndarray) and len(data_array) == 2:
                                    w_min, w_max = data_array[0], data_array[1]
                                elif 'metadata' in cached_range and 'wavelength_range' in cached_range['metadata']:
                                    # ä»å…ƒæ•°æ®ä¸­è·å–
                                    w_range = cached_range['metadata']['wavelength_range']
                                    w_min, w_max = w_range[0], w_range[1]
                                else:
                                    # æ— æ³•æå–ï¼Œè·³è¿‡ç¼“å­˜æ•°æ®
                                    print(f"ç¼“å­˜æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {spec_file}ï¼Œé‡æ–°å¤„ç†")
                                    cached_range = None
                            elif isinstance(cached_range, np.ndarray) and len(cached_range) >= 2:
                                # numpyæ•°ç»„æ ¼å¼
                                w_min, w_max = cached_range[0], cached_range[1]
                            else:
                                # ä¸æ”¯æŒçš„æ ¼å¼
                                print(f"ç¼“å­˜æ•°æ®ç±»å‹ä¸æ”¯æŒ: {type(cached_range)}ï¼Œé‡æ–°å¤„ç†")
                                cached_range = None
                                
                            if cached_range:  # å¦‚æœæˆåŠŸæå–äº†æ³¢é•¿èŒƒå›´
                                self.processed_ranges.append((w_min, w_max))
                                print(f"ä½¿ç”¨ç¼“å­˜çš„æ³¢é•¿èŒƒå›´ {spec_file}: {w_min:.2f}~{w_max:.2f}")
                        except Exception as e:
                            print(f"å¤„ç†ç¼“å­˜æ•°æ®æ—¶å‡ºé”™ {spec_file}: {e}ï¼Œé‡æ–°å¤„ç†")
                            cached_range = None
                    
                    if not cached_range:  # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–è€…ç¼“å­˜å¤„ç†å¤±è´¥
                        # æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦è¯»å–æ–‡ä»¶æå–æ³¢é•¿èŒƒå›´
                        try:
                            # æŸ¥æ‰¾FITSæ–‡ä»¶
                            file_path = self._get_file_extension(spec_file)
                            if not file_path:
                                print(f"æ— æ³•æ‰¾åˆ°æ–‡ä»¶: {spec_file}")
                                continue
                                
                            # è¯»å–FITSæ–‡ä»¶ï¼Œè·å–æ³¢é•¿å’Œæµé‡
                            wavelength, flux, v_helio, z, snr, snr_bands = self.read_fits_file(file_path)
                            if wavelength is None or flux is None:
                                print(f"æ— æ³•è¯»å–æ³¢é•¿å’Œæµé‡æ•°æ®: {file_path}")
                                continue
                            
                            # æ£€æŸ¥å¹¶è¿‡æ»¤æ— æ•ˆå€¼
                            valid_mask = ~np.isnan(flux) & ~np.isinf(flux)
                            if not np.any(valid_mask):
                                print(f"æ‰€æœ‰æµé‡å€¼éƒ½æ˜¯æ— æ•ˆçš„: {file_path}")
                                continue
                                
                            wavelength_valid = wavelength[valid_mask]
                            if len(wavelength_valid) < 2:
                                print(f"æœ‰æ•ˆæ³¢é•¿ç‚¹æ•°å¤ªå°‘: {file_path}")
                                continue
                                
                            # è·å–æ³¢é•¿èŒƒå›´
                            w_min, w_max = wavelength_valid.min(), wavelength_valid.max()
                            self.processed_ranges.append((w_min, w_max))
                            
                            # ä¿å­˜æ³¢é•¿èŒƒå›´åˆ°ç¼“å­˜ - ä½¿ç”¨å­—å…¸æ ¼å¼
                            wavelength_range_data = {
                                'data': np.array([w_min, w_max]),  # æ•°æ®å¿…é¡»æ˜¯numpyæ•°ç»„
                                'metadata': {
                                    'filename': spec_file,
                                    'wavelength_range': [w_min, w_max]
                                },
                                'validation_metrics': {
                                    'quality_metrics': {
                                        'snr': 10.0,  # å ä½å€¼
                                        'wavelength_coverage': 1.0,
                                        'normalization_quality': 1.0
                                    }
                                }
                            }
                            self.cache_manager.set_cache(cache_key, wavelength_range_data)
                            
                            # æ›´æ–°æœ€å¤§å…¬æœ‰èŒƒå›´
                            if len(self.processed_ranges) > 1:
                                common_min = max(r[0] for r in self.processed_ranges)
                                common_max = min(r[1] for r in self.processed_ranges)
                                
                                if common_min < common_max:
                                    self.wavelength_range = (common_min, common_max)
                                    
                        except Exception as e:
                            print(f"å¤„ç†æ–‡ä»¶ {spec_file} æ³¢é•¿èŒƒå›´æ—¶å‡ºé”™: {e}")
                            continue
                    
                    first_stage_count += 1
                    
                    # æ¯å¤„ç†100ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡è¿›åº¦
                    if first_stage_count % 100 == 0:
                        # ä¿å­˜æ³¢é•¿èŒƒå›´è¿›åº¦
                        with open(wave_range_file, 'wb') as f:
                            pickle.dump({
                                'processed_ranges': self.processed_ranges,
                                'wavelength_range': self.wavelength_range,
                                'processed_count': first_stage_processed + first_stage_count
                            }, f)
                        print(f"\nå·²ä¿å­˜æ³¢é•¿èŒƒå›´è¿›åº¦: {first_stage_processed + first_stage_count}/{total_records}")
                        print(f"å½“å‰æœ€å¤§å…¬å…±æ³¢é•¿èŒƒå›´: {self.wavelength_range}")
            
            # ä¿å­˜æœ€ç»ˆçš„æ³¢é•¿èŒƒå›´è¿›åº¦
            with open(wave_range_file, 'wb') as f:
                pickle.dump({
                    'processed_ranges': self.processed_ranges,
                    'wavelength_range': self.wavelength_range,
                    'processed_count': total_records  # æ ‡è®°ä¸ºå…¨éƒ¨å¤„ç†å®Œæˆ
                }, f)
            print(f"\nç¬¬ä¸€é˜¶æ®µå®Œæˆï¼šå·²ç¡®å®šæœ€å¤§å…¬å…±æ³¢é•¿èŒƒå›´ {self.wavelength_range}")
        
        # ç¬¬äºŒé˜¶æ®µï¼šé‡é‡‡æ ·å’Œåç»­å¤„ç†
        print(f"\n===== ç¬¬äºŒé˜¶æ®µï¼šé‡é‡‡æ ·å’Œåç»­å¤„ç† =====")
        print(f"ä½¿ç”¨ç¡®å®šçš„å…¬å…±æ³¢é•¿èŒƒå›´: {self.wavelength_range}")
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„å…¬å…±æ³¢é•¿èŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤èŒƒå›´
        if not hasattr(self, 'wavelength_range') or self.wavelength_range is None:
            self.wavelength_range = (3690, 9100)
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„å…¬å…±æ³¢é•¿èŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤èŒƒå›´ {self.wavelength_range}")
        
        # å¤„ç†æ¯ä¸ªå…ƒç´ çš„æ•°æ®
        processed_records = 0
        for i, (df, element) in enumerate(zip(dataframes, elements)):
            # æ›´é«˜æ•ˆçš„æ–¹å¼ç»Ÿè®¡å·²å¤„ç†å…ƒç´ è®°å½•
            # é¦–å…ˆæ£€æŸ¥ç¼“å­˜ä¸­å·²æœ‰çš„å¤„ç†ç»“æœæ•°é‡
            cached_element_count = 0
            
            print(f"æ£€æŸ¥{element}å…ƒç´ çš„ç¼“å­˜é¢„å¤„ç†ç»“æœ...")
            
            # ç›´æ¥æœç´¢ç¼“å­˜ç›®å½•ä¸­çš„æ–‡ä»¶
            cache_files = glob.glob(os.path.join(self.cache_dir, "processed_spectrum_*"))
            print(f"å‘ç° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
            
            # è·å–å…ƒç´ ä¸­æ‰€æœ‰æ ·æœ¬çš„specæ–‡ä»¶å
            spec_files = df['spec'].values
            
            # åˆ›å»ºæ˜ å°„ï¼Œå°†specæ–‡ä»¶åçš„å„ç§å¯èƒ½å½¢å¼æ˜ å°„åˆ°å®é™…çš„specæ–‡ä»¶
            spec_mapping = {}
            for spec_file in spec_files:
                spec_base = os.path.basename(str(spec_file))
                if '.' in spec_base:
                    spec_base = spec_base.split('.')[0]
                
                # å­˜å‚¨ä¸åŒçš„å½¢å¼
                spec_mapping[spec_file] = spec_file
                spec_mapping[spec_base] = spec_file
                
                # è¿˜å¯ä»¥æ·»åŠ å…¶ä»–å¯èƒ½çš„å˜ä½“
                if spec_base.isdigit():
                    spec_mapping[spec_base.lstrip('0')] = spec_file
            
            # éå†æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•spec
            for cache_file in tqdm(cache_files, desc=f"æ‰«æ{element}ç¼“å­˜"):
                try:
                    cache_basename = os.path.basename(cache_file)
                    # ä»ç¼“å­˜æ–‡ä»¶åä¸­æå–å¯èƒ½çš„specæ ‡è¯†
                    if cache_basename.startswith("processed_spectrum_"):
                        cache_spec = cache_basename.replace("processed_spectrum_", "")
                        
                        # æ£€æŸ¥æ˜¯å¦èƒ½åœ¨æ˜ å°„ä¸­æ‰¾åˆ°åŒ¹é…
                        matched = False
                        matched_spec = None
                        
                        for spec_key, spec_value in spec_mapping.items():
                            if str(spec_key) in cache_spec or cache_spec in str(spec_key):
                                matched = True
                                matched_spec = spec_value
                                break
                        
                        if matched:
                            print(f"æ‰¾åˆ°åŒ¹é…: {cache_basename} -> {matched_spec}")
                            # ä»æ–‡ä»¶ç›´æ¥è¯»å–ç¼“å­˜å†…å®¹
                            try:
                                with open(cache_file, 'rb') as f:
                                    cached_result = pickle.load(f)
                                    
                                if cached_result is not None:
                                    cached_element_count += 1
                                    # å°†å·²ç¼“å­˜çš„ç»“æœç›´æ¥æ·»åŠ åˆ°all_dataä¸­
                                    if 'metadata' not in cached_result:
                                        cached_result['metadata'] = {}
                                    cached_result['metadata']['element'] = element
                                    all_data.append(cached_result)
                            except Exception as e:
                                print(f"è¯»å–ç¼“å­˜æ–‡ä»¶ {cache_file} æ—¶å‡ºé”™: {e}")
                except Exception as e:
                    print(f"å¤„ç†ç¼“å­˜æ–‡ä»¶ {cache_file} æ—¶å‡ºé”™: {e}")
            
            # æ‰“å°åŒ¹é…æƒ…å†µçš„è¯¦ç»†ä¿¡æ¯
            print(f"\n===== ç¼“å­˜åŒ¹é…æƒ…å†µ =====")
            print(f"å…ƒç´ : {element}")
            print(f"æ€»è®°å½•æ•°: {len(df)}")
            print(f"åŒ¹é…åˆ°ç¼“å­˜çš„è®°å½•æ•°: {cached_element_count}")
            print(f"åŒ¹é…ç‡: {cached_element_count/len(df)*100:.2f}%")
            print(f"=======================\n")
            
            # ç»Ÿè®¡å·²å¤„ç†çš„å…ƒç´ è®°å½•æ•°
            element_records = cached_element_count
            # å…ƒç´ çš„æ€»è®°å½•æ•°
            element_total = len(df)
            
            print(f"ä»ç¼“å­˜ä¸­å‘ç°{element}å…ƒç´ å·²å¤„ç†è®°å½•: {element_records}/{element_total}")
            
            # åªæœ‰å½“å…ƒç´ çš„æ‰€æœ‰è®°å½•éƒ½å·²å¤„ç†æ—¶ï¼Œæ‰è®¤ä¸ºè¯¥å…ƒç´ å¤„ç†å®Œæˆ
            if element_records >= element_total:
                print(f"{element}æ•°æ®å·²åœ¨ä¹‹å‰çš„è¿è¡Œä¸­å¤„ç†å®Œæˆ ({element_records}/{element_total}æ¡è®°å½•)")
                processed_records += element_records
                # æ³¨æ„ï¼šæˆ‘ä»¬å·²ç»åœ¨æ‰«æç¼“å­˜æ—¶å°†æ•°æ®æ·»åŠ åˆ°all_dataä¸­äº†
            else:
                print(f"\nå¤„ç†å…ƒç´  {i+1}/{len(dataframes)}: {element} (ç¬¬äºŒé˜¶æ®µ)")
                print(f"å·²å¤„ç†: {element_records}/{element_total}æ¡è®°å½•")
                print(f"å½“å‰è¿›åº¦: [{processed_records/total_records:.2%}]")
                
                # å¦‚æœå…ƒç´ å·²éƒ¨åˆ†å¤„ç†ï¼Œä»æœªå¤„ç†çš„éƒ¨åˆ†ç»§ç»­
                start_idx = element_records
                print(f"ä»ç´¢å¼•{start_idx}ç»§ç»­å¤„ç†...")
                
                results = self.process_element_data(df, element, start_idx=start_idx)
                all_data.extend(results)
                processed_records += len(results)
                
                # æ›´æ–°æ€»ä½“è¿›åº¦
                overall_progress = processed_records / total_records
                print(f"æ€»è¿›åº¦: [{overall_progress:.2%}] å·²å®Œæˆ{processed_records}/{total_records}æ¡è®°å½•")
            
            # ä¿å­˜æ€»è¿›åº¦
            with open(progress_file, 'wb') as f:
                pickle.dump(all_data, f)
                
            # è¾“å‡ºé˜¶æ®µæ€§ç»Ÿè®¡
            elapsed_time = time.time() - start_time
            records_per_second = processed_records / elapsed_time if elapsed_time > 0 else 0
            print(f"å½“å‰å·²å¤„ç†æ€»æ•°æ®é‡: {len(all_data)}æ¡")
            print(f"å¤„ç†é€Ÿåº¦: {records_per_second:.2f}æ¡/ç§’")
            
            # ä¼°è®¡å‰©ä½™æ—¶é—´
            if processed_records < total_records and records_per_second > 0:
                remaining_records = total_records - processed_records
                estimated_time = remaining_records / records_per_second
                hours, remainder = divmod(estimated_time, 3600)
                minutes, seconds = divmod(remainder, 60)
                print(f"é¢„è®¡å‰©ä½™æ—¶é—´: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’")
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            self.check_memory_usage()
        
        # å¦‚æœæ²¡æœ‰å¤„ç†åˆ°ä»»ä½•æ•°æ®
        if not all_data:
            print("é”™è¯¯: æ²¡æœ‰å¤„ç†åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥fitsæ–‡ä»¶è·¯å¾„å’ŒCSVæ–‡ä»¶")
            return np.array([]), np.array([]), np.array([]), np.array([])
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        elapsed_time = time.time() - start_time
        hours, remainder = divmod(elapsed_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        print(f"\nå¤„ç†å®Œæˆ! æ€»è€—æ—¶: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’")
        print(f"å¤„ç†è®°å½•: {len(all_data)}/{total_records} ({len(all_data)/total_records:.2%})")
        
        # ä¿å­˜æ–‡ä»¶ç¼“å­˜
        self._save_files_cache()
        
        # è½¬æ¢ä¸ºNumPyæ•°ç»„å¹¶è¿”å›
        return self._prepare_arrays(all_data)
    
    def _prepare_arrays(self, all_data):
        """å‡†å¤‡è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®æ•°ç»„"""
        if not all_data:
            print("æ²¡æœ‰å¯ç”¨çš„æ•°æ®")
            return None, None, None, None
            
        # æå–å…‰è°±æ•°æ®å’Œæ ‡ç­¾
        spectra = []
        labels = []
        filenames = []
        elements = []
        
        for data in all_data:
            # å¿½ç•¥æ— æ•ˆæ•°æ®
            if not data or 'data' not in data or data['data'] is None:
                continue
                
            # è·å–å…‰è°±æ•°æ®
            spectrum = data['data']
            # æ£€æŸ¥å…‰è°±æ•°æ®æœ‰æ•ˆæ€§
            if spectrum is None or len(spectrum) == 0:
                continue
                
            # æ›¿æ¢NaNå’Œæ— ç©·å€¼
            if np.isnan(spectrum).any() or np.isinf(spectrum).any():
                print(f"å‘ç°æ— æ•ˆå€¼ï¼Œæ›¿æ¢ä¸º0: {data['filename']}")
                spectrum = np.nan_to_num(spectrum, nan=0.0, posinf=0.0, neginf=0.0)
            
            # è·å–æ ‡ç­¾
            label = data['metadata']['label']
            # æ£€æŸ¥æ ‡ç­¾æœ‰æ•ˆæ€§
            if pd.isna(label):
                print(f"è·³è¿‡æ•°æ®ï¼Œæ ‡ç­¾ä¸ºNaN: {data['filename']}")
                continue
                
            spectra.append(spectrum)
            labels.append(label)
            filenames.append(data['filename'])
            elements.append(data['metadata']['element'])
        
        if not spectra:
            print("å¤„ç†åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return None, None, None, None
        
        # æ£€æŸ¥æ‰€æœ‰å…‰è°±çš„é•¿åº¦æ˜¯å¦ä¸€è‡´
        expected_length = len(spectra[0])
        inconsistent_indices = []
        
        for i, spec in enumerate(spectra):
            if len(spec) != expected_length:
                print(f"è­¦å‘Š: å…‰è°± {i} ({filenames[i]}) é•¿åº¦ä¸ä¸€è‡´ã€‚æœŸæœ›é•¿åº¦: {expected_length}, å®é™…é•¿åº¦: {len(spec)}")
                inconsistent_indices.append(i)
        
        if inconsistent_indices:
            print(f"å‘ç° {len(inconsistent_indices)} ä¸ªå…‰è°±é•¿åº¦ä¸ä¸€è‡´ï¼Œå°†ä»æ•°æ®é›†ä¸­ç§»é™¤")
            # ä»åå¾€å‰åˆ é™¤ï¼Œé¿å…ç´¢å¼•å˜åŒ–
            for i in sorted(inconsistent_indices, reverse=True):
                print(f"ç§»é™¤ä¸ä¸€è‡´å…‰è°±: {filenames[i]}")
                del spectra[i]
                del labels[i]
                del filenames[i]
                del elements[i]
        
        if not spectra:
            print("ç§»é™¤ä¸ä¸€è‡´å…‰è°±åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return None, None, None, None
        
        try:
            # è½¬æ¢ä¸ºNumPyæ•°ç»„
            X = np.array(spectra, dtype=np.float32)
            y = np.array(labels, dtype=np.float32)
            
            # æœ€ç»ˆæ£€æŸ¥Xä¸­çš„æ— æ•ˆå€¼
            n_samples, n_features = X.shape
            for i in range(n_samples):
                if np.isnan(X[i]).any() or np.isinf(X[i]).any():
                    nan_count = np.isnan(X[i]).sum()
                    inf_count = np.isinf(X[i]).sum()
                    print(f"è­¦å‘Š: æ ·æœ¬ {i} ({filenames[i]}) åŒ…å« {nan_count} ä¸ªNaNå’Œ {inf_count} ä¸ªæ— é™å€¼ï¼Œæ›¿æ¢ä¸º0")
                    X[i] = np.nan_to_num(X[i], nan=0.0, posinf=0.0, neginf=0.0)
            
            # æ£€æŸ¥yä¸­çš„æ— æ•ˆå€¼
            if np.isnan(y).any() or np.isinf(y).any():
                nan_count = np.isnan(y).sum()
                inf_count = np.isinf(y).sum()
                print(f"è­¦å‘Š: æ ‡ç­¾ä¸­åŒ…å« {nan_count} ä¸ªNaNå’Œ {inf_count} ä¸ªæ— é™å€¼")
                # æ‰¾å‡ºåŒ…å«NaNçš„æ ·æœ¬
                nan_indices = np.where(np.isnan(y))[0]
                for idx in nan_indices:
                    print(f"  æ ·æœ¬ {idx} ({filenames[idx]}) çš„æ ‡ç­¾ä¸ºNaN")
                # æ›¿æ¢NaNä¸ºä¸­ä½æ•°
                if nan_count > 0:
                    median_y = np.nanmedian(y)
                    print(f"  ä½¿ç”¨ä¸­ä½æ•° {median_y} æ›¿æ¢æ ‡ç­¾ä¸­çš„NaN")
                    y = np.nan_to_num(y, nan=median_y)
            
            print(f"å‡†å¤‡å®Œæˆ {len(X)} ä¸ªæ ·æœ¬, ç‰¹å¾æ•°: {X.shape[1]}")
            return X, y, filenames, elements
            
        except ValueError as e:
            print(f"åˆ›å»ºæ•°ç»„æ—¶å‡ºé”™: {e}")
            print("å°è¯•è¯Šæ–­é—®é¢˜...")
            
            # æ”¶é›†é•¿åº¦ä¿¡æ¯è¿›è¡Œè¯Šæ–­
            lengths = [len(spec) for spec in spectra]
            unique_lengths = set(lengths)
            
            if len(unique_lengths) > 1:
                print(f"å‘ç°å¤šç§ä¸åŒçš„å…‰è°±é•¿åº¦: {unique_lengths}")
                
                # é€‰æ‹©æœ€å¸¸è§çš„é•¿åº¦
                from collections import Counter
                length_counts = Counter(lengths)
                most_common_length = length_counts.most_common(1)[0][0]
                print(f"æœ€å¸¸è§çš„å…‰è°±é•¿åº¦ä¸º {most_common_length}ï¼Œå°†åªä¿ç•™è¿™ä¸ªé•¿åº¦çš„å…‰è°±")
                
                # åªä¿ç•™é•¿åº¦ä¸€è‡´çš„å…‰è°±
                consistent_data = []
                for i, spec in enumerate(spectra):
                    if len(spec) == most_common_length:
                        consistent_data.append((spectra[i], labels[i], filenames[i], elements[i]))
                
                if not consistent_data:
                    print("æ²¡æœ‰è¶³å¤Ÿçš„ä¸€è‡´é•¿åº¦å…‰è°±")
                    return None, None, None, None
                
                # é‡å»ºæ•°æ®
                spectra, labels, filenames, elements = zip(*consistent_data)
                
                # å†æ¬¡å°è¯•åˆ›å»ºæ•°ç»„
                try:
                    X = np.array(spectra, dtype=np.float32)
                    y = np.array(labels, dtype=np.float32)
                    print(f"æˆåŠŸåˆ›å»ºä¸€è‡´é•¿åº¦çš„æ•°ç»„: {X.shape}")
                    
                    # æœ€ç»ˆæ£€æŸ¥æ— æ•ˆå€¼
                    n_samples, n_features = X.shape
                    for i in range(n_samples):
                        if np.isnan(X[i]).any() or np.isinf(X[i]).any():
                            X[i] = np.nan_to_num(X[i], nan=0.0, posinf=0.0, neginf=0.0)
                    
                    if np.isnan(y).any() or np.isinf(y).any():
                        y = np.nan_to_num(y, nan=np.nanmedian(y))
                    
                    return X, y, filenames, elements
                except Exception as e2:
                    print(f"ç¬¬äºŒæ¬¡å°è¯•åˆ›å»ºæ•°ç»„æ—¶å‡ºé”™: {e2}")
                    return None, None, None, None
            else:
                print(f"æ‰€æœ‰å…‰è°±é•¿åº¦éƒ½ç›¸åŒ ({lengths[0]})ï¼Œä½†ä»ç„¶å‘ç”Ÿé”™è¯¯")
                return None, None, None, None
    
    def split_dataset(self, X, y, elements, ask_for_split=True):
        """å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†"""
        if ask_for_split:
            response = input("æ˜¯å¦è¦è¿›è¡Œæ•°æ®é›†åˆ’åˆ†ï¼Ÿ(y/n): ").lower()
            if response != 'y':
                print(f"è·³è¿‡æ•°æ®é›†åˆ’åˆ†ï¼Œå°†å®Œæ•´æ•°æ®é›†ä¿å­˜åˆ° {os.path.join(self.output_dir, 'full_dataset.npz')}")
                
                # ä¿å­˜å®Œæ•´æ•°æ®é›†ï¼Œä½¿ç”¨å·²æœ‰çš„æ•°æ®
                np.savez(os.path.join(self.output_dir, 'full_dataset.npz'),
                        X=X, y=y, elements=elements)
                
                print(f"å®Œæ•´æ•°æ®é›†ä¿å­˜å®Œæˆ: {X.shape[0]}æ¡è®°å½•")
                
                # è¿”å›å®Œæ•´æ•°æ®é›†ä½œä¸ºè®­ç»ƒé›†ï¼Œç©ºæ•°ç»„ä½œä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†
                return (X, y, elements), (np.array([]), np.array([]), np.array([])), (np.array([]), np.array([]), np.array([]))
        
        # æ£€æŸ¥æ•°æ®é›†å¤§å°ï¼Œå¦‚æœæ ·æœ¬æ•°é‡å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œåˆ†å‰²
        n_samples = X.shape[0]
        if n_samples <= 5:  # å¦‚æœæ ·æœ¬æ•°é‡å¤ªå°‘ï¼Œä¸è¿›è¡Œåˆ†å‰²
            print(f"è­¦å‘Š: æ ·æœ¬æ•°é‡({n_samples})å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„æ•°æ®é›†åˆ’åˆ†")
            print(f"å°†ç›¸åŒçš„æ•°æ®é›†ç”¨ä½œè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†")
            
            # ä¿å­˜ç›¸åŒçš„æ•°æ®é›†
            np.savez(os.path.join(self.output_dir, 'train_dataset.npz'),
                    spectra=X, abundance=y, elements=elements)
            np.savez(os.path.join(self.output_dir, 'val_dataset.npz'),
                    spectra=X, abundance=y, elements=elements)
            np.savez(os.path.join(self.output_dir, 'test_dataset.npz'),
                    spectra=X, abundance=y, elements=elements)
            
            print(f"æ•°æ®é›†ä¿å­˜å®Œæˆ: æ¯ä¸ªé›†åˆ {n_samples} æ¡è®°å½•")
            
            return (X, y, elements), (X, y, elements), (X, y, elements)
        
        # åŸæœ‰çš„æ•°æ®é›†åˆ†å‰²é€»è¾‘ï¼Œç”¨äºæ ·æœ¬æ•°é‡è¶³å¤Ÿçš„æƒ…å†µ
        # é¦–å…ˆåˆ†å‰²å‡ºæµ‹è¯•é›† (80% vs 20%)
        X_temp, X_test, y_temp, y_test, elements_temp, elements_test = train_test_split(
            X, y, elements, test_size=0.2, random_state=42)
        
        # å†ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‰²å‡ºéªŒè¯é›† (70% vs 10%)
        X_train, X_val, y_train, y_val, elements_train, elements_val = train_test_split(
            X_temp, y_temp, elements_temp, test_size=1/8, random_state=42)
        
        # ä¿å­˜æ•°æ®é›†
        np.savez(os.path.join(self.output_dir, 'train_dataset.npz'),
                spectra=X_train, abundance=y_train, elements=elements_train)
        np.savez(os.path.join(self.output_dir, 'val_dataset.npz'),
                spectra=X_val, abundance=y_val, elements=elements_val)
        np.savez(os.path.join(self.output_dir, 'test_dataset.npz'),
                spectra=X_test, abundance=y_test, elements=elements_test)
        
        print(f"æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
        print(f"è®­ç»ƒé›†: {X_train.shape[0]}æ¡ (70%)")
        print(f"éªŒè¯é›†: {X_val.shape[0]}æ¡ (10%)")
        print(f"æµ‹è¯•é›†: {X_test.shape[0]}æ¡ (20%)")
        
        return (X_train, y_train, elements_train), (X_val, y_val, elements_val), (X_test, y_test, elements_test)
    
    def predict_abundance(self, fits_file, model):
        """ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹å•ä¸ªå…‰è°±çš„ä¸°åº¦"""
        # å¤„ç†å…‰è°±
        result = self.process_single_spectrum(fits_file, 0.0)  # ä½¿ç”¨å ä½ç¬¦æ ‡ç­¾
        if result is None:
            print(f"æ— æ³•å¤„ç†æ–‡ä»¶: {fits_file}")
            return None
        
        # è·å–å¤„ç†åçš„å…‰è°±æ•°æ®
        spectrum = None
        if 'data' in result:
            spectrum = result['data']
        elif 'spectrum' in result:
            spectrum = result['spectrum']
        
        if spectrum is None:
            print(f"æ— æ³•è·å–å…‰è°±æ•°æ®: {fits_file}")
            return None
            
        # ç¡®ä¿å…‰è°±æ˜¯äºŒç»´æ•°ç»„ (æ ·æœ¬æ•°, ç‰¹å¾æ•°)
        spectrum_array = np.array(spectrum).reshape(1, -1)
        
        # è¿›è¡Œé¢„æµ‹
        try:
            prediction = model.predict(spectrum_array)
            print(f"é¢„æµ‹ç»“æœ: {prediction[0]}")
            return prediction[0]
        except Exception as e:
            print(f"é¢„æµ‹æ—¶å‡ºé”™: {e}")
            return None
    
    def visualize_spectrum(self, spec_file, processed=True, save=True):
        """å¯è§†åŒ–å•ä¸ªå…‰è°±ï¼ŒåŸå§‹å…‰è°±æˆ–å¤„ç†åçš„å…‰è°±"""
        if processed:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜
            cache_key = f"processed_{spec_file.replace('/', '_')}"
            cached_data = self.cache_manager.get_cache(cache_key)
            
            if cached_data is None:
                # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œå¤„ç†å…‰è°±
                print(f"æ²¡æœ‰æ‰¾åˆ°å¤„ç†åçš„å…‰è°±ç¼“å­˜ï¼Œé‡æ–°å¤„ç†: {spec_file}")
                processed_data = self.process_single_spectrum(spec_file, 0.0)  # ä½¿ç”¨å ä½ç¬¦æ ‡ç­¾
                if processed_data is None:
                    print(f"æ— æ³•å¤„ç†æ–‡ä»¶: {spec_file}")
                    return
            else:
                processed_data = cached_data
                
            # æå–æ•°æ®ï¼Œæ”¯æŒæ–°æ—§ç¼“å­˜ç»“æ„
            if 'metadata' in processed_data:
                metadata = processed_data['metadata']
                original_wavelength = metadata.get('original_wavelength')
                original_flux = metadata.get('original_flux')
                wavelength_calibrated = metadata.get('wavelength_calibrated')
                wavelength_corrected = metadata.get('wavelength_corrected')
                wavelength_rest = metadata.get('wavelength_rest')
                denoised_flux = metadata.get('denoised_flux')
                wavelength_resampled = metadata.get('wavelength_resampled')
                flux_resampled = metadata.get('flux_resampled')
                flux_continuum = metadata.get('flux_continuum')
                flux_denoised_second = metadata.get('flux_denoised_second')
                z = metadata.get('z', 0)
                spectrum = processed_data.get('data')
            else:
                # å…¼å®¹æ—§æ ¼å¼
                original_wavelength = processed_data.get('original_wavelength')
                original_flux = processed_data.get('original_flux')
                wavelength_calibrated = processed_data.get('wavelength_calibrated')
                wavelength_corrected = processed_data.get('wavelength_corrected')
                wavelength_rest = processed_data.get('wavelength_rest')
                denoised_flux = processed_data.get('denoised_flux')
                wavelength_resampled = processed_data.get('wavelength_resampled')
                flux_resampled = processed_data.get('flux_resampled')
                flux_continuum = processed_data.get('flux_continuum')
                flux_denoised_second = processed_data.get('flux_denoised_second')
                z = processed_data.get('z', 0)
                spectrum = processed_data.get('spectrum')
        
        # è®¾ç½®å­—ä½“å’Œå›¾å½¢æ ·å¼
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(14, 16))  # è°ƒæ•´é«˜åº¦ä»¥å®¹çº³å››å¼ å­å›¾
        
        # è·å–å…‰è°±ç±»å‹å’Œè§‚æµ‹æ—¥æœŸä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        spec_type = ""
        obs_date = ""
        try:
            file_path = self._get_file_extension(spec_file)
            with fits.open(file_path, ignore_missing_end=True) as hdul:
                header = hdul[0].header
                # å°è¯•è·å–å…‰è°±ç±»å‹
                if 'OBJTYPE' in header:
                    spec_type = header['OBJTYPE']
                elif 'CLASS' in header:
                    spec_type = header['CLASS']
                # å°è¯•è·å–è§‚æµ‹æ—¥æœŸ
                if 'DATE-OBS' in header:
                    obs_date = header['DATE-OBS']
                elif 'MJD' in header:
                    obs_date = f"MJD: {header['MJD']}"
                # è·å–èµ¤ç»èµ¤çº¬
                ra = header.get('RA', '')
                dec = header.get('DEC', '')
                # è·å–çº¢ç§»æˆ–è§†å‘é€Ÿåº¦
                z = header.get('Z', '')
                if z:
                    spec_type = f"{spec_type} z={z}" if spec_type else f"z={z}"
                # è·å–æ’æ˜Ÿç±»å‹
                if 'OBJTYPE' in header and header['OBJTYPE'] == 'STAR':
                    star_type = header.get('SPTCLASS', '')
                    if star_type:
                        spec_type = f"STAR {star_type}"
                # è·å–è§†å‘é€Ÿåº¦
                v_helio_header = header.get('V_HELIO', None)
                if v_helio_header is not None:
                    v_helio_err = header.get('V_HELIO_ERR', '')
                    v_helio_text = f"cz = {v_helio_header:.1f}"
                    if v_helio_err:
                        v_helio_text += f" Â± {v_helio_err:.1f} km/s"
                    spec_type = f"{spec_type}\n{v_helio_text}" if spec_type else v_helio_text
        except:
            pass
        
        # å¦‚æœheaderä¸­æ²¡æœ‰æ‰¾åˆ°çº¢ç§»å€¼ï¼Œå°è¯•ä»CSVæ–‡ä»¶ä¸­è·å–
        if not z:
            try:
                for csv_file in self.csv_files:
                    if os.path.exists(csv_file):
                        df = pd.read_csv(csv_file)
                        if 'spec' in df.columns and 'z' in df.columns:
                            # æå–æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾„å’Œæ‰©å±•åï¼‰
                            base_file = os.path.basename(spec_file)
                            if '.' in base_file:
                                base_file = base_file.split('.')[0]
                            
                            # åœ¨CSVä¸­æŸ¥æ‰¾åŒ¹é…è®°å½•
                            matches = df[df['spec'].str.contains(base_file, case=False, na=False)]
                            if not matches.empty:
                                z = matches.iloc[0]['z']
                                print(f"ä»CSVæ‰¾åˆ°çº¢ç§»å€¼: z = {z}")
                                break
            except Exception as e:
                print(f"ä»CSVæŸ¥æ‰¾çº¢ç§»æ•°æ®å‡ºé”™: {e}")
        
        # å®šä¹‰ä¸»è¦å¸æ”¶çº¿ä½ç½®(åŸƒ)å’Œæ ‡ç­¾
        absorption_lines = {
            'CaII K': 3933.7,
            'CaII H': 3968.5,
            'HÎ´': 4101.7,
            'HÎ³': 4340.5,
            'HÎ²': 4861.3,
            'Mg': 5175.3,  # MgI bä¸‰é‡çº¿ä¸­å¿ƒ
            'Na D': 5892.9, # NaI DåŒçº¿ä¸­å¿ƒ
            'HÎ±': 6562.8,
            'OI': 7774.2,
            'CaII IR1': 8498.0, # é’™ä¸‰é‡çº¿
            'CaII IR2': 8542.1,
            'CaII IR3': 8662.1
        }
        
        # ä¸ºå¤„ç†ç‰¹å®šå¯†é›†åŒºåŸŸæ ‡ç­¾ï¼Œæ‰‹åŠ¨å®šä¹‰ä¸€äº›æ ‡ç­¾çš„æ°´å¹³åç§»
        # å€¼ä¸ºå·¦å³åç§»ç™¾åˆ†æ¯”ï¼Œæ­£å€¼å‘å³ï¼Œè´Ÿå€¼å‘å·¦
        label_shifts = {
            'CaII K': -1.0,    # åŠ å¤§å·¦ç§»
            'CaII H': 1.0,     # åŠ å¤§å³ç§»
            'HÎ´': -0.5,        # å·¦ç§»
            'HÎ³': 0.5,         # å³ç§»
            'CaII IR1': -1.5,  # å¤§å¹…å·¦ç§»
            'CaII IR2': 0,     # ä¸åŠ¨
            'CaII IR3': 1.5    # å¤§å¹…å³ç§»
        }
        
        def plot_with_labels(ax, x, y, wave_range, label_data, color='blue', label_name=''):
            """åœ¨æŒ‡å®šçš„è½´ä¸Šç»˜åˆ¶å¸¦æ ‡ç­¾çš„å›¾å½¢
            
            Args:
                ax: matplotlibè½´å¯¹è±¡
                x: æ³¢é•¿æ•°ç»„
                y: æµé‡æ•°ç»„
                wave_range: (æœ€å°æ³¢é•¿, æœ€å¤§æ³¢é•¿)
                label_data: å¸æ”¶çº¿æ•°æ®å­—å…¸
                color: çº¿æ¡é¢œè‰²
                label_name: å›¾ä¾‹æ ‡ç­¾
            """
            # ç»˜åˆ¶ä¸»æ›²çº¿
            ax.plot(x, y, label=label_name, color=color)
            ax.set_xlabel('Wavelength (Ã…)')
            
            # è·å–å½“å‰yè½´èŒƒå›´
            ymin, ymax = ax.get_ylim()
            y_range = ymax - ymin
            wave_min, wave_max = wave_range
            x_range = wave_max - wave_min
            
            # é‡ç½®æ ‡ç­¾ä½ç½®è·Ÿè¸ª
            label_positions = {}  # æ ¼å¼ï¼š{wave: (vertical_offset, horizontal_offset)}
            
            # å¯¹å¸æ”¶çº¿æŒ‰æ³¢é•¿æ’åº
            sorted_lines = sorted(label_data.items(), key=lambda x: x[1])
            
            # å…ˆä¸ºé¢„å®šä¹‰çš„æ ‡ç­¾åˆ†é…ä½ç½®
            for name, wave in sorted_lines:
                if wave >= x.min() and wave <= x.max() and name in label_shifts:
                    # å‚ç›´åç§»äº¤æ›¿æ”¾ç½®ï¼š0=ä¸‹ï¼Œ1=ä¸Š
                    v_offset = 0 if len(label_positions) % 2 == 0 else 1
                    label_positions[wave] = (v_offset, label_shifts[name])
            
            # å†å¤„ç†å…¶ä»–æ ‡ç­¾
            for name, wave in sorted_lines:
                if wave >= x.min() and wave <= x.max() and wave not in label_positions:
                    # æ£€æŸ¥æ­¤æ ‡ç­¾æ˜¯å¦å¤ªé è¿‘å…¶ä»–æ ‡ç­¾
                    too_close = []
                    for prev_wave in label_positions:
                        # è®¡ç®—æ³¢é•¿å·®é™¤ä»¥æ•´ä¸ªæ³¢é•¿èŒƒå›´çš„ç™¾åˆ†æ¯”
                        distance_percent = abs(wave - prev_wave) / x_range
                        if distance_percent < 0.05:  # ä½¿ç”¨5%ä½œä¸ºè·ç¦»åˆ¤æ–­
                            too_close.append((prev_wave, distance_percent))
                    
                    # æ ¹æ®æ¥è¿‘ç¨‹åº¦å†³å®šä½ç½®åç§»
                    v_offset = 0  # å‚ç›´åç§»(0=ä¸‹ï¼Œ1=ä¸Š)
                    h_shift = 0   # æ°´å¹³åç§»
                    
                    if too_close:
                        # æŒ‰æ¥è¿‘ç¨‹åº¦æ’åº
                        too_close.sort(key=lambda x: x[1])
                        
                        # æ”¶é›†é™„è¿‘æ ‡ç­¾çš„å‚ç›´ä½ç½®ï¼Œé¿å…ä½¿ç”¨ç›¸åŒé«˜åº¦
                        nearby_v_offsets = [label_positions.get(w)[0] for w, _ in too_close]
                        
                        # ä¼˜å…ˆé€‰æ‹©ä¸åŒçš„å‚ç›´ä½ç½®
                        if 0 in nearby_v_offsets and 1 not in nearby_v_offsets:
                            v_offset = 1
                        elif 1 in nearby_v_offsets and 0 not in nearby_v_offsets:
                            v_offset = 0
                        else:
                            # å¦‚æœä¸¤ç§å‚ç›´ä½ç½®éƒ½è¢«ä½¿ç”¨ï¼Œåˆ™ä½¿ç”¨è·ç¦»æœ€è¿‘æ ‡ç­¾çš„åå‘ä½ç½®
                            closest_wave, _ = too_close[0]
                            closest_v_offset, _ = label_positions.get(closest_wave, (0, 0))
                            v_offset = 1 - closest_v_offset
                        
                        # æ°´å¹³åç§»å¤„ç†ï¼šæ ¹æ®é™„è¿‘æ ‡ç­¾æƒ…å†µåˆ†é…æ°´å¹³ä½ç½®
                        nearby_h_shifts = [label_positions.get(w)[1] for w, _ in too_close]
                        
                        # æ‰¾å‡ºæœªè¢«å ç”¨çš„æ–¹å‘
                        if all(h < 0 for h in nearby_h_shifts):
                            h_shift = 1.0  # å¦‚æœé™„è¿‘éƒ½åœ¨å·¦è¾¹ï¼Œåˆ™æ”¾å³è¾¹
                        elif all(h > 0 for h in nearby_h_shifts):
                            h_shift = -1.0  # å¦‚æœé™„è¿‘éƒ½åœ¨å³è¾¹ï¼Œåˆ™æ”¾å·¦è¾¹
                        else:
                            # å¤æ‚æƒ…å†µï¼Œå°è¯•æ‰¾åˆ°æœ€å¤§é—´éš™
                            nearby_h_shifts.append(-2.0)  # å·¦è¾¹ç•Œ
                            nearby_h_shifts.append(2.0)   # å³è¾¹ç•Œ
                            nearby_h_shifts.sort()
                            
                            max_gap = 0
                            best_pos = 0
                            
                            for i in range(len(nearby_h_shifts) - 1):
                                gap = nearby_h_shifts[i+1] - nearby_h_shifts[i]
                                if gap > max_gap:
                                    max_gap = gap
                                    best_pos = nearby_h_shifts[i] + gap/2
                            
                            h_shift = best_pos
                    
                    # è®°å½•æ­¤æ ‡ç­¾çš„ä½ç½®åç§»
                    label_positions[wave] = (v_offset, h_shift)
            
            # ç»˜åˆ¶çº¿æ¡å’Œæ ‡ç­¾
            for name, wave in sorted_lines:
                if wave >= x.min() and wave <= x.max():
                    # æ·»åŠ å‚ç›´çº¿ - ä½¿ç”¨è¾ƒæµ…çš„é€æ˜åº¦ï¼Œå‡å°‘å¯¹å›¾åƒçš„å¹²æ‰°
                    ax.axvline(x=wave, color='red', linestyle=':', alpha=0.5, linewidth=0.7)
                    
                    # è·å–æ­¤æ ‡ç­¾çš„ä½ç½®ä¿¡æ¯
                    v_offset, h_shift = label_positions.get(wave, (0, 0))
                    
                    # è®¡ç®—æ ‡ç­¾ä½ç½®ï¼Œç¡®ä¿æ°´å¹³ä½ç½®æœ‰è¶³å¤Ÿåç§»
                    x_pos = wave + h_shift * 0.04 * x_range  # å¢åŠ æ°´å¹³åç§»é‡åˆ°4%
                    
                    # è®¡ç®—å‚ç›´ä½ç½®ï¼Œç¡®ä¿ä¸åŒé«˜åº¦çš„æ ‡ç­¾æœ‰æ˜æ˜¾å·®åˆ«
                    if v_offset == 0:
                        y_pos = ymax + y_range * 0.035  # ä½ä½ç½®
                    else:
                        y_pos = ymax + y_range * 0.12  # é«˜ä½ç½®ï¼Œå¢åŠ å·®å¼‚
                    
                    # ç»˜åˆ¶æ ‡ç­¾ï¼Œä½¿ç”¨ç™½åº•é»‘å­—æé«˜å¯è¯»æ€§
                    ax.text(x_pos, y_pos, name, rotation=0, 
                            horizontalalignment='center', verticalalignment='bottom',
                            fontsize=8, alpha=0.9, bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=1))
            
            # è°ƒæ•´yè½´ä¸Šé™ï¼Œä¸ºæ ‡ç­¾ç•™å‡ºç©ºé—´
            ax.set_ylim(ymin, ymax + y_range * 0.2)  # å¢åŠ ä¸Šæ–¹ç©ºé—´
            ax.grid(True, linestyle='--', alpha=0.5)  # å‡å°‘ç½‘æ ¼çº¿çš„æ˜æ˜¾ç¨‹åº¦
        
        # æ˜¾ç¤ºåŸå§‹å…‰è°±
        ax1 = plt.subplot(4, 1, 1)
        plot_with_labels(ax1, original_wavelength, original_flux, 
                         (min(original_wavelength), max(original_wavelength)), 
                         absorption_lines, color='blue', label_name='Raw Spectrum')
        ax1.set_ylabel('Flux (relative)')
        overall_title = f"Spectrum: {os.path.basename(spec_file)}"
        ax1.set_title(overall_title)
        
        # åœ¨ç¬¬ä¸€ä¸ªå­å›¾ä¸‹æ–¹æ·»åŠ çº¢ç§»å’Œè§‚æµ‹æ—¥æœŸä¿¡æ¯
        if z or obs_date:
            info_text = ""
            if z:
                info_text += f"Star z={z} "
            if obs_date:
                info_text += f"{obs_date}"
            ax1.text(0.02, -0.2, info_text, transform=ax1.transAxes, fontsize=9)
        
        # å¦‚æœæ˜¯å¤„ç†åçš„å…‰è°±ï¼Œæ˜¾ç¤ºå¤„ç†æ•ˆæœ
        if processed:
            # æ ¡å‡†ã€é€Ÿåº¦æ ¡æ­£ã€å»å™ªã€çº¢ç§»æ ¡æ­£å’Œé‡é‡‡æ ·åçš„ç»“æœ
            ax2 = plt.subplot(4, 1, 2)
            plot_with_labels(ax2, wavelength_resampled, flux_resampled, 
                            (min(wavelength_resampled), max(wavelength_resampled)), 
                            absorption_lines, color='green', label_name='Calibrated, Velocity Corrected, Denoised, Redshift Corrected & Resampled')
            ax2.set_ylabel('Flux')
            ax2.set_title("Spectrum after Calibration, Velocity Correction, Denoising, Redshift Correction & Resampling")
            
            # è¿ç»­è°±å½’ä¸€åŒ– - è¿™æ˜¯ç¬¬ä¸‰å¼ å›¾
            ax3 = plt.subplot(4, 1, 3)
            plot_with_labels(ax3, wavelength_resampled, flux_continuum, 
                            (min(wavelength_resampled), max(wavelength_resampled)), 
                            absorption_lines, color='purple', label_name='Continuum Normalized')
            ax3.set_ylabel('Normalized Flux')
            ax3.set_title("Spectrum after Continuum Normalization")
            
            # äºŒæ¬¡å»å™ªå’Œæœ€ç»ˆå½’ä¸€åŒ– - è¿™æ˜¯ç¬¬å››å¼ å›¾
            ax4 = plt.subplot(4, 1, 4)
            # ç¡®ä¿æœ€ç»ˆå½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
            spectrum_normalized = np.clip(spectrum, 0, 1)
            plot_with_labels(ax4, wavelength_resampled, spectrum_normalized, 
                            (min(wavelength_resampled), max(wavelength_resampled)), 
                            absorption_lines, color='red', label_name='Fully Processed')
            ax4.set_ylabel('Final Normalized Flux')
            ax4.set_title("Spectrum after Second Denoising and Final Normalization")
            ax4.set_ylim(0, 1)  # è®¾ç½®yè½´èŒƒå›´ä¸º[0,1]
        
        else:
            # å¦‚æœä¸æ˜¯å¤„ç†åå…‰è°±ï¼Œåˆ™ä½¿ç”¨åŸå§‹å…‰è°±è¿›è¡Œå¤„ç†å¹¶æ˜¾ç¤º
            
            # 1. æ³¢é•¿æ ¡æ­£
            wavelength_calibrated = self.correct_wavelength(original_wavelength, original_flux)
            print(f"æ³¢é•¿æ ¡æ­£å: æ³¢é•¿èŒƒå›´{wavelength_calibrated[0]}~{wavelength_calibrated[-1]}")
            
            # ä»FITSæ–‡ä»¶è¯»å–è§†å‘é€Ÿåº¦
            v_helio = 0  # é»˜è®¤å€¼
            try:
                _, _, v_helio, _, _, _ = self.read_fits_file(spec_file)
            except Exception as e:
                print(f"è¯»å–FITSæ–‡ä»¶è·å–è§†å‘é€Ÿåº¦æ—¶å‡ºé”™: {e}")
            
            # 2. è§†å‘é€Ÿåº¦æ ¡æ­£
            wavelength_corrected = self.correct_velocity(wavelength_calibrated, original_flux, v_helio)
            print(f"è§†å‘é€Ÿåº¦æ ¡æ­£å: æ³¢é•¿èŒƒå›´{wavelength_corrected[0]}~{wavelength_corrected[-1]}")
            
            # 3. å»å™ª
            flux_denoised = self.denoise_spectrum(wavelength_corrected, original_flux)
            if flux_denoised is None:
                print(f"å»å™ª{spec_file}å¤±è´¥")
                return
            
            # 4. çº¢ç§»æ ¡æ­£
            wavelength_rest = self.correct_redshift(wavelength_corrected, flux_denoised, z)
            print(f"çº¢ç§»æ ¡æ­£å: æ³¢é•¿èŒƒå›´{wavelength_rest[0]}~{wavelength_rest[-1]}")
            
            # 5. é‡é‡‡æ ·
            print(f"é‡é‡‡æ ·åˆ°æ³¢é•¿èŒƒå›´: {self.wavelength_range}, ç‚¹æ•°={self.n_points}")
            wavelength_resampled, flux_resampled = self.resample_spectrum(wavelength_rest, flux_denoised)
            if wavelength_resampled is None or flux_resampled is None:
                print(f"é‡é‡‡æ ·{spec_file}å¤±è´¥")
                return
            
            # 6. è¿ç»­è°±å½’ä¸€åŒ–
            flux_continuum, continuum_params = self.normalize_continuum(wavelength_resampled, flux_resampled)
            if flux_continuum is None:
                print(f"è¿ç»­è°±å½’ä¸€åŒ–{spec_file}å¤±è´¥")
                return None
            
            # 7. äºŒæ¬¡å»å™ª
            flux_denoised_second = self.denoise_spectrum_second(wavelength_resampled, flux_continuum)
            
            # 8. æœ€ç»ˆå½’ä¸€åŒ– (æœ€å¤§æœ€å°å€¼å½’ä¸€åŒ–)
            print(f"å¯¹æµé‡è¿›è¡Œæœ€ç»ˆå½’ä¸€åŒ–")
            flux_normalized, norm_params = self.normalize_spectrum(flux_denoised_second)
            if flux_normalized is None:
                print(f"å½’ä¸€åŒ–{spec_file}å¤±è´¥")
                return
            
            spectrum = flux_normalized
            
            # æ˜¾ç¤ºå¤„ç†è¿‡ç¨‹ - è°ƒæ•´ä¸ºç¬¦åˆè¦æ±‚çš„æ ¼å¼
            ax2 = plt.subplot(4, 1, 2)
            # æ ¡å‡†ã€é€Ÿåº¦æ ¡æ­£ã€å»å™ªã€çº¢ç§»æ ¡æ­£å’Œé‡é‡‡æ ·åçš„ç»“æœ
            plot_with_labels(ax2, wavelength_resampled, flux_resampled, 
                           (min(wavelength_resampled), max(wavelength_resampled)), 
                           absorption_lines, color='green', label_name='Calibrated, Velocity Corrected, Denoised, Redshift Corrected & Resampled')
            ax2.set_ylabel('Flux')
            ax2.set_title("Spectrum after Calibration, Velocity Correction, Denoising, Redshift Correction & Resampling")
            
            # è¿ç»­è°±å½’ä¸€åŒ–åçš„å…‰è°±
            ax3 = plt.subplot(4, 1, 3)
            plot_with_labels(ax3, wavelength_resampled, flux_continuum, 
                           (min(wavelength_resampled), max(wavelength_resampled)), 
                           absorption_lines, color='purple', label_name='Continuum Normalized')
            ax3.set_ylabel('Normalized Flux')
            ax3.set_title("Spectrum after Continuum Normalization")
            
            # äºŒæ¬¡å»å™ªå’Œæœ€ç»ˆå½’ä¸€åŒ–åçš„å…‰è°±
            ax4 = plt.subplot(4, 1, 4)
            # ç¡®ä¿æœ€ç»ˆå½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
            spectrum_normalized = np.clip(spectrum, 0, 1)
            plot_with_labels(ax4, wavelength_resampled, spectrum_normalized, 
                           (min(wavelength_resampled), max(wavelength_resampled)), 
                           absorption_lines, color='red', label_name='Fully Processed')
            ax4.set_ylabel('Final Normalized Flux')
            ax4.set_title("Spectrum after Second Denoising and Final Normalization")
            ax4.set_ylim(0, 1)  # è®¾ç½®yè½´èŒƒå›´ä¸º[0,1]
        
        # æ·»åŠ æ³¢é•¿èŒƒå›´å’Œå¤„ç†ä¿¡æ¯
        if self.compute_common_range and len(self.processed_ranges) > 1:
            range_description = f'Wavelength Range: {self.wavelength_range[0]:.2f}-{self.wavelength_range[1]:.2f} Ã…'
        else:
            range_description = f'Wavelength Range: {self.wavelength_range[0]:.2f}-{self.wavelength_range[1]:.2f} Ã…'
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¯¹æ•°æ­¥é•¿
        if hasattr(self, 'log_step') and self.log_step:
            step_description = f'Log Step: {self.log_step} dex'
        else:
            step_description = f'Points: {len(wavelength_resampled)}'
        
        # æ·»åŠ çº¢ç§»ä¿¡æ¯
        z_description = f'Redshift: z = {z}' if z else ''
        
        info_text = f'{range_description}, {step_description}'
        if z_description:
            info_text += f', {z_description}'
        
        plt.figtext(0.5, 0.01, info_text, ha='center', fontsize=10)
        
        plt.tight_layout(pad=2.0, rect=[0, 0.02, 1, 0.98])
        
        if save:
            output_file = os.path.join(self.output_dir, f"{os.path.basename(spec_file)}_visualization.png")
            plt.savefig(output_file, dpi=150)
            print(f"Image saved to: {output_file}")
            plt.close()
        else:
            plt.show()
    
    def check_data_sources(self):
        """æ£€æŸ¥æ•°æ®æºæ˜¯å¦å­˜åœ¨å¹¶å¯ç”¨"""
        print("\n=== æ£€æŸ¥æ•°æ®æº ===")
        
        # æ£€æŸ¥æ˜¯å¦è·³è¿‡è¯¦ç»†æ£€æŸ¥
        if len(self.fits_file_cache) > 0:
            skip_check = input(f"å·²åŠ è½½{len(self.fits_file_cache)}ä¸ªæ–‡ä»¶è·¯å¾„ç¼“å­˜ã€‚æ˜¯å¦è·³è¿‡è¯¦ç»†FITSæ–‡ä»¶æ£€æŸ¥ï¼Ÿ(y/n): ").lower() == 'y'
            if skip_check:
                print("è·³è¿‡è¯¦ç»†FITSæ–‡ä»¶æ£€æŸ¥ï¼Œå‡å®šæ–‡ä»¶å­˜åœ¨")
                print("âœ“ CSVæ–‡ä»¶å·²å°±ç»ª")
                print("âœ“ FITSç›®å½•å·²å°±ç»ª")
                print("\n=== æ£€æŸ¥å®Œæˆ ===\n")
                return True
        
        # æ£€æŸ¥CSVæ–‡ä»¶
        csv_ok = True
        for i, csv_file in enumerate(self.csv_files, 1):
            if os.path.exists(csv_file):
                print(f"âœ“ {i}. CSVæ–‡ä»¶å­˜åœ¨: {csv_file}")
                # æ£€æŸ¥å¤§å°æ˜¯å¦åˆç†
                size_mb = os.path.getsize(csv_file) / (1024 * 1024)
                print(f"   å¤§å°: {size_mb:.2f} MB")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«specåˆ—
                if 'spec' in pd.read_csv(csv_file).columns:
                    print(f"  - 'spec'åˆ—å­˜åœ¨")
                else:
                    print(f"  âœ— é”™è¯¯: {csv_file} ä¸­æ²¡æœ‰'spec'åˆ—")
                    csv_ok = False
            else:
                print(f"âœ— {i}. CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
                csv_ok = False
        
        # æ£€æŸ¥fitsç›®å½•
        fits_ok = True
        if os.path.exists(self.fits_dir):
            print(f"\nâœ“ fitsç›®å½•å­˜åœ¨: {os.path.abspath(self.fits_dir)}")
            
            # æ£€æŸ¥fitsç›®å½•ç»“æ„
            all_fits_files = []
            for root, dirs, files in os.walk(self.fits_dir):
                for file in files:
                    if any(file.endswith(ext) for ext in ['.fits', '.fits.gz', '.fit', '.fit.gz']):
                        all_fits_files.append(os.path.join(root, file))
                    
            print(f"  - æ‰¾åˆ° {len(all_fits_files)} ä¸ªFITSæ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰")
            
            if all_fits_files:
                print(f"  - å‰5ä¸ªæ–‡ä»¶ç¤ºä¾‹:")
                for i, file in enumerate(all_fits_files[:5]):
                    rel_path = os.path.relpath(file, self.fits_dir)
                    print(f"    {i+1}. {rel_path}")
                
                # æ£€æŸ¥æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ç»“æ„
                directories = set()
                for file in all_fits_files:
                    rel_dir = os.path.relpath(os.path.dirname(file), self.fits_dir)
                    if rel_dir != '.':
                        directories.add(rel_dir)
                
                if directories:
                    print(f"  - å‘ç°åµŒå¥—ç›®å½•ç»“æ„:")
                    for d in list(directories)[:5]:
                        print(f"    - {d}")
                    if len(directories) > 5:
                        print(f"    - ... å…± {len(directories)} ä¸ªå­ç›®å½•")
                
                # æ£€æŸ¥CSVæ–‡ä»¶ä¸­çš„specå€¼æ˜¯å¦åŒ¹é…fitsæ–‡ä»¶
                for csv_file in self.csv_files:
                    if os.path.exists(csv_file):
                        df = pd.read_csv(csv_file)
                        if 'spec' in df.columns and len(df) > 0:
                            # æ£€æŸ¥specåˆ—çš„æ•°æ®ç±»å‹å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            if not pd.api.types.is_string_dtype(df['spec']):
                                print(f"\n  è­¦å‘Š: {csv_file} ä¸­çš„specåˆ—ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œæ­£åœ¨è½¬æ¢...")
                                df['spec'] = df['spec'].astype(str)
                            
                            spec_examples = df['spec'].iloc[:5].tolist()
                            print(f"\n  æ£€æŸ¥ {csv_file} ä¸­çš„specå€¼æ˜¯å¦åŒ¹é…fitsæ–‡ä»¶:")
                            
                            for spec in spec_examples:
                                # ä½¿ç”¨æ–°çš„æŸ¥æ‰¾é€»è¾‘
                                found_path = self._find_fits_file(spec)
                                if found_path:
                                    rel_path = os.path.relpath(found_path, self.fits_dir)
                                    print(f"    âœ“ {spec} å­˜åœ¨: {rel_path}")
                                else:
                                    print(f"    âœ— {spec} ä¸å­˜åœ¨ï¼Œåœ¨æ‰€æœ‰ç›®å½•ä¸­éƒ½æœªæ‰¾åˆ°")
                            
                            # æ£€æµ‹å¸¸è§æ–‡ä»¶æ ¼å¼
                            extensions = [os.path.splitext(f)[1] for f in all_fits_files[:20]]
                            common_exts = set(extensions)
                            if common_exts:
                                print(f"  FITSæ–‡ä»¶ä¸­çš„å¸¸è§åç¼€: {', '.join(common_exts)}")
        else:
            print(f"\nâœ— fitsç›®å½•ä¸å­˜åœ¨: {os.path.abspath(self.fits_dir)}")
            fits_ok = False
        
        print("\n=== æ£€æŸ¥å®Œæˆ ===\n")
        
        return csv_ok and fits_ok
    
    def clean_cache(self):
        """æœ‰é€‰æ‹©åœ°æ¸…ç†ä¸åŒç±»å‹çš„ç¼“å­˜æ–‡ä»¶"""
        print("\n=== ç¼“å­˜æ¸…ç† ===")
        cache_options = {
            '1': 'æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜ (FITSæ–‡ä»¶ä½ç½®)',
            '2': 'é¢„å¤„ç†ç»“æœç¼“å­˜ (æ‰€æœ‰å¤„ç†å¥½çš„å…‰è°±)',
            '3': 'è¿›åº¦æ–‡ä»¶ (å¤„ç†è¿›åº¦è®°å½•)',
            '4': 'é€‰æ‹©æ€§æ¸…é™¤é¢„å¤„ç†æ­¥éª¤ç¼“å­˜',
            '5': 'æ‰€æœ‰ç¼“å­˜',
            '0': 'é€€å‡º'
        }
        
        while True:
            print("\nè¯·é€‰æ‹©è¦æ¸…ç†çš„ç¼“å­˜ç±»å‹:")
            for key, value in cache_options.items():
                print(f"{key}. {value}")
            
            choice = input("è¯·è¾“å…¥é€‰é¡¹(0-5): ").strip()
            
            if choice == '0':
                print("é€€å‡ºç¼“å­˜æ¸…ç†")
                break
                
            elif choice == '1':
                # æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜
                files_cache_path = self.cache_file
                if os.path.exists(files_cache_path):
                    try:
                        with open(files_cache_path, 'rb') as f:
                            cache_data = pickle.load(f)
                            cache_size = len(cache_data) if isinstance(cache_data, dict) else 0
                        print(f"å‘ç°æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜: {files_cache_path}")
                        print(f"ç¼“å­˜åŒ…å« {cache_size} ä¸ªFITSæ–‡ä»¶ä½ç½®è®°å½•")
                        if input("ç¡®è®¤æ¸…ç†? (y/n): ").lower() == 'y':
                            os.remove(files_cache_path)
                            print("âœ“ æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜å·²æ¸…ç†")
                            # é‡ç½®å†…å­˜ä¸­çš„ç¼“å­˜
                            self.fits_file_cache = {}
                    except Exception as e:
                        print(f"è¯»å–æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜å¤±è´¥: {e}")
                else:
                    print("æœªå‘ç°æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜")
            
            elif choice == '2':
                # é¢„å¤„ç†ç»“æœç¼“å­˜
                cache_files = glob.glob(os.path.join(self.cache_dir, "*"))
                cache_files = [f for f in cache_files if os.path.basename(f) != os.path.basename(self.cache_file)]
                
                if cache_files:
                    print(f"å‘ç° {len(cache_files)} ä¸ªé¢„å¤„ç†ç¼“å­˜æ–‡ä»¶")
                    if input("ç¡®è®¤æ¸…ç†æ‰€æœ‰é¢„å¤„ç†ç»“æœç¼“å­˜? (y/n): ").lower() == 'y':
                        for file in tqdm(cache_files, desc="æ¸…ç†é¢„å¤„ç†ç¼“å­˜"):
                            try:
                                os.remove(file)
                            except Exception as e:
                                print(f"æ¸…ç†æ–‡ä»¶ {file} å¤±è´¥: {e}")
                        print("âœ“ æ‰€æœ‰é¢„å¤„ç†ç¼“å­˜å·²æ¸…ç†")
                else:
                    print("æœªå‘ç°é¢„å¤„ç†ç¼“å­˜æ–‡ä»¶")
            
            elif choice == '3':
                # è¿›åº¦æ–‡ä»¶
                progress_files = glob.glob(os.path.join(self.progress_dir, "*_progress.pkl"))
                all_progress = os.path.join(self.progress_dir, "all_progress.pkl")
                if os.path.exists(all_progress):
                    progress_files.append(all_progress)
                    
                if progress_files:
                    print(f"\nå‘ç° {len(progress_files)} ä¸ªè¿›åº¦æ–‡ä»¶:")
                    for file in progress_files:
                        print(f"  - {os.path.basename(file)}")
                    if input("ç¡®è®¤æ¸…ç†æ‰€æœ‰è¿›åº¦æ–‡ä»¶? (y/n): ").lower() == 'y':
                        for file in tqdm(progress_files, desc="æ¸…ç†è¿›åº¦æ–‡ä»¶"):
                            try:
                                os.remove(file)
                            except Exception as e:
                                print(f"æ¸…ç†æ–‡ä»¶ {file} å¤±è´¥: {e}")
                        print("âœ“ æ‰€æœ‰è¿›åº¦æ–‡ä»¶å·²æ¸…ç†")
                else:
                    print("æœªå‘ç°è¿›åº¦æ–‡ä»¶")
            
            elif choice == '4':
                # é€‰æ‹©æ€§æ¸…é™¤é¢„å¤„ç†æ­¥éª¤
                print("\nå¯é€‰çš„é¢„å¤„ç†æ­¥éª¤:")
                steps = {
                    '1': 'æ³¢é•¿æ ¡æ­£å’Œè§†å‘é€Ÿåº¦æ ¡æ­£',
                    '2': 'å»å™ªå’Œçº¢ç§»æ ¡æ­£',
                    '3': 'é‡é‡‡æ ·',
                    '4': 'è¿ç»­è°±å½’ä¸€åŒ–',
                    '5': 'äºŒæ¬¡å»å™ª',
                    '6': 'æœ€ç»ˆå½’ä¸€åŒ–',
                    '0': 'è¿”å›ä¸Šçº§èœå•'
                }
                
                for key, value in steps.items():
                    print(f"{key}. {value}")
                
                step_choice = input("è¯·é€‰æ‹©è¦æ¸…é™¤çš„é¢„å¤„ç†æ­¥éª¤(0-6): ").strip()
                
                if step_choice == '0':
                    continue
                    
                if step_choice in steps:
                    # æ£€æŸ¥å­˜åœ¨çš„ç¼“å­˜æ–‡ä»¶
                    cache_files = glob.glob(os.path.join(self.cache_dir, "*"))
                    cache_files = [f for f in cache_files if os.path.basename(f) != os.path.basename(self.cache_file)]
                    
                    if cache_files:
                        print(f"å‘ç° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
                        print("æ¸…é™¤é¢„å¤„ç†æ­¥éª¤ä¼šåˆ é™¤æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ï¼Œä½¿ç³»ç»Ÿåœ¨ä¸‹æ¬¡è¿è¡Œæ—¶é‡æ–°ç”Ÿæˆæ‰€æœ‰æ­¥éª¤")
                        
                        if input(f"ç¡®è®¤åˆ é™¤ç¼“å­˜ä»¥é‡æ–°æ‰§è¡Œ'{steps[step_choice]}'? (y/n): ").lower() == 'y':
                            for file in tqdm(cache_files, desc=f"æ¸…é™¤ç¼“å­˜"):
                                try:
                                    os.remove(file)
                                except Exception as e:
                                    print(f"æ¸…ç†æ–‡ä»¶ {file} å¤±è´¥: {e}")
                            print("âœ“ æ‰€æœ‰é¢„å¤„ç†ç¼“å­˜å·²æ¸…ç†")
                            print(f"ä¸‹æ¬¡è¿è¡Œæ—¶å°†é‡æ–°æ‰§è¡Œ'{steps[step_choice]}'åŠå…¶åç»­æ­¥éª¤")
                    else:
                        print("æœªå‘ç°é¢„å¤„ç†ç¼“å­˜æ–‡ä»¶")
                else:
                    print("æ— æ•ˆçš„é€‰æ‹©")
            
            elif choice == '5':
                # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
                if input("ç¡®è®¤æ¸…é™¤æ‰€æœ‰ç¼“å­˜? è¿™å°†åˆ é™¤æ‰€æœ‰æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜ã€é¢„å¤„ç†ç»“æœå’Œè¿›åº¦æ–‡ä»¶ (y/n): ").lower() == 'y':
                    # æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜
                    if os.path.exists(self.cache_file):
                        try:
                            os.remove(self.cache_file)
                            self.fits_file_cache = {}
                        except Exception as e:
                            print(f"æ¸…ç†æ–‡ä»¶æŸ¥æ‰¾ç¼“å­˜å¤±è´¥: {e}")
                    
                    # é¢„å¤„ç†ç»“æœç¼“å­˜
                    cache_files = glob.glob(os.path.join(self.cache_dir, "*"))
                    for file in tqdm(cache_files, desc="æ¸…ç†é¢„å¤„ç†ç¼“å­˜"):
                        try:
                            os.remove(file)
                        except Exception as e:
                            print(f"æ¸…ç†æ–‡ä»¶ {file} å¤±è´¥: {e}")
                    
                    # è¿›åº¦æ–‡ä»¶
                    progress_files = glob.glob(os.path.join(self.progress_dir, "*"))
                    for file in tqdm(progress_files, desc="æ¸…ç†è¿›åº¦æ–‡ä»¶"):
                        try:
                            os.remove(file)
                        except Exception as e:
                            print(f"æ¸…ç†æ–‡ä»¶ {file} å¤±è´¥: {e}")
                    
                    print("âœ“ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç†")
            
            else:
                print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                
        print("\n=== ç¼“å­˜æ¸…ç†å®Œæˆ ===\n")
    
    def check_and_fix_file_paths(self):
        """æ£€æŸ¥å¹¶ä¿®å¤æ–‡ä»¶è·¯å¾„é—®é¢˜"""
        print("\n=== è·¯å¾„é—®é¢˜è¯Šæ–­ ===")
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶åˆ—è¡¨
        test_files = []
        for csv_file in self.csv_files:
            if os.path.exists(csv_file):
                df = pd.read_csv(csv_file)
                if 'spec' in df.columns:
                    # ç¡®ä¿specåˆ—çš„ç±»å‹ä¸ºå­—ç¬¦ä¸²
                    if not pd.api.types.is_string_dtype(df['spec']):
                        print(f"æ³¨æ„: {csv_file} ä¸­çš„specåˆ—ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œæ­£åœ¨è½¬æ¢...")
                        df['spec'] = df['spec'].astype(str)
                    
                    for spec in df['spec'].values[:5]:  # åªå–å‰5ä¸ªæµ‹è¯•
                        spec = str(spec)  # ç¡®ä¿ç±»å‹ä¸ºå­—ç¬¦ä¸²
                        if spec not in test_files:
                            test_files.append(spec)
        
        if not test_files:
            print("æ— æ³•æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥CSVæ–‡ä»¶")
            return
        
        # æ˜¾ç¤ºfitsç›®å½•å†…å®¹
        print(f"FITSç›®å½•å†…å®¹ ({self.fits_dir}):")
        fits_files = []
        for root, _, files in os.walk(self.fits_dir):
            for file in files:
                if file.endswith(('.fits', '.fits.gz', '.fit', '.fit.gz')):
                    rel_path = os.path.relpath(os.path.join(root, file), self.fits_dir)
                    fits_files.append(rel_path)
        
        print(f"æ€»è®¡æ‰¾åˆ° {len(fits_files)} ä¸ªFITSæ–‡ä»¶")
        if fits_files:
            print("å‰5ä¸ªç¤ºä¾‹:")
            for i, file in enumerate(fits_files[:5]):
                print(f"  {i+1}. {file}")
        
        # æµ‹è¯•æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾
        print("\næ–‡ä»¶æŸ¥æ‰¾æµ‹è¯•:")
        for spec in test_files:
            # å†æ¬¡ç¡®ä¿specæ˜¯å­—ç¬¦ä¸²ç±»å‹
            spec = str(spec)
            
            print(f"æµ‹è¯•: {spec}")
            # ç›´æ¥è·¯å¾„æµ‹è¯•
            direct_path = os.path.join(self.fits_dir, spec)
            if os.path.exists(direct_path):
                print(f"  ç›´æ¥è·¯å¾„å­˜åœ¨: {direct_path}")
            else:
                print(f"  ç›´æ¥è·¯å¾„ä¸å­˜åœ¨: {direct_path}")
            
            # æµ‹è¯•å¸¦æ‰©å±•åçš„è·¯å¾„
            for ext in ['.fits', '.fits.gz', '.fit', '.fit.gz']:
                ext_path = direct_path + ext
                if os.path.exists(ext_path):
                    print(f"  å¸¦æ‰©å±•åçš„è·¯å¾„å­˜åœ¨: {ext_path}")
                    break
            
            # ä½¿ç”¨æŸ¥æ‰¾å‡½æ•°
            found_path = self._find_fits_file(spec)
            if found_path:
                print(f"  _find_fits_fileæ‰¾åˆ°: {found_path}")
            else:
                print(f"  _find_fits_fileæœªæ‰¾åˆ°æ–‡ä»¶")
            
            # ä½¿ç”¨ç¼“å­˜å‡½æ•°
            cached_path = self._get_file_extension(spec)
            if cached_path:
                print(f"  _get_file_extensionæ‰¾åˆ°: {cached_path}")
            else:
                print(f"  _get_file_extensionæœªæ‰¾åˆ°æ–‡ä»¶")
        
        # æ¸…é™¤ç¼“å­˜å¹¶é‡æ–°æµ‹è¯•
        print("\næ¸…é™¤ç¼“å­˜åé‡æ–°æµ‹è¯•:")
        self.fits_file_cache = {}
        for spec in test_files[:1]:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ª
            # ç¡®ä¿specæ˜¯å­—ç¬¦ä¸²ç±»å‹
            spec = str(spec)
            print(f"é‡æ–°æµ‹è¯•: {spec}")
            found_path = self._find_fits_file(spec)
            if found_path:
                print(f"  é‡æ–°æµ‹è¯•æ‰¾åˆ°: {found_path}")
            else:
                print(f"  é‡æ–°æµ‹è¯•æœªæ‰¾åˆ°æ–‡ä»¶")
        
        print("\n=== è¯Šæ–­å®Œæˆ ===\n")

    def _validate_wavelength_range(self, data):
        """éªŒè¯æ³¢é•¿èŒƒå›´æ•°æ®çš„å‡½æ•°
        
        å‚æ•°:
            data: è¦éªŒè¯çš„æ•°æ®å¯¹è±¡
            
        è¿”å›:
            bool: å¦‚æœæ•°æ®æ˜¯æœ‰æ•ˆçš„æ³¢é•¿èŒƒå›´å…ƒç»„åˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        # å¦‚æœæ˜¯æ³¢é•¿èŒƒå›´å…ƒç»„ï¼Œç›´æ¥è¿”å›True
        if isinstance(data, tuple) and len(data) == 2:
            return True
        # å¦åˆ™ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨çš„åŸå§‹éªŒè¯æ–¹æ³•
        if hasattr(self.cache_manager, '_original_validate'):
            return self.cache_manager._original_validate(data)
        # å¦‚æœæ²¡æœ‰åŸå§‹éªŒè¯æ–¹æ³•ï¼Œç›´æ¥è¿”å›True
        return True
        
    def update_cache_manager(self):
        """ä¿®æ”¹ç¼“å­˜ç®¡ç†å™¨çš„éªŒè¯åŠŸèƒ½ï¼Œå…è®¸ä¿å­˜éå­—å…¸æ ¼å¼çš„æ•°æ®"""
        # ä¿å­˜åŸå§‹çš„éªŒè¯æ–¹æ³•
        if hasattr(self.cache_manager, '_validate_cache_data'):
            self.cache_manager._original_validate = self.cache_manager._validate_cache_data
        
        # æ›¿æ¢éªŒè¯æ–¹æ³•ä¸ºç±»æ–¹æ³•
        self.cache_manager._validate_cache_data = self._validate_wavelength_range

    def visualize_example_spectra(self, element=None):
        """å¯è§†åŒ–æŒ‡å®šå…ƒç´ çš„ç¤ºä¾‹å…‰è°±
        
        Args:
            element (str, optional): å…ƒç´ åç§°ï¼Œå¦‚ 'C_FE'ã€'MG_FE'ã€'CA_FE'ã€‚
                                     ä¸ºNoneæ—¶ä¼šå°è¯•æ‰€æœ‰å·²å¤„ç†çš„å…ƒç´ ã€‚
        """
        print(f"å‡†å¤‡å¯è§†åŒ–ç¤ºä¾‹å…‰è°±...")
        
        # å®šä¹‰è¦å¤„ç†çš„å…ƒç´ åˆ—è¡¨
        elements_to_process = []
        if element:
            elements_to_process = [element]
        else:
            # å°è¯•ä»å·²çŸ¥çš„CSVæ–‡ä»¶ä¸­è·å–å…ƒç´ åˆ—è¡¨
            standard_elements = ['C_FE', 'MG_FE', 'CA_FE']
            for elem in standard_elements:
                if os.path.exists(f"{elem}.csv"):
                    elements_to_process.append(elem)
        
        if not elements_to_process:
            print("æœªæ‰¾åˆ°æŒ‡å®šå…ƒç´ æˆ–ä»»ä½•æ ‡å‡†å…ƒç´ çš„CSVæ–‡ä»¶")
            return
        
        print(f"å°†å¤„ç†ä»¥ä¸‹å…ƒç´ : {', '.join(elements_to_process)}")
        
        # å¤„ç†æ¯ä¸ªå…ƒç´ 
        for elem in elements_to_process:
            print(f"\n===== å¤„ç†å…ƒç´ : {elem} =====")
            
            # 1. æŸ¥æ‰¾è¯¥å…ƒç´ çš„CSVæ–‡ä»¶
            csv_path = f"{elem}.csv"
            if not os.path.exists(csv_path):
                print(f"æ‰¾ä¸åˆ°{csv_path}ï¼Œè·³è¿‡æ­¤å…ƒç´ ")
                continue
                
            # 2. è¯»å–CSVæ–‡ä»¶è·å–å…‰è°±æ–‡ä»¶åˆ—è¡¨
            try:
                import pandas as pd
                df = pd.read_csv(csv_path)
                
                if 'spec' not in df.columns:
                    print(f"CSVæ–‡ä»¶{csv_path}ä¸­æ‰¾ä¸åˆ°'spec'åˆ—")
                    continue
                    
                # å–å‰5ä¸ªç¤ºä¾‹
                sample_specs = df['spec'].values[:5]
                if len(sample_specs) == 0:
                    print(f"CSVæ–‡ä»¶{csv_path}ä¸­æ²¡æœ‰å…‰è°±æ•°æ®")
                    continue
                    
                print(f"æ‰¾åˆ°{len(sample_specs)}ä¸ªæ ·æœ¬å…‰è°±")
                
                # 3. å¯è§†åŒ–æ¯ä¸ªæ ·æœ¬å…‰è°±
                for i, spec in enumerate(sample_specs):
                    print(f"\nå¤„ç†æ ·æœ¬ {i+1}/{len(sample_specs)}: {spec}")
                    
                    # æ£€æŸ¥ç¼“å­˜å’Œå¤„ç†è¯¥å…‰è°±
                    cache_key = f"processed_{spec.replace('/', '_')}"
                    cached_data = self.cache_manager.get_cache(cache_key)
                    
                    if cached_data is None:
                        print(f"æ²¡æœ‰æ‰¾åˆ°å¤„ç†åçš„å…‰è°±ç¼“å­˜ï¼Œå°è¯•å¤„ç†: {spec}")
                        
                        # æŸ¥æ‰¾FITSæ–‡ä»¶
                        fits_file = self._find_fits_file(spec)
                        if fits_file is None:
                            print(f"æ‰¾ä¸åˆ°FITSæ–‡ä»¶: {spec}")
                            continue
                            
                        print(f"å¼€å§‹å¤„ç†FITSæ–‡ä»¶: {fits_file}")
                        try:
                            # ä½¿ç”¨0.0ä½œä¸ºå ä½ç¬¦æ ‡ç­¾å¤„ç†å…‰è°±
                            processed_data = self.process_single_spectrum(spec, 0.0)
                            if processed_data is None:
                                print(f"å¤„ç†å…‰è°±å¤±è´¥: {spec}")
                                continue
                                
                            print(f"å…‰è°±å¤„ç†æˆåŠŸï¼Œä¿å­˜åˆ°ç¼“å­˜")
                        except Exception as e:
                            print(f"å¤„ç†å…‰è°±æ—¶å‡ºé”™: {e}")
                            import traceback
                            traceback.print_exc()
                            continue
                    else:
                        print(f"ä½¿ç”¨ç¼“å­˜çš„é¢„å¤„ç†å…‰è°±")
                    
                    # å¯è§†åŒ–å¤„ç†åçš„å…‰è°±
                    try:
                        print(f"å¯è§†åŒ–å…‰è°±: {spec}")
                        self.visualize_spectrum(spec, processed=True, save=True)
                        print(f"å…‰è°±å¯è§†åŒ–å®Œæˆ")
                    except Exception as e:
                        print(f"å¯è§†åŒ–å…‰è°±æ—¶å‡ºé”™: {e}")
                        import traceback
                        traceback.print_exc()
                        
            except Exception as e:
                print(f"å¤„ç†å…ƒç´ {elem}æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                
        print("\n===== ç¤ºä¾‹å…‰è°±å¯è§†åŒ–å®Œæˆ =====")
        print(f"å›¾åƒä¿å­˜åœ¨: {os.path.abspath(self.output_dir)}")

    # åœ¨LAMOSTPreprocessorç±»å†…æ·»åŠ GPUæ’å€¼å‡½æ•°
    def _gpu_interp(self, x, y, xnew):
        """åœ¨GPUä¸Šè¿›è¡Œçº¿æ€§æ’å€¼
        
        å‚æ•°:
            x: åŸå§‹xåæ ‡
            y: åŸå§‹yå€¼
            xnew: æ–°çš„xåæ ‡
            
        è¿”å›:
            æ’å€¼åçš„yå€¼
        """
        import cupy as cp
        
        # ç¡®ä¿xæ˜¯å•è°ƒé€’å¢çš„
        if not cp.all(cp.diff(x) > 0):
            # æ’åºxå’Œy
            sort_indices = cp.argsort(x)
            x = x[sort_indices]
            y = y[sort_indices]
        
        # æ‰¾åˆ°xnewä¸­æ¯ä¸ªç‚¹åœ¨xä¸­çš„ä½ç½®
        indices = cp.searchsorted(x, xnew) - 1
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µ
        indices = cp.clip(indices, 0, len(x) - 2)
        
        # è®¡ç®—æƒé‡
        interval_width = x[indices + 1] - x[indices]
        weights = (xnew - x[indices]) / interval_width
        
        # çº¿æ€§æ’å€¼
        ynew = y[indices] * (1 - weights) + y[indices + 1] * weights
        
        return ynew

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    import argparse
    
    parser = argparse.ArgumentParser(description="LAMOSTå…‰è°±æ•°æ®é¢„å¤„ç†å™¨")
    parser.add_argument('--csv_files', nargs='+', default=None,
                      help='è¦å¤„ç†çš„CSVæ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å«ä¸€ä¸ªå…ƒç´ çš„æ•°æ®ã€‚ä¸æŒ‡å®šæ—¶è‡ªåŠ¨æ£€æµ‹å½“å‰ç›®å½•æ‰€æœ‰CSVæ–‡ä»¶')
    parser.add_argument('--fits_dir', default='fits', help='FITSæ–‡ä»¶ç›®å½•')
    parser.add_argument('--output_dir', default='processed_data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--wavelength_range', nargs=2, type=float, default=None,
                      help='æ³¢é•¿èŒƒå›´ï¼Œä¾‹å¦‚: 4000 8000')
    parser.add_argument('--n_points', type=int, default=None,
                      help='é‡é‡‡æ ·åçš„ç‚¹æ•°')
    parser.add_argument('--log_step', type=float, default=0.0001,
                      help='å¯¹æ•°ç©ºé—´ä¸­çš„é‡é‡‡æ ·æ­¥é•¿ï¼ˆdexï¼‰')
    parser.add_argument('--batch_size', type=int, default=20,
                      help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--max_workers', type=int, default=None,
                      help='æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°çš„ä¸€åŠ')
    parser.add_argument('--memory_limit', type=float, default=0.7,
                      help='å†…å­˜ä½¿ç”¨é™åˆ¶(å æ€»å†…å­˜æ¯”ä¾‹)')
    parser.add_argument('--no_resume', action='store_true',
                      help='ä¸æ¢å¤ä¹‹å‰çš„è¿›åº¦ï¼Œä»å¤´å¼€å§‹å¤„ç†')
    parser.add_argument('--evaluate', action='store_true',
                      help='è¯„ä¼°é¢„å¤„ç†æ•ˆæœ')
    parser.add_argument('--single_element', type=str, default=None,
                      help='ä»…å¤„ç†æŒ‡å®šå…ƒç´ çš„CSVæ–‡ä»¶ï¼Œä¾‹å¦‚: C_FE')
    parser.add_argument('--low_memory_mode', action='store_true', 
                      help='å¯ç”¨ä½å†…å­˜æ¨¡å¼ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ä½†é€Ÿåº¦å˜æ…¢')
    parser.add_argument('--use_gpu', action='store_true',
                      help='ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—(éœ€è¦å®‰è£…CuPyåº“)')
    parser.add_argument('--no_gpu', action='store_false', dest='use_gpu',
                      help='ç¦ç”¨GPUåŠ é€Ÿï¼Œå¼ºåˆ¶ä½¿ç”¨CPU')
    
    args = parser.parse_args()
    
    # è®¾ç½®åŸºç¡€è·¯å¾„
    base_path = '/content' if IN_COLAB else os.path.abspath('.')
    print(f"åŸºç¡€è·¯å¾„: {base_path}")
    
    # è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
    mem_info = psutil.virtual_memory()
    print(f"ç³»ç»Ÿå†…å­˜: æ€»è®¡ {mem_info.total / (1024**3):.1f}GB, "
          f"å¯ç”¨ {mem_info.available / (1024**3):.1f}GB, "
          f"ä½¿ç”¨ç‡ {mem_info.percent}%")
    
    # æ£€æµ‹å†…å­˜æƒ…å†µï¼Œè‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨ä½å†…å­˜æ¨¡å¼
    low_memory_mode = args.low_memory_mode or mem_info.percent > 80
    
    if low_memory_mode and not args.low_memory_mode:
        print("æ£€æµ‹åˆ°ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œè‡ªåŠ¨å¯ç”¨ä½å†…å­˜æ¨¡å¼")
        user_choice = input("æ˜¯å¦å¯ç”¨ä½å†…å­˜æ¨¡å¼? è¿™å°†å‡å°‘å†…å­˜ä½¿ç”¨ä½†å¤„ç†é€Ÿåº¦ä¼šå˜æ…¢ (y/n): ").lower()
        low_memory_mode = user_choice == 'y'
    
    # æ£€æŸ¥GPUæ”¯æŒ
    use_gpu = args.use_gpu
    if use_gpu:
        if not HAS_GPU:
            print("è­¦å‘Š: è¦æ±‚ä½¿ç”¨GPUä½†æœªæ‰¾åˆ°CuPyåº“ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            print("è¦ä½¿ç”¨GPUï¼Œè¯·å®‰è£…CuPyåº“: pip install cupy-cuda11x (æ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„åŒ…)")
            use_gpu = False
        else:
            # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
            gpu_available = check_gpu_available()
            if not gpu_available:
                print("è­¦å‘Š: æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPUè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
                use_gpu = False
            else:
                print("ğŸš€ å°†ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—")
    
    # è®¾ç½®CSVæ–‡ä»¶è·¯å¾„
    if args.csv_files is None:
        # è‡ªåŠ¨æ£€æµ‹å½“å‰ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
        args.csv_files = [f for f in os.listdir() if f.endswith('.csv')]
        if not args.csv_files:
            print("é”™è¯¯: å½“å‰ç›®å½•æœªæ‰¾åˆ°CSVæ–‡ä»¶ï¼Œè¯·æŒ‡å®š--csv_fileså‚æ•°")
            return
        print(f"è‡ªåŠ¨æ£€æµ‹åˆ°ä»¥ä¸‹CSVæ–‡ä»¶: {args.csv_files}")
    
    # å¦‚æœæŒ‡å®šäº†å•ä¸ªå…ƒç´ ï¼Œå°±åªå¤„ç†å¯¹åº”çš„CSVæ–‡ä»¶
    if args.single_element:
        # æŸ¥æ‰¾åŒ¹é…è¯¥å…ƒç´ çš„CSVæ–‡ä»¶
        matching_files = []
        for csv_file in args.csv_files:
            element_name = os.path.basename(csv_file).split('.')[0]
            if element_name == args.single_element:
                matching_files.append(csv_file)
        
        if not matching_files:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°å…ƒç´  {args.single_element} å¯¹åº”çš„CSVæ–‡ä»¶")
            return
        
        args.csv_files = matching_files
        print(f"ä»…å¤„ç†å…ƒç´  {args.single_element} çš„æ•°æ®: {args.csv_files}")
    
    fits_dir = args.fits_dir
    if not os.path.isabs(fits_dir):
        fits_dir = os.path.join(base_path, fits_dir)
    
    output_dir = args.output_dir
    if not os.path.isabs(output_dir):
        output_dir = os.path.join(base_path, output_dir)
    
    # å±•ç¤ºè·¯å¾„ä¿¡æ¯
    print(f"CSVæ–‡ä»¶è·¯å¾„: {args.csv_files}")
    print(f"FITSç›®å½•è·¯å¾„: {fits_dir}")
    print(f"è¾“å‡ºç›®å½•è·¯å¾„: {output_dir}")
    
    # ç¡®ä¿fitsç›®å½•å­˜åœ¨
    if not os.path.exists(fits_dir):
        print(f"åˆ›å»ºFITSç›®å½•: {fits_dir}")
        os.makedirs(fits_dir, exist_ok=True)
    
    # æ£€æµ‹FITSæ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
    print("\n=== æ£€æŸ¥FITSæ–‡ä»¶æœ‰æ•ˆæ€§ ===")
    fits_files = []
    if os.path.exists(fits_dir):
        fits_files = [os.path.join(fits_dir, f) for f in os.listdir(fits_dir) 
                    if f.endswith(('.fits', '.fits.gz', '.fit', '.fit.gz'))]
    
    if not fits_files:
        print("è­¦å‘Šï¼šæ‰¾ä¸åˆ°ä»»ä½•FITSæ–‡ä»¶ï¼")
    else:
        print(f"æ‰¾åˆ°{len(fits_files)}ä¸ªFITSæ–‡ä»¶ï¼Œå¼€å§‹æ£€æŸ¥...")
        
        # æŠ½æ ·æ£€æŸ¥å‡ ä¸ªæ–‡ä»¶
        sample_size = min(5, len(fits_files))
        sample_files = fits_files[:sample_size]
        
        valid_count = 0
        for file in sample_files:
            print(f"\næ£€æŸ¥æ–‡ä»¶: {os.path.basename(file)}")
            try:
                with fits.open(file, ignore_missing_end=True, memmap=False) as hdul:
                    print(f"  HDUæ•°é‡: {len(hdul)}")
                    print(f"  ä¸»HDUç±»å‹: {type(hdul[0]).__name__}")
                    header = hdul[0].header
                    print(f"  ä¸»è¦å¤´ä¿¡æ¯: NAXIS={header.get('NAXIS')}, NAXIS1={header.get('NAXIS1')}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    has_data = False
                    for i, hdu in enumerate(hdul):
                        if hdu.data is not None:
                            data_shape = hdu.data.shape if hasattr(hdu.data, 'shape') else "æ— å½¢çŠ¶"
                            print(f"  HDU{i}æœ‰æ•°æ®: å½¢çŠ¶={data_shape}")
                            has_data = True
                            break
                    
                    if not has_data:
                        print("  è­¦å‘Š: æ‰€æœ‰HDUä¸­éƒ½æ²¡æœ‰æ•°æ®")
                    else:
                        valid_count += 1
                        
            except Exception as e:
                print(f"  è¯»å–é”™è¯¯: {e}")
        
        print(f"\næ£€æŸ¥ç»“æœ: {valid_count}/{sample_size}ä¸ªæ–‡ä»¶æœ‰æœ‰æ•ˆæ•°æ®")
        
        if valid_count == 0:
            print("æ‰€æœ‰æµ‹è¯•æ–‡ä»¶éƒ½æ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½æ˜¯FITSæ–‡ä»¶æ ¼å¼æœ‰é—®é¢˜ã€‚")
            fix_option = input("æ˜¯å¦å°è¯•è‡ªåŠ¨ä¿®å¤FITSæ–‡ä»¶? (y/n): ").lower()
            if fix_option == 'y':
                print("å°è¯•ä½¿ç”¨astropyä¿®å¤FITSæ–‡ä»¶...")
                # è¿™é‡Œåªä¿®å¤ç¤ºä¾‹æ–‡ä»¶
                for file in sample_files:
                    try:
                        # è¯»å–æ–‡ä»¶å¹¶é‡æ–°å†™å…¥ï¼Œå¯èƒ½ä¼šä¿®å¤ä¸€äº›æ ¼å¼é—®é¢˜
                        with fits.open(file, ignore_missing_end=True) as hdul:
                            fixed_file = file + '.fixed'
                            hdul.writeto(fixed_file, overwrite=True)
                            print(f"  å·²ä¿®å¤: {os.path.basename(file)} -> {os.path.basename(fixed_file)}")
                    except Exception as e:
                        print(f"  ä¿®å¤å¤±è´¥: {e}")
    
    print("\n=== æ£€æŸ¥å®Œæˆ ===\n")
    
    # åˆå§‹åŒ–é¢„å¤„ç†å™¨
    wavelength_range = tuple(args.wavelength_range) if args.wavelength_range else None
    
    preprocessor = LAMOSTPreprocessor(
        csv_files=args.csv_files,
        fits_dir=fits_dir,
        output_dir=output_dir,
        wavelength_range=(3690, 9100),  # ä½¿ç”¨å›ºå®šèŒƒå›´
        n_points=None,  # ä½¿ç”¨æ­¥é•¿è‡ªåŠ¨è®¡ç®—ç‚¹æ•°
        log_step=0.0001,
        compute_common_range=False,  # å¼ºåˆ¶ä¸è®¡ç®—å…¬å…±èŒƒå›´
        max_workers=args.max_workers if args.max_workers is not None else (1 if low_memory_mode else 2),
        batch_size=5 if low_memory_mode else args.batch_size,
        memory_limit=args.memory_limit,
        low_memory_mode=low_memory_mode,
        use_gpu=use_gpu  # æ·»åŠ GPUæ”¯æŒ
    )
    
    # æ£€æŸ¥æ•°æ®æº
    preprocessor.check_data_sources()
    
    # æ·»åŠ è·¯å¾„è¯Šæ–­
    preprocessor.check_and_fix_file_paths()
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦æ¸…ç†ç¼“å­˜
    preprocessor.clean_cache()
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
    user_input = input("æ˜¯å¦ç»§ç»­å¤„ç†æ•°æ®? (y/n): ").strip().lower()
    if user_input != 'y':
        print("ç¨‹åºå·²ç»ˆæ­¢")
        return
    
    # å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    X, y, filenames, elements = preprocessor.process_all_data(resume=not args.no_resume)
    
    if X is None or len(X) == 0:
        print("é”™è¯¯: æ²¡æœ‰å¤„ç†åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥fitsæ–‡ä»¶è·¯å¾„å’ŒCSVæ–‡ä»¶")
        return
        
    # åˆ†å‰²æ•°æ®é›†
    preprocessor.split_dataset(X, y, elements)
    
    # æ¸…ç†ç¼“å­˜æ–‡ä»¶æŸ¥æ‰¾è®°å½•
    preprocessor._save_files_cache()
    
    # è®¡ç®—å¹¶æ˜¾ç¤ºæ€»è€—æ—¶
    elapsed_time = time.time() - start_time
    hours, remainder = divmod(elapsed_time, 3600)
    minutes, seconds = divmod(remainder, 60)
    print(f"\nå¤„ç†å®Œæˆ! æ€»è€—æ—¶: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’")

if __name__ == "__main__":
    try:
        # å¤„ç†Colabç¯å¢ƒ
        if IN_COLAB:
            try:
                # ä½¿ç”¨åŠ¨æ€å¯¼å…¥é¿å…IDEæŠ¥é”™
                import importlib
                colab_files = importlib.import_module('google.colab.files')
                
                # è¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦ä¸Šä¼ æ–‡ä»¶
                if input("æ˜¯å¦éœ€è¦ä¸Šä¼ CSVæ–‡ä»¶? (y/n): ").lower() == 'y':
                    print("è¯·ä¸Šä¼ C_FE.csv, MG_FE.csv, CA_FE.csvæ–‡ä»¶...")
                    uploaded = colab_files.upload()
                    print("ä¸Šä¼ çš„æ–‡ä»¶:", list(uploaded.keys()))

                # è¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦ä¸Šä¼ FITSæ–‡ä»¶
                if input("æ˜¯å¦éœ€è¦ä¸Šä¼ FITSæ–‡ä»¶? (y/n): ").lower() == 'y':
                    # å¦‚æœFITSæ–‡ä»¶æ˜¯æ‰“åŒ…çš„ï¼Œä¸Šä¼ å¹¶è§£å‹
                    print("è¯·ä¸Šä¼ åŒ…å«FITSæ–‡ä»¶çš„å‹ç¼©åŒ…...")
                    fits_archive = colab_files.upload()
                    archive_name = list(fits_archive.keys())[0]

                    # åˆ›å»ºfitsç›®å½•
                    os.makedirs('fits', exist_ok=True)

                    # è§£å‹ç¼©æ–‡ä»¶åˆ°fitsç›®å½•
                    if archive_name.endswith('.zip'):
                        print(f"æ­£åœ¨è§£å‹ {archive_name}...")
                        with zipfile.ZipFile(archive_name, 'r') as zip_ref:
                            zip_ref.extractall('fits/')
                        print(f"å·²å°†{archive_name}è§£å‹åˆ°fitsç›®å½•")
                        
                        # æ£€æŸ¥è§£å‹åçš„ç›®å½•ç»“æ„
                        fits_files = []
                        for root, dirs, files in os.walk('fits'):
                            for file in files:
                                if any(file.endswith(ext) for ext in ['.fits', '.fits.gz', '.fit', '.fit.gz']):
                                    fits_files.append(os.path.join(root, file))
                        
                        print(f"è§£å‹åæ‰¾åˆ° {len(fits_files)} ä¸ªFITSæ–‡ä»¶")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰åµŒå¥—ç›®å½•
                        nested_dirs = set()
                        for file in fits_files:
                            rel_dir = os.path.relpath(os.path.dirname(file), 'fits')
                            if rel_dir != '.':
                                nested_dirs.add(rel_dir)
                        
                        if nested_dirs:
                            print(f"å‘ç°åµŒå¥—ç›®å½•ç»“æ„: {', '.join(list(nested_dirs)[:3])}")
                            move_files = input("æ˜¯å¦å°†æ‰€æœ‰FITSæ–‡ä»¶ç§»åŠ¨åˆ°fitsæ ¹ç›®å½•? (y/n): ").lower() == 'y'
                            
                            if move_files:
                                # ç§»åŠ¨æ–‡ä»¶åˆ°æ ¹ç›®å½•
                                for file in fits_files:
                                    if os.path.dirname(file) != 'fits':
                                        target = os.path.join('fits', os.path.basename(file))
                                        print(f"ç§»åŠ¨ {os.path.basename(file)}")
                                        # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                                        if os.path.exists(target):
                                            os.remove(target)
                                        os.rename(file, target)
                                print("æ–‡ä»¶ç§»åŠ¨å®Œæˆ")
                    else:
                        print("ä¸æ”¯æŒçš„å‹ç¼©æ ¼å¼ï¼Œè¯·ä¸Šä¼ .zipæ–‡ä»¶")
            except Exception as e:
                print(f"Colabç¯å¢ƒè®¾ç½®å‡ºé”™: {e}")
                print("ç»§ç»­ä½¿ç”¨æœ¬åœ°æ–‡ä»¶...")
                
        # è¿è¡Œä¸»ç¨‹åº
        main()
        
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œæ‚¨å¯ä»¥ç¨åé‡æ–°è¿è¡Œç»§ç»­å¤„ç†")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 